
UNE COURTE HISTOIRE DE L'EBOOK


MARIE LEBERT


NEF, Université de Toronto, 2009


Copyright © 2009 Marie Lebert. Tous droits réservés.


  ----
  Ce livre est dédié à toutes les personnes
  ayant répondu à mes questions pendant dix ans,
  en Europe, en Amérique (tout le continent),
  en Afrique et en Asie.
  Avec tous mes remerciements pour leur temps
  et pour leur amitié.
  ----

Une courte histoire de l'ebook - appelé aussi livre numérique - de 1971
à nos jours, avec le Projet Gutenberg, Amazon, Adobe, Mobipocket,
Google Books, l'Internet Archive et bien d'autres. Ce livre se base sur
quelques milliers d'heures de navigation sur le web pendant dix ans et
sur une centaine d'entretiens conduits de par le monde.

Ce livre est disponible aussi en anglais et en espagnol. Les trois
versions sont disponibles dans les Dossiers du NEF <http://www.etudes-
francaises.net/dossiers/ebook.htm>.

Marie Lebert, chercheuse et journaliste, s'intéresse aux technologies
dans le monde du livre, des autres médias et des langues. Elle est
l'auteure de "Technologies et livre pour tous" (en français et en
anglais, 2008), "Les mutations du livre" (en français, 2007) et "Le
Livre 010101" (en français, 2003). Ses livres et dossiers sont publiés
par le NEF (Net des études françaises), Université de Toronto, et sont
librement disponibles sur le NEF <http://www.etudes-francaises.net>.



TABLE

  ====  Introduction
  1971: Le Projet Gutenberg est la première bibliothèque numérique.
  1990: Le web booste l'internet.
  1993: L'Online Books Page recense les ebooks gratuits.
  1994: De plus en plus de textes sont disponibles en ligne.
  1995: Amazon.com est la première grande librairie en ligne.
  1996: Des éditeurs se lancent sur l'internet.
  1997: La convergence multimédia est le sujet d'un colloque.
  1998: Les bibliothèques emménagent sur le web.
  1999: Les bibliothécaires deviennent cyberthécaires.
  2000: L'information devient multilingue.
  2001: Copyright, copyleft et Creative Commons.
  2002: Le web devient une vaste encyclopédie.
  2003: Les nouveautés sont en version numérique.
  2004: Des auteurs sont créatifs sur le net.
  2005: Google s'intéresse à l'ebook.
  2006: Vers une bibliothèque numérique planétaire.
  2007: Nous lisons sur divers appareils électroniques.
  2008: Les ebooks sont partout.
  2009: Cyberespace et société de l'information.
  ====  Chronologie
  ====  Remerciements



INTRODUCTION


Le livre a beaucoup changé depuis 1971.

Le livre imprimé a cinq siècles et demi. Le livre numérique a bientôt
quarante ans. Il est né avec le Projet Gutenberg, créé en juillet 1971
par Michael Hart pour distribuer gratuitement les oeuvres du domaine
public par voie électronique. Mais il faut attendre le web et le
premier navigateur au début des années 1990 pour que le Projet
Gutenberg trouve sa vitesse de croisière.

Signe des temps, en novembre 2000, la British Library met en ligne la
version numérique de la Bible de Gutenberg, premier livre à avoir
jamais été imprimé. Datant de 1454 ou 1455, cette Bible aurait été
imprimée par Gutenberg en 180 exemplaires dans son atelier de Mayence,
en Allemagne. 48 exemplaires, dont certains incomplets, existeraient
toujours, dont trois - deux version complètes et une partielle - à la
British Library.

Des milliers d’oeuvres du domaine public sont en accès libre sur le
web. Les libraires et les éditeurs ont pour la plupart un site web.
Certains naissent  directement sur le web, avec la totalité de leurs
transactions s'effectuant via l’internet. De plus en plus de livres et
revues ne sont disponibles qu’en version numérique, pour éviter les
coûts d’une publication imprimée. On peut désormais lire un livre sur
son ordinateur, sur son assistant personnel (PDA), sur son téléphone,
sur son smartphone ou sur un appareil dédié.

L’internet est devenu indispensable pour se documenter, pour
communiquer, pour avoir accès aux documents et pour élargir ses
connaissances. Le web est devenu une gigantesque encyclopédie, une
énorme bibliothèque, une immense librairie et un médium des plus
complets. De statique dans les livres imprimés, l’information est
devenue fluide, avec possibilité d’actualisation constante.

Nous n'avons plus besoin de courir désespérément après l'information
dont nous avons besoin. L'information dont nous avons besoin est enfin
à notre portée. Y compris pour ceux qui suivent leurs études par
correspondance, qui vivent en rase campagne, qui travaillent à domicile
ou qui sont cloués dans un lit.

Voici le voyage «virtuel» que nous allons suivre:

  1971: Le Projet Gutenberg est la première bibliothèque numérique.
  1990: Le web booste l'internet.
  1993: L'Online Books Page recense les ebooks gratuits.
  1994: De plus en plus de textes sont disponibles en ligne.
  1995: Amazon.com est la première grande librairie en ligne.
  1996: Des éditeurs se lancent sur l'internet.
  1997: La convergence multimédia fait l'objet d'un colloque.
  1998: Les bibliothèques emménagent sur le web.
  1999: Les bibliothécaires deviennent cyberthécaires.
  2000: L'information devient multilingue.
  2001: Copyright, copyleft et Creative Commons.
  2002: Le web devient une vaste encyclopédie.
  2003: Les nouveautés sont en version numérique.
  2004: Des auteurs sont créatifs sur le net.
  2005: Google s'intéresse à l'ebook.
  2006: Vers une bibliothèque numérique planétaire.
  2007: Nous lisons sur divers appareils électroniques.
  2008: Les ebooks sont partout.
  2009: Cyberespace et société de l'information.

Sauf indication contraire, les citations présentes dans ce livre sont
des extraits des Entretiens du NEF <http://www.etudes-
francaises.net/entretiens/>.



1971: LE PROJET GUTENBERG EST LA PREMIERE BIBLIOTHEQUE NUMERIQUE


= [Résumé]

Fondé par Michael Hart en juillet 1971 alors qu’il était étudiant à
l’Université d’Illinois (Etats-Unis), le Projet Gutenberg a pour but de
diffuser gratuitement par voie électronique le plus grand nombre
possible d’oeuvres du domaine public. «Nous considérons le texte
électronique comme un nouveau médium, sans véritable relation avec le
papier, écrit Michael. Le seul point commun est que nous diffusons les
mêmes oeuvres, mais je ne vois pas comment le papier peut concurrencer
le texte électronique une fois que les gens y sont habitués,
particulièrement dans les établissements d'enseignement.» Lorsque
l’utilisation du web se généralise dans les années 1990, le Projet
Gutenberg trouve un second souffle et un rayonnement international. Au
fil des ans, des centaines d’oeuvres sont patiemment numérisées en mode
texte par des milliers de volontaires. D’abord essentiellement
anglophones, les collections deviennent peu à peu multilingues. Le
Projet Gutenberg Europe débute en janvier 2004. Le Projet Gutenberg
franchit la barre des 20.000 titres en décembre 2006 et celle des
25.000 titres en avril 2008.


= Un pari depuis 38 ans

# Gestation

Quels furent les tous débuts du projet? Alors étudiant à l’Université
d’Illinois (Etats-Unis), Michael Hart se voit attribuer quelques
millions de dollars de «temps machine» dans le laboratoire informatique
(Materials Research Lab) de son université. Le 4 juillet 1971, jour de
la fête nationale, il saisit "The United States Declaration of
Independence" (Déclaration de l’indépendance des Etats-Unis, signée le
4 juillet 1776) sur le clavier de son ordinateur. En caractères
majuscules, puisque les caractères minuscules n’existent pas encore. Le
texte électronique représente 5 Ko (kilo-octets). Mais l’envoi d’un
fichier de 5 Ko à la centaine de personnes que représente le réseau de
l’époque aurait fait imploser celui-ci, la bande passante étant infime.
Michael diffuse donc un message indiquant où le texte est stocké - sans
lien hypertexte toutefois, puisque le web ne voit le jour que vingt ans
après - suite à quoi le fichier est téléchargé par six personnes.

Dans la foulée, Michael décide de consacrer ce crédit-temps de quelques
millions de dollars à la recherche des oeuvres du domaine public
disponibles en bibliothèque et à la numérisation de celles-ci. Il
décide aussi de stocker les textes électroniques de la manière la plus
simple possible, au format ASCII, pour que ces textes puissent être lus
sans problème quels que soient la machine, la plateforme et le logiciel
utilisés. Au lieu d’être un ensemble de pages reliées, le livre devient
un texte électronique que l’on peut dérouler en continu, avec des
lettres capitales pour les termes en italique, en gras et soulignés de
la version imprimée.

Peu après, Michael définit la mission du Projet Gutenberg: mettre à la
disposition de tous, par voie électronique, le plus grand nombre
possible d’oeuvres du domaine public. «Nous considérons le texte
électronique comme un nouveau médium, sans véritable relation avec le
papier, explique-t-il plus tard, en août 1998. Le seul point commun est
que nous diffusons les mêmes oeuvres, mais je ne vois pas comment le
papier peut concurrencer le texte électronique une fois que les gens y
sont habitués, particulièrement dans les écoles.»

Après avoir saisi "The United States Declaration of Independence en
1971", Michael poursuit ses efforts en 1972 en saisissant un texte plus
long, "The United States Bill of Rights" (Déclaration des droits
américaine). Cette Déclaration des droits comprend les dix premiers
amendements ajoutés en 1789 à la Constitution des Etats-Unis (qui date
elle-même de 1787), et définissant les droits individuels des citoyens
et les pouvoirs respectifs du gouvernement fédéral et des Etats. En
1973, un volontaire saisit "The United States Constitution"
(Constitution des Etats-Unis) dans son entier.

# Persévérance

D’année en année, la capacité de la disquette augmente régulièrement -
le disque dur n’existe pas encore - si bien qu'il est possible
d’envisager des fichiers de plus en plus volumineux. Des volontaires
entreprennent la numérisation de la Bible, composée elle-même de
plusieurs livres, qui peuvent être traités séparément et occuper chacun
un fichier différent.

Michael Hart débute la saisie des oeuvres complètes de Shakespeare, une
pièce après l’autre, avec un fichier pour chaque pièce. Cette version
n'est d’ailleurs jamais mise en ligne, du fait d’une loi plus
contraignante sur le copyright entrée en vigueur dans l’intervalle, et
qui vise non pas le texte de Shakespeare, tombé depuis longtemps dans
le domaine public, mais les commentaires et notes de l'édition
correspondante. D’autres éditions annotées appartenant au domaine
public seront mises en ligne quelques années plus tard.

Parallèlement, l’internet, qui était encore embryonnaire en 1971,
débute véritablement en 1974, suite à la création du protocole TCP/IP
(transmission control protocol/internet protocol). En 1983, le réseau
est en plein essor.

# De 10 à 1.000 ebooks

En août 1989, le Projet Gutenberg met en ligne son dixième texte, "The
King James Bible", publiée pour la première fois en 1611 et dont la
version la plus connue date de 1769. L'ensemble des fichiers de
l'Ancien Testament et du Nouveau Testament représente 5 Mo (méga-
octets).

En 1990, les internautes sont au nombre de 250.000, et le standard en
vigueur est la disquette de 360 Ko. En janvier 1991, Michael Hart
saisit "Alice’s Adventures in Wonderland" (Alice au pays des
merveilles) de Lewis Carroll (paru en 1865). En juillet de la même
année, il saisit "Peter Pan" de James M. Barrie (paru en 1904). Ces
deux classiques de la littérature enfantine tiennent chacun sur une
disquette standard.

Arrive ensuite le web, opérationnel en 1991. Le premier navigateur,
Mosaic, apparaît en novembre 1993. Lorsque l’utilisation du web se
généralise, il devient plus facile de faire circuler les textes
électroniques et de recruter des volontaires.

Le Projet Gutenberg rode sa méthode de travail, avec la numérisation
d’un texte par mois en 1991, deux textes par mois en 1992, quatre
textes par mois en 1993 et huit textes par mois en 1994.

En janvier 1994, le Projet Gutenberg fête son centième livre avec la
mise en ligne de "The Complete Works of William Shakespeare" (Les
oeuvres complètes de William Shakespeare). Shakespeare écrivit
l'essentiel de son oeuvre entre 1590 et 1613.

La production continue ensuite d’augmenter, avec une moyenne de 8
textes par mois en 1994, 16 textes par mois en 1995 et 32 textes par
mois en 1996.

Comme on le voit, entre 1991 et 1996, la production double chaque
année. Tout en continuant de numériser des livres, Michael coordonne
désormais le travail de dizaines de volontaires.

Depuis la fin 1993, le Projet Gutenberg s’articule en trois grands
secteurs: a) Light Literature (littérature de divertissement), qui
inclut par exemple "Alice’s Adventures in Wonderland", "Peter Pan" ou
"Aesop’s Fables" (Les Fables d’Esope); b) Heavy Literature (littérature
«sérieuse»), qui inclut par exemple La Bible, les oeuvres de
Shakespeare ou "Moby Dick"; c) Reference Literature (littérature de
référence), composée d’encyclopédies et de dictionnaires, par exemple
le "Roget’s Thesaurus". Cette présentation en trois secteurs est
abandonnée par la suite pour laisser place à un classement par
rubriques plus détaillé.

Le Projet Gutenberg se veut universel, aussi bien pour les oeuvres
choisies que pour le public visé, le but étant de mettre la littérature
à la disposition de tous, en dépassant largement le public habituel des
étudiants et des enseignants. Le secteur consacré à la littérature de
divertissement est destiné à amener devant l’écran un public très
divers, par exemple des enfants et leurs grands-parents recherchant le
texte électronique de "Peter Pan" après avoir vu le film Hook, ou
recherchant la version électronique d’"Alice au pays des merveilles"
après avoir regardé l'adaptation filmée à la télévision, ou recherchant
l’origine d’une citation littéraire après avoir vu un épisode de Star
Trek. Pratiquement tous les épisodes de Star Trek citent des livres
ayant leur correspondant numérique dans le Projet Gutenberg.

L’objectif est donc que le public, qu’il soit familier ou non avec le
livre imprimé, puisse facilement retrouver des textes entendus dans des
conversations, des films, des musiques, ou alors lus dans d’autres
livres, journaux et magazines. Les fichiers électroniques prennent peu
de place grâce à l’utilisation du format ASCII. On peut facilement les
télécharger par le biais de la ligne téléphonique. La recherche
textuelle est tout aussi simple. Il suffit d’utiliser la fonction
«chercher» présente dans n’importe quel logiciel.

En 1997, la production est toujours de 32 titres par mois. En juin
1997, le Projet Gutenberg met en ligne "The Merry Adventures of Robin
Hood" (Les aventures de Robin des Bois) de Howard Pyle (paru en 1883).
En août 1997, il met en ligne son millième texte électronique, "La
Divina Commedia" (La Divine Comédie) de Dante Alighieri (parue en
1321), dans sa langue d’origine, en italien.

En août 1998, Michael Hart écrit: «Mon projet est de mettre 10.000
textes électroniques sur l’internet. (Ce sera chose faite en octobre
2003, ndlr.) Si je pouvais avoir des subventions importantes,
j’aimerais aller jusqu’à un million et étendre aussi le nombre de nos
usagers potentiels de 1,x% à 10% de la population mondiale, ce qui
représenterait la diffusion de 1.000 fois un milliard de textes
électroniques, au lieu d’un milliard seulement.»

# De 1.000 à 10.000 ebooks

Entre 1998 et 2000, la moyenne est constante, avec 36 textes par mois.
En mai 1999, les collections comptent 2.000 livres. Le 2.000e texte est
Don Quijote (Don Quichotte) de Cervantès (paru en 1605), dans sa langue
d’origine, en espagnol.

Disponible en décembre 2000, le 3.000e titre est le troisième volume de
"A l’ombre des jeunes filles en fleurs" de Marcel Proust (paru en
1919), dans sa langue d'origine, en français. La moyenne passe à 104
livres par mois en 2001.

Mis en ligne en octobre 2001, le 4.000e texte est "The French Immortals
Series" (La série des Immortels français), dans sa traduction anglaise.
Publié à Paris en 1905 par la Maison Mazarin, ce livre rassemble
plusieurs fictions d’écrivains couronnés par l’Académie française,
comme Emile Souvestre, Pierre Loti, Hector Malot, Charles de Bernard,
Alphonse Daudet, etc.

Disponible en avril 2002, le 5.000e texte est "The Notebooks of
Leonardo da Vinci" (Les Carnets de Léonard de Vinci), qui datent du
début du 16e siècle. Un texte qui, en 2009, se trouve toujours dans le
Top 100 des livres téléchargés.

En 1988, Michael Hart choisit de numériser "Alice’s Adventures in
Wonderland" et "Peter Pan" parce que, dans l’un et l’autre cas, leur
version numérisée tient sur une disquette de 360 Ko, le standard de
l’époque. Quinze ans plus tard, en 2002, on dispose de disquettes de
1,44 Mo et on peut aisément compresser les fichiers en les zippant. Un
fichier standard peut désormais comporter trois millions de caractères,
plus qu’il n’en faut pour un livre de taille moyenne, puisqu'un roman
de 300 pages numérisé au format ASCII représente un mégaoctet. Un livre
volumineux tient sur deux fichiers ASCII, téléchargeables tels quels ou
en version zippée. Cinquante heures environ sont nécessaires pour
sélectionner un livre de taille moyenne, vérifier qu’il est bien du
domaine public, le scanner, le corriger, le formater et le mettre en
page.

Quelques numéros de livres sont réservés pour l’avenir, par exemple le
numéro 1984 (eBook #1984) pour le roman éponyme de George Orwell,
publié en 1949, et qui est donc loin d’être tombé dans le domaine
public.

En 2002, les collections s’accroissent de 203 titres par mois. Au
printemps 2002, elles représentent le quart des oeuvres du domaine
public en accès libre sur le web, recensées de manière pratiquement
exhaustive par l’Internet Public Library (IPL). Un beau résultat dû au
patient travail de milliers de volontaires actifs dans de nombreux
pays.

1.000 livres en août 1997, 2.000 livres en mai 1999, 3.000 livres en
décembre 2000, 4.000 livres en octobre 2001, 5.000 livres en avril
2002, 10.000 livres en octobre 2003. Le 10.000e livre est The Magna
Carta, qui fut le premier texte constitutionnel anglais, signé en 1215.

Entre avril 2002 et octobre 2003, les collections doublent, passant de
5.000 à 10.000 livres en dix-huit mois. La moyenne mensuelle est de 348
livres numérisés en 2003.

Dix mille livres. Un chiffre impressionnant quand on pense à ce que
cela représente de pages scannées, relues et corrigées. Cette
croissance rapide est due à l’activité de Distributed Proofreaders
(DP), un site conçu en 2000 par Charles Franks pour permettre la
correction partagée. Les volontaires choisissent un livre en cours de
traitement pour relire et corriger une page donnée. Chacun travaille à
son propre rythme. A titre indicatif, il est conseillé de relire une
page par jour. C’est peu de temps sur une journée, et c’est beaucoup
pour le projet.

En août 2003, un CD "Best of Gutenberg" est disponible avec une
sélection de 600 livres. En décembre 2003, date à laquelle le Projet
Gutenberg franchit la barre des 10.000 livres, la quasi-totalité des
livres (9.400 livres) est gravée sur un DVD. CD et DVD sont envoyés
gratuitement à qui en fait la demande. Libre ensuite à chacun de faire
autant de copies que possible et de les distribuer autour de soi.

# De 10.000 à 20.000 ebooks

En décembre 2003, les collections approchent les 11.000 livres.
Plusieurs formats sont désormais présents, par exemple les formats
HTML, XML et RTF, le format principal - et obligatoire - restant
l’ASCII. Le tout représente 46.000 fichiers, soit une capacité totale
de 110 gigaoctets. Le 13 février 2004, date de la conférence de Michael
Hart au siège de l’UNESCO à Paris, les collections comprennent très
exactement 11.340 livres dans 25 langues. En mai 2004, les 12.500
livres disponibles représentent 100.000 fichiers dans vingt formats
différents, soit une capacité totale de 135 Go (giga-octets), destinée
à doubler chaque année avec l’ajout d'environ 300 livres par mois (338
livres en 2004).

Parallèlement, le Project Gutenberg Consortia Center (PGCC), qui avait
été lancé en 1997 pour rassembler des collections de livres numériques
déjà existantes et provenant de sources extérieures, est officiellement
affilié au Projet Gutenberg en 2003.

Par ailleurs, un projet européen est lancé à l’instigation du Projet
Rastko, basé à Belgrade, en Serbie. Distributed Proofreaders Europe
débute en décembre 2003, et Projet Gutenberg Europe en janvier 2004,
avec cent livres disponibles en avril 2005. Les livres sont en
plusieurs langues pour refléter la diversité linguistique prévalant en
Europe, avec cent langues prévues sur le long terme.

En janvier 2005, le Projet Gutenberg fête ses 15.000 livres, avec la
mise en ligne de "The Life of Reason" de George Santayana (paru en
1906).

En juin 2005, le nombre de livres s’élève à 16.000. Si 25 langues
seulement étaient présentes en février 2004, 42 langues sont
représentées en juin 2005, dont l’iroquois, le sanscrit et les langues
mayas.En décembre 2006, on compte 50 langues. A la date du 16 décembre
2006, les langues comprenant plus de 50 titres sont l’anglais (17.377
livres), le français (966 titres), l’allemand (412 titres), le finnois
(344 titres), le hollandais (244 titres), l’espagnol (140 titres),
l’italien (102 titres), le chinois (69 titres), le portugais (68
titres) et le tagalogue (51 titres).

Lancé en août 2001, le Project Gutenberg Australia fête ses 500 livres
en juillet 2005, tandis que le Project Gutenberg Canada est en
gestation, tout comme un Projet Gutenberg au Portugal et un autre aux
Philippines.

En décembre 2006, le Projet Gutenberg franchit la barre des 20.000
livres. Le 20.000e livre est un livre audio, "Twenty Thousand Leagues
Under the Sea", version anglaise de "Vingt mille lieues sous les mers
de Jules Verne" (publié en 1869). La moyenne est de 345 nouveaux livres
par mois en 2006.

S'il a fallu 32 ans, de juillet 1971 à octobre 2003, pour numériser les
10.000 premiers livres, il n’aura fallu que trois ans et deux mois,
d’octobre 2003 à décembre 2006, pour numériser les 10.000 livres
suivants.

A la même date, le Project Gutenberg Australia approche les 1.500
livres (c'est chose faite en avril 2007) et le Projet Gutenberg Europe
compte 400 livres.

La section Project Gutenberg PrePrints débute en janvier 2006 pour
accueillir de nouveaux documents suffisamment intéressants pour être
mis en ligne, mais ne pouvant être intégrés aux collections existantes
sans traitement ultérieur par des volontaires, pour diverses raisons:
collections incomplètes, qualité insuffisante, conversion souhaitée
dans un autre format, etc. Cette section comprend 379 titres en
décembre 2006.

# Des dizaines de milliers d'ebooks

Project Gutenberg News débute en novembre 2006 à l’instigation de Mike
Cook. Il s'agit d'un site web qui complète la lettre d’information
hebdomadaire et mensuelle existant depuis nombre d'années. Le site
offre par exemple les statistiques de production hebdomadaires,
mensuelles et annuelles depuis 2001. La production hebdomadaire est de
24 livres en 2001, 47 livres en 2002, 79 livres en 2003, 78 livres en
2004, 58 livres en 2005, 80 livres en 2006 et 78 livres en 2007. La
production mensuelle est de 104 livres en 2001, 203 livres en 2002, 348
livres en 2003, 338 livres en 2004, 252 livres en 2005, 345 livres en
2006 et 338 livres en 2007. La production annuelle est de 1.244 livres
en 2001, 2.432 livres en 2002, 4.176 livres en 2003, 4.058 livres en
2004, 3.019 livres en 2005, 4.141 livres en 2006 et 4.049 livres en
2007.

Le Projet Gutenberg Canada (PGC) voit le jour le 1er juillet 2007, le
jour de la fête nationale, à l'instigation de Michael Shepard et David
Jones. Il est suivi de Distributed Proofreaders Canada (DPC), avec une
production qui débute en décembre 2007. Les cent premiers livres sont
disponibles en mars 2008, avec des livres en anglais, en français et en
italien.

Distributed Proofreaders (DP), lancé en octobre 2000, comptabilise
52.000 volontaires en janvier 2008, avec un nombre total de 11.950
livres traités en sept ans et trois mois. Distributed Proofreaders
Europe (DP Europe), lancé en décembre 2003, comptabilise 1.500
volontaires. Distributed Proofreaders Canada (DPC), lancé en décembre
2007, comptabilise 250 volontaires.

Le Projet Gutenberg franchit la barre des 25.000 livres en avril 2008.
Le 25.000e livre est "English Book Collectors", de William Younger
Fletcher (publié en 1902).

Le Projet Gutenberg comptabilise 32.500 ebooks le 1er mars 2009 pour
l'ensemble de ses sites, avec 28.147 ebooks pour le Project Gutenberg
USA, 1.750 ebooks pour le Project Gutenberg Australia, 600 ebooks pour
le Project Gutenberg Europe et 250 ebooks pour le Project Gutenberg
Canada, auxquels il convient d'ajouter les 2.020 ebooks de la section
PrePrints. Le Project Gutenberg Consortia Center (PGCC) – qui rassemble
des collections de livres numérisés par d'autres sources - comptabilise
75.000 ebooks à la même date.


= Du passé vers l'avenir

Le pari fait par Michael Hart en 1971 est donc réussi, avec une
progression assez impressionnante si on pense au nombre de pages relues
et corrigées: 10 livres en août 1989, 100 livres en janvier 1994, 1.000
livres en août 1997, 2.000 livres en mai 1999, 3.000 livres en décembre
2000, 4.000 livres en octobre 2001, 5.000 livres en avril 2002, 10.000
livres en octobre 2003, 15.000 livres en janvier 2005, 20.000 livres en
décembre 2006 et 25.000 livres en avril 2008.

Mais les résultats du Projet Gutenberg ne se mesurent pas seulement à
ces chiffres, qui restent assez modestes par rapport au nombre de
livres imprimés appartenant au domaine public. Les résultats se
mesurent également à l’influence du projet, qui est considérable.
Premier site d’information sur l’internet et première bibliothèque
numérique, le Projet Gutenberg a inspiré bien d’autres bibliothèques
numériques au fil des ans, par exemple le Projekt Runeberg pour la
littérature scandinave ou le Projekt Gutenberg-DE pour la littérature
allemande, pour n’en citer que deux.

La structure administrative et financière du Projet Gutenberg se limite
au strict minimum, avec une devise qui tient en trois mots: «Less is
more.» Michael Hart insiste régulièrement sur la nécessité d’un cadre
aussi souple que possible laissant toute initiative aux volontaires, et
la porte grande ouverte aux idées nouvelles. Le but est d’assurer la
pérennité du projet indépendamment des crédits, des coupures de crédits
et des priorités culturelles, financières et politiques du moment. Pas
de pression possible donc par le pouvoir et par l’argent. Et respect à
l’égard des volontaires, qui sont assurés de voir leur travail utilisé
pendant de nombreuses années, si ce n’est pour plusieurs générations,
d’où l’intérêt d’un format numérique qui soit toujours valable dans
quelques siècles. Le suivi régulier du projet est assuré grâce à une
lettre d’information hebdomadaire et mensuelle, des forums de
discussion, des wikis et des blogs.

Les dons servent à financer des ordinateurs et des scanners, et à
envoyer des CD et DVD gratuits à tous ceux qui en font la demande.
Suite au CD Best of Gutenberg disponible en août 2003 avec une
sélection de 600 titres et à un premier DVD disponible en décembre 2003
avec 9.400 titres, un deuxième DVD est disponible en juillet 2006 avec
17.000 titres. A partir de 2005, CD et DVD sont disponibles sous forme
d'images ISO sur le site de BitTorrent, ces images pouvant être
téléchargées pour graver des CD et DVD sur place à titre personnel. En
2007, le Projet Gutenberg envoie 15 millions de livres par voie postale
sous forme de CD et DVD.

Chose souvent passée sous silence, Michael Hart est le véritable
inventeur de l’ebook. Si on considère l’ebook dans son sens
étymologique, à savoir un livre numérisé pour diffusion sous forme de
fichier électronique, celui-ci aurait bientôt quarante ans et serait né
avec le Projet Gutenberg en juillet 1971. Une paternité beaucoup plus
réconfortante que les divers lancements commerciaux dans un format
propriétaire ayant émaillé le début des années 2000. Il n’y a aucune
raison pour que la dénomination «ebook» ne désigne que l’ebook
commercial et soit réservée aux Amazon, Barnes & Noble, 00h00, Gemstar
et autres. L’ebook non commercial est un ebook à part entière - et non
un parent pauvre - tout comme l’édition électronique non commerciale
est une forme d’édition à part entière - et tout aussi valable que
l’édition commerciale. En 2003, les etexts du Projet Gutenberg
deviennent des ebooks, pour coller à la terminologie ambiante.

En juillet 1971, l’envoi d’un fichier de 5 Ko à cent personnes aurait
fait sauter l’embryon de réseau disponible à l’époque. En novembre
2002, le Projet Gutenberg peut mettre en ligne les 75 fichiers du
"Human Genome Project" (à savoir le séquençage du génome humain),
chaque fichier se chiffrant en dizaines sinon en centaines de méga-
octets. Ceci peu de temps après la parution initiale du Human Genome
Project en février 2001, puisqu’il appartient d’emblée au domaine
public.

En 2004, la capacité de stockage des disques durs est telle qu’il
serait possible de faire tenir l’intégralité de la Library of Congress
au format texte sur un support de stockage coûtant 140 dollars US. Et
quelques années seulement nous sépareraient d’une clé USB (universal
serial bus) permettant de stocker l’intégralité du patrimoine écrit de
l’humanité.

Qu’en est-il des documents autres que l’écrit? En septembre 2003, le
Projet Gutenberg se lance dans la diffusion de livres audio. En
décembre 2006, on compte 367 livres lus par une synthèse vocale (Audio
Book, computer-generated) et 132 livres lus par l’être humain (Audio
Book, human-read). Le nombre de ces derniers devrait régulièrement
augmenter. Par contre, les livres lus par une synthèse vocale ne seront
plus être stockés dans une section spécifique, mais réalisés à la
demande à partir des fichiers électroniques existant dans les
collections générales. Les lecteurs aveugles ou malvoyants pourront à
l'avenir utiliser une commande vocale pour demander le fichier de tel
ou tel livre.

Lancée elle aussi en septembre 2003, la section "Sheet Music
Subproject" est consacrée aux partitions musicales numérisées (Music,
Sheet). Elle est complétée par une section d’enregistrements musicaux
(Music, recorded). Des sections sont également disponibles pour les
images fixes (Pictures, still) et animées (Pictures, moving). Ces
collections devraient être développées dans les prochaines années.

Mais la numérisation des livres reste prioritaire. Et la demande est
énorme. En témoigne le nombre de téléchargements, qui se comptent
désormais en dizaines de milliers par jour. A la date du 31 juillet
2005, on compte 37.532 fichiers téléchargés dans la journée, 243.808
fichiers téléchargés dans la semaine et 1.154.765 fichiers téléchargés
dans le mois. A la date du 6 mai 2007, on compte 89.841 fichiers
téléchargés dans la journée, 697.818 fichiers téléchargés dans la
semaine et 2.995.436 fichiers téléchargés dans le mois. Courant mai, ce
nombre atteint les 3 millions. Ceci uniquement pour le principal site
de téléchargement, ibiblio.org (basé à l’Université de Caroline du
Nord, Etats-Unis), qui héberge aussi le site du Projet Gutenberg. Le
deuxième site de téléchargement est l’Internet Archive, qui est le site
de sauvegarde et qui met à la disposition du Projet Gutenberg une
capacité de stockage illimitée.

Un Top 100 recense les cent titres et les cent auteurs les plus
téléchargés dans la journée, dans la semaine et dans le mois.

Le Projet Gutenberg dispose de 40 sites miroirs répartis dans de
nombreux pays, et il en cherche d’autres. La circulation des fichiers
se fait aussi en mode P2P (peer-to-peer), qui permet d’échanger des
fichiers directement d’un utilisateur à l’autre.

Les livres du Projet Gutenberg peuvent aider à combler la fracture
numérique. Ils sont aisément téléchargeables sur PDA. Un ordinateur ou
un PDA d’occasion ne coûte que quelques dollars ou quelques dizaines de
dollars, en fonction du modèle. Certains PDA fonctionnent à l’énergie
solaire, permettant la lecture dans les régions pauvres et reculées.

Plus tard, il sera peut-être possible d'envisager une traduction
simultanée dans une centaine de langues, en utilisant un logiciel de
traduction automatique qui aurait alors un taux de fiabilité de l’ordre
de 99%, un pourcentage dont on est encore loin. Ce logiciel de
traduction automatique serait relayé par des traducteurs (non pas des
machines, mais des êtres humains), sur un modèle comparable à la
technologie OCR relayée par des correcteurs (non pas des logiciels,
mais des êtres humains) pour offrir un contenu de grande qualité.

38 ans après les débuts du Projet Gutenberg, Michael Hart se définit
toujours comme un fou de travail dédiant toute sa vie à son projet,
qu’il voit comme étant à l’origine d’une révolution néo-industrielle.
Il se définit aussi comme altruiste, pragmatique et visionnaire. Après
avoir été traité de toqué pendant de nombreuses années, il force
maintenant le respect.

Au fil des ans, la mission du Projet Gutenberg reste la même, à savoir
changer le monde par le biais de l’ebook gratuit indéfiniment
utilisable et reproductible, et favoriser ainsi la lecture et la
culture pour tous à moindres frais. Cette mission se résume en quelques
mots: «encourager la création et la distribution d’ebooks», par autant
de personnes que possible, et par tous les moyens. Tout en prenant les
virages nécessaires pour intégrer de nouvelles idées, de nouvelles
méthodes et de nouveaux supports.



1990: LE WEB BOOSTE L'INTERNET


= [Résumé]

Vinton Cerf est souvent appelé le père de l’internet parce qu’il crée
en 1974 (avec Bob Kahn) le protocole TCP/IP (transmission control
protocol/internet protocol), à la base de tout échange de données.
L'internet se développe à partir de 1983. Le web est conçu en 1989-90
par Tim Berners-Lee, alors chercheur au CERN (Centre européen pour la
recherche nucléaire) à Genève. En 1989, Tim Berners-Lee met en réseau
des documents utilisant l'hypertexte. En 1990, il met au point le
premier serveur HTTP (hypertext transfert protocol) et le premier
navigateur web. En 1991, le web est opérationnel et change radicalement
l'utilisation de l'internet. Le web prend son essor en novembre 1993
grâce à Mosaic, premier navigateur à destination du grand public.
Quinze ans après la création du web, le magazine Wired constate dans
son numéro d'août 2005 que «moins de la moitié du web est commercial,
le reste fonctionne avec la passion». Quant à l'internet, quelque
trente ans après ses débuts, «ses trois pouvoirs - l'ubiquité, la
variété et l'interactivité - rendent son potentiel d'usages quasi
infini» (Le Monde, 19 août 2005).


= L'internet et le web

Apparu en 1974, l’internet est d’abord un phénomène expérimental
enthousiasmant quelques «branchés». A partir de 1983, il relie les
centres de recherche et les universités. Suite à l’apparition du web en
1990 et du premier navigateur en 1993, il envahit notre vie
quotidienne. Les signes cabalistiques des adresses web fleurissent sur
les livres, les journaux, les affiches et les publicités.

La presse s’enflamme pour ce nouveau médium. L'internet est défini
comme un ensemble de réseaux commerciaux, réseaux publics, réseaux
privés, réseaux d'enseignement, réseaux de services, etc., qui opèrent
à l'échelle planétaire pour offrir d'énormes ressources en information
et en communication. On nous promet l'internet dans tous les foyers. On
parle de mariage de l'ordinateur et de la télévision avec écrans
interchangeables ou intégrés, et d'accès à l'internet par le même biais
que la télévision câblée.

La majuscule d’origine d’Internet s’estompe. Internet devient
l’internet, avec un «i» minuscule. De nom propre il devient nom commun,
au même titre que l’ordinateur, le téléphone, le fax et le minitel. La
même remarque vaut pour le World Wide Web, qui devient tout simplement
le web.

Une définition officielle de l'internet est entérinée en octobre 1995
aux Etats-Unis par une résolution du Federal Networking Council (FNC),
en consultation avec les différentes communautés d'internautes et les
organismes défendant la propriété intellectuelle. L'internet est défini
comme un système d'information global obéissant aux trois
caractéristiques suivantes: (a) des adresses d'un type unique basées
sur le protocole IP (internet protocol) ou ses extensions, (b) des
communications utilisant le TCP/IP (transmission control
protocol/internet protocol), ses extensions ou des protocoles
compatibles, (c) la mise à disposition de services publics ou privés à
partir de ces infrastructures.

C'est le web qui rend l'internet très populaire et qui permet sa
gigantesque progression. Directeur de l'Internet Activities Board
(IAB), Christian Huitema explique que le World Wide Web «repose sur
trois idées principales, la navigation par "hypertexte", le support du
multimédia, et l'intégration des services préexistants».

Plus communément appelé web, Web, WWW ou W3, le World Wide Web est créé
par Tim Berners-Lee en 1989-1990 au CERN (Centre européen pour la
recherche nucléaire) à Genève, en Suisse. Le web révolutionne la
consultation de l'internet en permettant la publication de documents au
moyen du système hypertexte, à savoir un ensemble de liens hypertextes
permettant de passer d'un document textuel ou visuel à l'autre au moyen
d'un simple clic de souris. Devenue véritablement interactive,
l'information devient soudain beaucoup plus attractive.

Un site web est le plus souvent formé d'un ensemble de pages-écran
reliées entre elles par des liens hypertextes, qui sont en général
soulignés et d'une couleur différente de celle du texte. Grâce à un
simple clic, l'utilisateur est renvoyé soit à une autre partie du
document, soit à un autre document du site, soit à un autre site. Cette
interactivité est ensuite encore accrue par la possibilité de liens
hypermédia permettant de lier des textes et des images avec des
graphiques, vidéos ou bandes sonores.

Comme on le voit, le web est très postérieur à l'internet, réseau
informatique global mis sur pied en 1974 et connectant universités et
centres de recherche depuis 1983. Et même si, improprement, on les
considère souvent comme synonymes, le web n'est qu'un des aspects de
l'internet, qui englobe plusieurs autres services: courriel, gopher,
telnet (terminal network protocol), FTP (file transfer protocol), IRC
(internet relay chat), forums de discussion, messagerie instantanée,
visioconférence, téléphonie sur IP (internet protocol), etc.

Le web bénéficie logiquement de l'infrastructure internet,
particulièrement aux Etats-Unis et au Canada. A la question posée en
décembre 1997 par Pierre Ruetschi, journaliste à la Tribune de Genève,
un quotidien suisse: «Pourquoi l'Europe a-t-elle accumulé un tel retard
sur les Etats-Unis en matière de présence et de développement sur
l'internet?», Tim Berners-Lee répond en expliquant l'avance des Etats-
Unis par les énormes investissements faits par l'État. Il insiste aussi
sur l'avance technologique de l'Europe dans plusieurs domaines:
minitel, cartes à puce, téléphones cellulaires, etc.

On se plaint souvent de l'hégémonie américaine alors que il s'agit
surtout d'une avance technique. Malgré tous les efforts des
«dynosaures» politiques et commerciaux, il est impossible à quelque
pays ou à quelque communauté que ce soit de «mettre la main» sur le
web, ou du moins de le contrôler totalement.

Développé par le NSCA (National Center for Supercomputing Applications)
à l'Université d'Illinois et distribué gratuitement à partir de
novembre 1993, Mosaic est le premier logiciel de navigation destiné au
grand public et contribue largement au développement rapide du web.
Début 1994, une partie de l'équipe de Mosaic émigre dans la Netscape
Communications Corporation pour commercialiser son logiciel sous le nom
de Nescape Navigator. En 1995, pour concurrencer le Netscape Navigator,
Microsoft crée l'Internet Explorer. Viennent ensuite d'autres
navigateurs, comme Opera ou Safari.

Deux étudiants de l'Université de Stanford (Californie), Jerry Lang et
David Filo, lancent en janvier 1994 l'annuaire Yahoo! pour recenser les
sites web et les classer par thèmes. L'annuaire est un succès, avec un
classement plus pointu que celui de moteurs de recherche comme
AltaVista, où ces tâches sont entièrement automatisées. Divisé en 63
grandes catégories (en 1998), Yahoo! offre une interface en plusieurs
langues: anglais, allemand, coréen, français, japonais, norvégien et
suédois. Yahoo! travaille d'ailleurs de concert avec AltaVista. Quand
une recherche ne donne pas de résultat dans l'un, elle est
automatiquement aiguillée sur l'autre.

En décembre 1997, AltaVista propose AltaVista Translation, un service
de traduction automatisée de l'anglais vers les langues suivantes:
allemand, espagnol, français, italien et portugais, et vice versa. Bien
qu'ayant ses limites, avec une traduction de trois pages maximum et un
texte traduit très approximatif, ce service est immédiatement très
apprécié. De plus, il ouvre la voie à d'autres services du même genre
et contribue grandement au plurilinguisme du web.


= Quelques concepts

L'internet est bien plus qu'une invention purement technique. Sur le
site de l'Internet Society, organisme professionnel international fondé
en 1992 pour coordonner et promouvoir le développement de l'internet,
le document "The Brief History of Internet" propose de l'internet une
triple définition. L'internet est: (a) un instrument de diffusion
internationale, (b) un mécanisme de diffusion de l'information, (c) un
moyen de collaboration et d'interaction entre les individus et les
ordinateurs, indépendamment de leur situation géographique.

Selon ce document, bien plus que toute autre invention (télégraphe,
téléphone, radio ou ordinateur), l'internet révolutionne de fond en
comble le monde des communications. Il représente l'un des exemples les
plus réussis d'interaction entre un investissement soutenu dans la
recherche et le développement d'une infrastructure de l'information,
tous deux l'objet d'un réel partenariat entre les gouvernements, les
universités et les entreprises.

Sur le site du World Wide Web Consortium (W3C), fondé en octobre 1994
pour développer les protocoles communs nécessaires au web, l'écrivain
Bruce Sterling décrit le développement spectaculaire de l'internet dans
le document "Short History of the Internet". L'internet se développe
plus vite que les téléphones cellulaires ou les télécopieurs. En 1996,
sa croissance est de 20% par mois. Le nombre des machines ayant une
connexion directe TCP/IP a doublé depuis 1988. D'abord présent dans
l'armée et les instituts de recherche, l'internet déferle dans les
écoles, les universités et les bibliothèques, et il est également pris
d'assaut par le secteur commercial.

Bruce Sterling s'intéresse aussi aux raisons pour lesquelles on se
connecte à l'internet. Une des raisons essentielles lui semble être la
liberté. L'internet est un exemple d'«anarchie réelle, moderne et
fonctionnelle». Il n'y a pas de censeurs officiels (tout au moins les
premières années, NDLR), de patrons, de comités de direction ou
d'actionnaires. Toute personne peut parler d'égale à égale avec une
autre, du moment qu'elle se conforme aux protocoles TCP/IP, des
procotoles qui ne sont pas sociaux ou politiques mais strictement
techniques.

Bruce Sterling indique enfin que l'internet est aussi une bonne affaire
commerciale. Contrairement à la téléphonie traditionnelle (de l'époque,
NDLR), il n'y a pas de frais longue distance. Et, contrairement aux
réseaux informatiques commerciaux, il n'y pas de frais d'accès. En
fait, l'internet, qui n'existe même pas officiellement en tant
qu'entité, n'a pas de facturation propre. Chaque groupe ayant accès à
l'internet est responsable de ses propres machines et de ses propres
connexions.

L'internet concurrence-t-il la télévision et de la lecture? se
demandent avec inquiétude les médias traditionnels. Au Québec, où 30,7%
de la population est connectée à l'internet en mars 1998, un sondage
réalisé par l'institut Som pour le magazine en ligne Branchez-vous!
indique que 28,8% des Québécois connectés regardent moins la télévision
qu'avant. Par contre, seuls 12,1% lisent moins, ce qui, d'après le
quotidien en ligne Multimédium, est «plutôt encourageant pour le
ministère de la Culture et des Communications qui a la double tâche de
favoriser l'essor de l'inforoute et celui... de la lecture!»

En France, lors d'un entretien avec Annick Rivoire publié dans le
quotidien Libération du 16 janvier 1998, le philosophe Pierre Lévy
explique que l'internet va contribuer à la fin des monopoles: «Le
réseau désenclave, donne plus de chance aux petits. On crie "ah! le
monopole de Microsoft", mais on oublie de dire que l'internet sonne la
fin du monopole de la presse, de la radio et de la télévision, et de
tous les intermédiaires.» Pierre Lévy définit aussi ce qu'il appelle
«l'intelligence collective»: «Les réseaux permettent de mettre en
commun nos mémoires, nos compétences, nos imaginations, nos projets,
nos idées, et de faire en sorte que toutes les différences, les
singularités se relancent les unes les autres, entrent en
complémentarité, en synergie.»

Le philosophe Timothy Leary constate en 1994 dans son livre "Chaos et
cyberculture": «Jamais l'individu n'a eu à sa portée un tel pouvoir.
Mais, à l'âge de l'information, il faut saisir les signaux. Populariser
signifie "rendre accessible au peuple". Aujourd'hui, le rôle du
philosophe est de personnaliser, de populariser et d'humaniser les
concepts informatiques, de façon à ce que personne ne se sente exclu.»

Il nous faut cependant garder la tête froide. Pour contrer à la fois
ceux qui mettent les technologies sur un piédestal et ceux qui y sont
systématiquement hostiles, un mouvement appelé Technorealism est lancé
sur le web en mars 1998 aux Etats-Unis. Les idées émises dans
"Technorealism Overview" sont ensuite reprises au Québec dans le
"Manifeste pour un technoréalisme". Ce manifeste s'appuie sur les huit
principes suivants: (1) les technologies ne sont pas neutres, (2)
l'internet est un média révolutionnaire, mais ce n'est pas une utopie,
(3) le gouvernement a un rôle important à jouer dans le cyberespace,
(4) l'information n'est pas un gage de connaissance, (5) brancher les
écoles n'assurera pas une éducation de meilleure qualité, (6)
l'information doit être protégée (en relation avec le droit d'auteur,
NDLR), (7) les ondes sont du domaine public et c'est le public qui
devrait en tirer les bénéfices, (8) une bonne compréhension des
technologies devrait constituer un des fondements de la citoyenneté.

Selon ce manifeste, «plus le cyberespace devient populaire, plus il
ressemble à la société réelle dans toute sa complexité. Chacun des
côtés positifs ou habilitants de la vie en ligne est accompagné de
dimensions malicieuses, perverses. (...) Contrairement à ce que
certains prétendent, le cyberespace n'est pas un lieu distinct qui
serait régi par des règles distinctes de celles de la société civile.
Les gouvernements doivent respecter les règles et coutumes nées avec le
cyberespace, mais cela ne veut pas dire pour autant que le public n'a
aucun droit sur un citoyen qui déraille ou une entreprise qui commet
une fraude. En tant que représentant du peuple et gardien des valeurs
démocratiques, l'État a le droit et la responsabilité d'aider à
intégrer le cyberespace à la société civile. (...) Peu importe la
puissance de nos ordinateurs, nous ne devrions jamais nous en servir
pour pallier la lucidité, le raisonnement et le jugement.»

Le web est toutefois une formidable aventure. Selon les termes mêmes de
Tim Berners-Lee, son inventeur, «le rêve derrière le web est un espace
d’information commun dans lequel nous communiquons en partageant
l’information. Son universalité est essentielle, à savoir le fait qu’un
lien hypertexte puisse pointer sur quoi que ce soit, quelque chose de
personnel, de local ou de global, aussi bien une ébauche qu’une
réalisation très sophistiquée. Deuxième partie de ce rêve, le web
deviendrait d'une utilisation tellement courante qu'il serait un miroir
réaliste (sinon la principale incarnation) de la manière dont nous
travaillons, jouons et nouons des relations sociales. Une fois que ces
interactions seraient en ligne, nous pourrions utiliser nos ordinateurs
pour nous aider à les analyser, donner un sens à ce que nous faisons,
et voir comment chacun trouve sa place et comment nous pouvons mieux
travailler ensemble.» ("The World Wide Web: A very short personal
history", avril 1998, disponible sur le site web du W3C)



1993: L'ONLINE BOOKS PAGE RECENSE LES EBOOKS GRATUITS


= [Résumé]

L'Online Books Page est créée en janvier 1993 par John Mark Ockerbloom
pour répertorier les textes électroniques anglophones du domaine public
en accès libre sur le web. A cette date, John Mark est doctorant à
l’Université Carnegie Mellon (Etats-Unis). En 1999, il rejoint
l’Université de Pennsylvanie pour travailler à la R&D (recherche et
développement) de la bibliothèque numérique. A la même époque, il y
transfère l'Online Books Page tout en gardant la même présentation,
très sobre, et tout en poursuivant son travail d’inventaire dans le
même esprit. En 2003, ce répertoire fête ses dix ans d'existence et
recense plus de 20.000 textes électroniques, dont 4.000 textes publiés
par des femmes. En décembre 2006, il recense 25.000 titres. Fin 2007,
il compte 30.000 titres, dont 7.000 titres du Projet Gutenberg.


= [Texte]

Alors que certains numérisent les oeuvres du domaine public, comme le
Projet Gutenberg et des projets connexes, d'autres se donnent pour
tâche de répertorier celles qui sont en accès libre sur le web, en
offrant au lecteur un point d’accès commun. C’est le cas de John Mark
Ockerbloom, doctorant à l’Université Carnegie Mellon (Pittsburgh,
Pennsylvanie, Etats-Unis), qui crée l’Online Books Page pour recenser
les oeuvres anglophones.

Cinq ans plus tard, en septembre 1998, John Mark relate: «J’étais
webmestre ici pour la section informatique de la CMU (Carnegie Mellon
University), et j’ai débuté notre site local en 1993. Il comprenait des
pages avec des liens vers des ressources disponibles localement, et à
l’origine l’Online Books Page était l’une de ces pages, avec des liens
vers des livres mis en ligne par des collègues de notre département
(par exemple Robert Stockton, qui a fait des versions web de certains
textes du Projet Gutenberg). Ensuite les gens ont commencé à demander
des liens vers des livres disponibles sur d’autres sites. J’ai remarqué
que de nombreux sites (et pas seulement le Projet Gutenberg ou Wiretap)
proposaient des livres en ligne, et qu’il serait utile d’en avoir une
liste complète qui permette de télécharger ou de lire des livres où
qu’ils soient sur l’internet. C’est ainsi que mon index a débuté. J’ai
quitté mes fonctions de webmestre en 1996, mais j’ai gardé la gestion
de l’Online Books Page, parce qu’entre temps je m’étais passionné pour
l’énorme potentiel qu’a l’internet de rendre la littérature accessible
au plus grand nombre. Maintenant il y a tant de livres mis en ligne que
j’ai du mal à rester à jour. Je pense pourtant poursuivre cette
activité d’une manière ou d’une autre. Je suis très intéressé par le
développement de l’internet en tant que médium de communication de
masse dans les prochaines années. J’aimerais aussi rester impliqué dans
la mise à disposition gratuite de livres sur l’internet, que ceci fasse
partie intégrante de mon activité professionnelle, ou que ceci soit une
activité bénévole menée sur mon temps libre.»

Fin 1998, John Mark Ockerbloom obtient son doctorat en informatique. En
1999, il rejoint l’Université de Pennsylvanie, où il travaille à la R&D
(recherche et développement) de la bibliothèque numérique. A la même
époque, il y transfère l’Online Books Page tout en gardant la même
présentation, très sobre, et tout en poursuivant son travail
d’inventaire dans le même esprit. Ce répertoire recense 12.000 textes
en ligne en 1999, 20.000 textes en 2003, dont 4.000 textes publiés par
des femmes, 25.000 textes en 2006 et 30.000 textes en 2007, dont 7.000
textes du Projet Gutenberg.

En 1999, le débat fait rage sur le durcissement de la loi de 1976 sur
le copyright par un amendement daté du 27 octobre 1998. De nombreuses
oeuvres censées tomber dans le domaine public restent désormais sous
copyright, au grand dam de Michael Hart, fondateur du Projet Gutenberg,
de John Mark Ockerbloom et de bien d'autres.

John Mark explique en août 1999: «A mon avis, il est important que les
internautes comprennent que le copyright est un contrat social conçu
pour le bien public - incluant à la fois les auteurs et les lecteurs.
Ceci signifie que les auteurs doivent avoir le droit d'utiliser de
manière exclusive et pour un temps limité les oeuvres qu'ils ont
créées, comme ceci est spécifié dans la loi actuelle sur le copyright.
Mais ceci signifie également que leurs lecteurs ont le droit de copier
et de réutiliser ce travail autant qu'ils le veulent à l'expiration de
ce copyright. Aux Etats-Unis, on voit maintenant diverses tentatives
visant à retirer ces droits aux lecteurs, en limitant les règles
relatives à l'utilisation de ces oeuvres, en prolongeant la durée du
copyright (y compris avec certaines propositions visant à le rendre
permanent) et en étendant la propriété intellectuelle à des travaux
distincts des oeuvres de création (comme on en trouve dans les
propositions de copyright pour les bases de données). Il existe même
des propositions visant à entièrement remplacer la loi sur le copyright
par une loi instituant un contrat beaucoup plus lourd. Je trouve
beaucoup plus difficile de soutenir la requête de Jack Valenti,
directeur de la MPAA (Motion Picture Association of America), qui
demande d'arrêter de copier les films sous copyright, quand je sais
que, si ceci était accepté, aucun film n'entrerait jamais dans le
domaine public (...). Si on voit les sociétés de médias tenter de
bloquer tout ce qu'elles peuvent, je ne trouve pas surprenant que
certains usagers réagissent en mettant en ligne tout ce qu'ils peuvent.
Malheureusement, cette attitude est à son tour contraire aux droits
légitimes des auteurs.»

Comment résoudre cela pratiquement? «Ceux qui ont des enjeux dans ce
débat doivent faire face à la réalité, et reconnaître que les
producteurs d'oeuvres et leurs usagers ont tous deux des intérêts
légitimes dans l'utilisation de celles-ci. Si la propriété
intellectuelle était négociée au moyen d'un équilibre des principes
plutôt que par le jeu du pouvoir et de l'argent que nous voyons
souvent, il serait peut-être possible d'arriver à un compromis
raisonnable.»



1994: DE PLUS EN PLUS DE TEXTES SONT DISPONIBLES EN LIGNE


= [Résumé]

Au début des années 1990, les premières éditions électroniques de
journaux sont disponibles par le biais de services commerciaux tels que
America Online ou CompuServe. Avec l'apparition du premier navigateur
fin 1993 et la croissance rapide du web qui s'ensuit, nombre de zines
non commerciaux naissent sous forme électronique, et les organes de
presse commerciaux créent aussi leurs propres sites. Par ailleurs,
certains éditeurs mettent certains de leurs titres sur le web, avec
accès libre et gratuit, dans l'espoir de voir les ventes des versions
imprimées augmenter. La NAP (National Academy Press) est la première à
prendre un tel risque, dès 1994, avec un pari gagné. Elle est suivie
par la MIT Press (MIT: Massachusetts Institute of Technology) en 1995.


= E-zines

Les premiers titres purement électroniques sont des oeuvres courtes,
répertoriées dans l’E-zine-list, une liste créée en été 1993 par John
Labovitz. Abrégé de fanzine ou magazine, un zine est généralement
l’oeuvre d’une personne ou d’un petit groupe. Quant au e-zine, abrégé
de zine électronique, il est uniquement diffusé par courriel ou sur un
site web. Le plus souvent, il ne contient pas de publicité, ne vise pas
un profit commercial et n’est pas dirigé vers une audience de masse.

Comment l’E-zine-list débute-t-elle? Dans l’historique présent sur le
site, John Labovitz relate qu’à l’origine son intention est de faire
connaître Crash, un zine imprimé dont il souhaite faire une version
électronique. A la recherche de répertoires, il ne trouve que le groupe
de discussion Alt.zines, et des archives comme The Well et The Etext
Archives. Lui vient alors l’idée d’un répertoire organisé. Il commence
avec douze titres classés manuellement sur un traitement de texte. Puis
il écrit sa propre base de données. En quatre ans, de 1993 à 1997, les
quelques dizaines d'e-zines deviennent plusieurs centaines, et la
signification même d’e-zine s’élargit pour recouvrir tout type de
publication publiée par voie électronique, même s’«il subsiste toujours
un groupe original et indépendant désormais minoritaire qui continue de
publier suivant son coeur ou de repousser les frontières de ce que nous
appelons un e-zine». En été 1998, l’E-zine-list comprend 3.000 titres.


= La presse en ligne

Le développement de la presse en ligne (dans les années 1990) est
intéressant parce qu’il préfigure celui du livre en ligne (dans les
années 2000).

Au début des années 1990, les premières éditions électroniques de
journaux sont disponibles par le biais de services commerciaux tels que
America Online ou CompuServe. Suite à l'apparition du premier
navigateur fin 1993 et à la croissance rapide du web qui s'ensuit, les
organes de presse créent leurs propres sites.

Au Royaume-Uni, le Times et le Sunday Times font web commun sur un site
dénommé Times Online, avec possibilité de créer une édition
personnalisée.

Aux Etats-Unis, la version en ligne du Wall Street Journal est payante,
avec 100.000 abonnés en 1998. Celle du New York Times est disponible
sur abonnement gratuit. Le Washington Post propose l’actualité
quotidienne en ligne et de nombreux articles archivés, le tout avec
images, sons et vidéos. Pathfinder (rebaptisé ensuite Time) est le site
web du groupe Time-Warner, éditeur de Time Magazine, Sports
Illustrated, Fortune, People, Southern Living, Money, Sunset, etc. On
peut y lire les articles «maison» et les rechercher par date ou par
sujet. Lancé en 1992 en Californie, Wired, premier magazine imprimé
entièrement consacré à la culture cyber, est bien évidemment présent
sur le web.

Mis en ligne en février 1995, le site web du mensuel Le Monde
diplomatique est le premier site d’un périodique imprimé français.
Monté dans le cadre d’un projet expérimental avec l’Institut national
de l’audiovisuel (INA), ce site est inauguré lors du forum des images
Imagina. Il donne accès à l’ensemble des articles depuis janvier 1994,
par date, par sujet et par pays. L’intégralité du mensuel en cours est
consultable gratuitement pendant deux semaines suivant sa parution. Un
forum de discussion permet au journal de discuter avec ses lecteurs.

Fin 1995, le quotidien Libération met en ligne son site web, peu après
le lancement du Cahier Multimédia, un cahier imprimé hebdomadaire
inclus dans l’édition du jeudi. Le site propose la Une du quotidien, la
rubrique Multimédia (qui regroupe les articles du Cahier Multimédia et
les archives des cahiers précédents), le Cahier Livres complété par
Chapitre Un (le premier chapitre des nouveautés retenues par le
quotidien), et bien d’autres rubriques. La rubrique Multimédia est
ensuite rebaptisée Numériques.

Le site du quotidien Le Monde est lancé en 1996. On y trouve des
dossiers en ligne, la Une en version graphique à partir de 13 h,
l’intégralité du journal avant 17 h, l’actualité en liaison avec l’AFP
(Agence France-Presse), et des rubriques sur la Bourse, les livres, le
multimédia et les sports. En 1998, le journal complet en ligne coûte 5
FF (0,76 euros) alors que l’édition papier coûte 7,50 FF (1,15 euros).
S’ils concernent le multimédia, les articles du supplément imprimé
hebdomadaire Télévision-Radio-Multimédia sont disponibles gratuitement
en ligne dans la rubrique Multimédia, rebaptisée ensuite Nouvelles
technologies.

L’Humanité est le premier quotidien français à proposer la version
intégrale du journal en accès libre. Classés par rubriques, les
articles sont disponibles entre 10 h et 11 h du matin, à l’exception de
L’Humanité du samedi, disponible en ligne le lundi suivant. Tous les
articles sont archivés sur le site.

La presse régionale est tout aussi présente sur le web, par exemple
Dernières nouvelles d’Alsace et Ouest-France.

Lancé en septembre 1995, le site des Dernières nouvelles d’Alsace
propose l’intégrale de l’édition du jour ainsi que des informations
pratiques: cours de la Bourse, calcul des impôts, etc., avec 5.500
visites quotidiennes en juin 1998. Il offre aussi une édition abrégée
en allemand.

Le site web du quotidien Ouest-France est mis en ligne en juillet 1996.
D’abord appelé France-Ouest, le site est ensuite renommé Ouest-France,
du nom du journal.

Quelles sont les retombées de l’internet pour les journalistes? Selon
Bernard Boudic, le responsable éditorial du site, interviewé en juin
1998, «elles sont encore minces (en juin 1998, NDLR). Nous commençons
seulement à offrir un accès internet à chacun (rédaction d’Ouest-
France: 370 journalistes répartis dans soixante rédactions, sur douze
départements... pas simple). Certains utilisent internet pour la
messagerie électronique (courrier interne ou externe, réception de
textes de correspondants à l’étranger, envoi de fichiers divers) et
comme source d’informations. Mais cette pratique demande encore à
s’étendre et à se généraliser. Bien sûr, nous réfléchissons aussi à
tout ce qui touche à l’écriture multimédia et à sa rétro-action sur
l’écriture imprimée, aux changements d’habitudes de nos lecteurs, etc.
(...)

Internet est à la fois une menace et une chance. Menace sur l’imprimé,
très certainement (captation de la pub et des petites annonces,
changement de réflexes des lecteurs, perte du goût de l’imprimé,
concurrence d’un média gratuit, que chacun peut utiliser pour diffuser
sa propre info, etc.). Mais c’est aussi l’occasion de relever tous ces
défis, de rajeunir la presse imprimée.»

Tous sujets que l'on retrouve quelques années plus tard dans les débuts
du livre numérique: rapport accru de l'auteur avec ses lecteurs,
nécessité d'une formation technique, version payante et/ou version
gratuite, version numérique et/ou version imprimée, etc.


= Livres gratuits / payants

La publication en ligne d’un livre à titre gratuit nuit-elle aux ventes
de la version imprimée ou non? La National Academy Press (NAP) est la
première à prendre un tel risque, dès 1994, avec un pari gagné.

«A première vue, cela paraît illogique», écrit Beth Berselli,
journaliste au Washington Post, dans un article repris par le Courrier
international de novembre 1997. «Un éditeur de Washington, la National
Academy Press (NAP), qui a publié sur internet 700 titres de son
catalogue actuel, permettant ainsi à tout un chacun de lire
gratuitement ses livres, a vu ses ventes augmenter de 17% l’année
suivante. Qui a dit que personne n’achèterait la vache si on pouvait
avoir le lait gratuitement?»

Une politique atypique porte donc ses fruits. Editeur universitaire, la
National Academy Press (qui devient ensuite la National Academies
Press) publie environ 200 livres par an, essentiellement des ouvrages
scientifiques et techniques et des ouvrages médicaux. En 1994,
l'éditeur choisit de mettre en accès libre sur le web le texte intégral
de plusieurs centaines de livres, afin que les lecteurs puissent les
«feuilleter» à l’écran, comme ils l’auraient fait dans une librairie,
avant de les acheter ensuite si utile. La NAP est le premier éditeur à
se lancer dans un tel pari, une initiative saluée par les autres
maisons d’édition, qui hésitent cependant à se lancer elles aussi dans
l’aventure, et ce pour trois raisons: le coût excessif qu’entraîne la
mise en ligne de milliers de pages, les problèmes liés au droit
d’auteur, et enfin une «concurrence» qu’ils estiment nuisible à la
vente.

Dans le cas de la NAP, ce sont les auteurs eux-mêmes qui, pour mieux
faire connaître leurs livres, demandent à ce que ceux-ci soient mis en
ligne sur le site. Pour l’éditeur, le web est un nouvel outil de
marketing face aux 50.000 ouvrages publiés chaque année aux Etats-Unis.
Une réduction de 20% est accordée pour toute commande effectuée en
ligne. La présence de ces livres sur le web entraîne aussi une
augmentation des ventes par téléphone. En 1998, le site de la NAP
propose le texte intégral d’un millier de titres. La solution choisie
par la NAP est également adoptée dès 1995 par la MIT Press, qui voit
rapidement ses ventes doubler pour les livres disponibles en version
intégrale sur le web.



1995: AMAZON.COM EST LA PREMIERE GRANDE LIBRAIRIE EN LIGNE


= [Résumé]

En juillet 1995, Jeff Bezos fonde à Seattle (Etats-Unis) la librairie
en ligne Amazon.com, que le public appellera tout simplement Amazon.
Amazon débute avec dix salariés et trois millions d’articles, et
devient vite un géant du commerce électronique. Cinq ans plus tard, en
novembre 2000, la société compte 7.500 salariés, 28 millions
d’articles, 23 millions de clients et quatre filiales au Royaume-Uni
(filiale ouverte en octobre 1998), en Allemagne (filiale ouverte à la
même date), en France (filiale ouverte en août 2000) et au Japon
(filiale ouverte en novembre 2000). Une cinquième filiale est ouverte
au Canada (en juin 2002), suivie d’une sixième filiale, Joyo, en Chine
(en septembre 2004). Au 3e trimestre 2003, la société devient
bénéficiaire pour la première fois. Présent dans sept pays et devenu
une référence mondiale du commerce en ligne (avec eBay), Amazon fête
ses dix ans d’existence en juillet 2005, avec 9.000 salariés et 41
millions de clients actifs, attirés par des produits attirés par des
produits culturels, high-tech et autres aux prix attractifs et une
livraison en 48 heures maximum dans les pays hébergeant une plateforme
Amazon.


= Aux Etats-Unis

# Débuts

Un nouveau type de librairie naît sur le web au milieu des années 1990.
Ces librairies n’ont ni murs, ni vitrine, ni enseigne sur la rue, et
toutes leurs transactions s’effectuent via l'internet. C’est le cas
d’Amazon.com qui, sous la houlette de Jeff Bezos, ouvre ses portes
«virtuelles» en juillet 1995 avec un catalogue de trois millions de
livres et dix salariés basés à Seattle, dans l’Etat de Washington, sur
la côte ouest des Etats-Unis.

Quinze mois auparavant, au printemps 1994, Jeff Bezos fait une étude de
marché pour décider du meilleur «produit» à vendre sur l’internet. Dans
sa liste de vingt produits marchands, qui comprennent entre autres les
vêtements et les instruments de jardinage, les cinq premiers du
classement se trouvent être les livres, les CD, les vidéos, les
logiciels et le matériel informatique.

«J’ai utilisé tout un ensemble de critères pour évaluer le potentiel de
chaque produit, relate Jeff Bezos en 1997 dans le kit de presse
d’Amazon. Le premier critère a été la taille des marchés existants.
J’ai vu que la vente des livres représentait un marché mondial de
82 milliards de dollars US. Le deuxième critère a été la question du
prix. Je voulais un produit bon marché. Mon raisonnement était le
suivant: puisque c’était le premier achat que les gens allaient faire
en ligne, il fallait que la somme à payer soit modique. Le troisième
critère a été la variété dans le choix: il y avait trois millions de
titres pour les livres alors qu’il n’y avait que 300.000 titres pour
les CD, par exemple.»

# Expansion

Au printemps 1997, Amazon décide de s'inspirer du système d'«associés»
en ligne lancé quelques mois auparavant par la grande librairie en
ligne britannique Internet Bookshop. Tout possesseur d'un site web peut
vendre des livres appartenant au catalogue d'Amazon et toucher un
pourcentage de 15% sur les ventes. L'«associé(e)» sélectionne les
titres du catalogue qui l'intéressent, en fonction de ses centres
d'intérêt, et rédige ses propres résumés. Amazon reçoit les commandes
par son intermédiaire, expédie les livres, rédige les factures et lui
envoie un rapport hebdomadaire d'activité avec règlement correspondant.
Au printemps 1998, le réseau d'Amazon compte plus de 30.000 sites
affiliés.

A la même date, outre les livres, on trouve aussi des CD, des DVD, des
jeux informatiques, etc., avec un catalogue qui serait au moins dix
fois supérieur à celui des plus grandes chaînes de supermarchés. On
peut consulter le catalogue à l’écran, lire le résumé des livres
choisis ou même des extraits, puis passer sa commande en ligne. Très
attractif, le contenu éditorial du site change quotidiennement et se
veut un magazine littéraire en ligne, avec des conseils de lecture, des
articles émanant de journalistes connus (qui travaillaient auparavant
dans la presse imprimée), des entretiens avec des auteurs et des
commentaires de lecteurs.

L'évolution rapide d'Amazon en tant que pionnier d’un nouveau modèle
économique est suivie de près par des analystes de tous bords, tout
comme sa popularité auprès d'un public qui s'habitue aux achats en
ligne. En 1998, avec 1,5 million de clients dans 160 pays et une très
bonne image de marque, Amazon est régulièrement cité comme un symbole
de réussite dans le cybercommerce. Si la librairie en ligne est
toujours déficitaire, sa cotation boursière est excellente suite à une
introduction à la Bourse de New York en mai 1997.

Avant qu'Amazon n'assoie définitivement sa suprématie nationale, le
libraire en ligne se lance dans une guerre des prix avec son principal
concurrent aux Etats-Unis, Barnes & Noble.com, à la grande joie des
clients qui profitent de cette course aux rabais pour faire une
économie de 20 à 40% sur certains titres. Contrairement à Amazon,
librairie uniquement «virtuelle», Barnesandnoble.com s'appuie sur la
grande chaîne de librairies traditionnelles Barnes & Noble (B&N) qui,
en 1997, comprend 480 librairies «en dur» réparties dans tout le pays.
Barnes & Noble lance sa librairie en ligne en mai 1997, en partenariat
avec le géant des médias allemand Bertelsmann, mais rachètera la part
détenue par Bertelsmann (36,8%) en juillet 2003 pour 164 millions de
dollars US.


= En Europe

La présence européenne d’Amazon débute en octobre 1998, avec les deux
premières filiales implantées simultanément en Allemagne et au Royaume-
Uni.

En août 2000, Amazon compte 1,8 million de clients au Royaume-Uni, 1,2
million de clients en Allemagne et quelques centaines de milliers de
clients en France. Amazon ouvre sa troisième filiale européenne, Amazon
France, avec livres, musique, DVD et vidéos (auxquels viennent
s'ajouter logiciels et jeux vidéos en juin 2001), et livraison en 48
heures. A cette date, la vente de livres en ligne en France ne
représente que 0,5% du marché, contre 5,4% aux Etats-Unis.

Préparée dans le plus grand secret, l'ouverture d'Amazon France n'est
rendue publique que le 23 août 2000. Avec une centaine de salariés,
dont certains ont été envoyés en formation au siège du groupe à Seattle
(Etats-Unis), la filiale française s'installe à Guyancourt, en région
parisienne, pour l'administration, les services techniques et le
marketing. Son service de distribution est basé à Boigny-sur-Bionne,
dans la banlieue d'Orléans. Son service clients est basé à La Haye, aux
Pays-Bas, dans l'optique d'une expansion future d'Amazon en Europe.

Amazon France compte au moins quatre rivaux de taille dans l'hexagone:
Fnac.com, Alapage, Chapitre.com et BOL.fr.

Le service en ligne Fnac.com s'appuie sur le réseau des librairies
Fnac, réparti sur toute la France et dans quelques autres pays
européens, et qui appartient au groupe Pinault-Printemps-Redoute.

Alapage, librairie en ligne fondée en 1996 par Patrice Magnard, rejoint
le groupe France Télécom en septembre 1999 puis devient en juillet 2000
une filiale à part entière de Wanadoo, le fournisseur d’accès internet
de France Télécom.

Chapitre.com est une librairie en ligne indépendante créée en 1997 par
Juan Pirlot de Corbion.

BOL.fr est la succursale française de BOL.com (BOL: Bertelsmann On
Line), lancée en août 1999 par Bertelsmann, géant allemand des médias,
en partenariat avec Vivendi, multinationale française.

Un mois après son lancement en août 2000, Amazon.fr est à la seconde
place des sites de biens culturels français. Selon les chiffres publiés
le 24 octobre 2000 par Media Metrix Europe, société d'étude d'audience
de l'internet, le site a reçu 217.000 visites uniques en septembre
2000, juste devant Alapage (209.000 visites) mais loin derrière
Fnac.com (401.000 visites). Suivent Cdiscount.com (115.000 visites) et
BOL.fr (74.000 visites).

Contrairement à leurs homologues anglophones, les librairies en ligne
françaises ne peuvent se permettre les réductions substantielles
proposées par celles des Etats-Unis ou du Royaume-Uni, pays dans
lesquels le prix du livre est libre. Si la loi française sur le prix
unique du livre (loi Lang) leur laisse peu de latitude, à savoir un
rabais de 5% seulement sur le prix du livre, les librairies en ligne
sont toutefois optimistes sur les perspectives d’un marché francophone
international. Dès 1997, un nombre significatif de commandes provient
de l’étranger, par exemple 10% des commandes pour le service en ligne
de la Fnac.

Interrogé par l'AFP (Agence France-Presse) au sujet de la loi Lang, qui
autorise un rabais de 5% seulement sur le prix du livre, Denis Terrien,
président d'Amazon France (jusqu'en mai 2001), répond en août 2000:
«L'expérience que nous avons en Allemagne, où le prix du livre est
fixé, nous montre que le prix n'est pas l'élément essentiel dans la
décision d'achat. C'est tout le service qui est ajouté qui compte. Chez
Amazon, nous avons tout un tas de services en plus, d'abord le choix -
nous vendons tous les produits culturels français. On a un moteur de
recherche très performant. En matière de choix de musique, on est ainsi
le seul site qui peut faire une recherche par titre de chanson. Outre
le contenu éditorial, qui nous situe entre un magasin et un magazine,
nous avons un service client 24h/24 7jours/7, ce qui est unique sur le
marché français. Enfin une autre spécificité d'Amazon, c'est le respect
de nos engagements de livraison. On s'est fixé pour objectif d'avoir
plus de 90% de nos ventes en stock.»

Admiré par beaucoup, le modèle économique d’Amazon a toutefois de
nombreux revers en matière de gestion du personnel, avec des contrats
de travail précaires, de bas salaires et des conditions de travail
laissant à désirer.

Malgré la discrétion d'Amazon à ce sujet, les problèmes commencent à
filtrer. En novembre 2000, le Prewitt Organizing Fund et le syndicat
SUD-PTT Loire Atlantique débutent une action de sensibilisation auprès
des salariés d'Amazon France pour de meilleures conditions de travail
et des salaires plus élevés. Ils rencontrent une cinquantaine de
salariés travaillant dans le centre de distribution de Boigny-sur-
Bionne. SUD-PTT dénonce dans un communiqué «des conditions de travail
dégradées, la flexibilité des horaires, le recours aux contrats
précaires dans les périodes de flux, des salaires au rabais, et des
garanties sociales minimales». Une action similaire est menée en
Allemagne et en Grande-Bretagne. Patrick Moran, responsable du Prewitt
Organizing Fund, entend constituer une alliance des salariés de la
nouvelle économie sous le nom d'Alliance of New Economy Workers. De son
côté, Amazon riposte en diffusant des documents internes sur
l'inutilité de syndicats au sein de l'entreprise.

Fin janvier 2001, Amazon, qui emploie 1.800 personnes en Europe,
annonce une réduction de 15% des effectifs et la fermeture du service
clientèle de La Hague (Pays-Bas). Les 240 personnes qu'emploie ce
service sont transférées dans les centres de Slough (Royaume-Uni) et
Regensberg (Allemagne).


= Dans le monde

Le deuxième groupe de clients étrangers (après les clients européens)
est la clientèle japonaise. Lors d'un colloque international sur les
technologies de l'information à Tokyo en juillet 2000, Jeff Bezos
annonce son intention prochaine d'implanter Amazon au Japon. Il insiste
aussi sur le marché à fort potentiel représenté par ce pays, avec des
prix immobiliers élevés se répercutant sur ceux des biens et services,
si bien que le shopping en ligne est plus avantageux que le shopping
traditionnel. La densité de la population entraîne des livraisons à
domicile faciles et peu coûteuses.

Un centre d'appels est ouvert en août 2000 dans la ville de Sapporo,
sur l'île d'Hokkaido. La filiale japonaise débute ses activités trois
mois plus tard, en novembre 2000. Amazon Japon, quatrième filiale du
géant américain et première filiale non européenne, ouvre ses portes
avec un catalogue de 1,1 million de titres en japonais et 600.000
titres en anglais. Pour réduire les délais de livraison et proposer des
délais de 24 à 48 heures au lieu des six semaines nécessaires à
l'acheminement des livres depuis les Etats-Unis, un centre de
distribution de 15.800 m2 est créé dans la ville d'Ichikawa, située à
l'est de Tokyo.

En novembre 2000, entre la maison-mère et les quatre filiales, la
société compte 7.500 salariés, 28 millions d’articles et 23 millions de
clients.

A la même date, Amazon débute l'embauche de personnel francophone
connaissant le marché canadien, dans le but de lancer une antenne
canadienne française avec vente de livres, musique et films (VHS et
DVD). Amazon Canada, cinquième filiale de la société, verra le jour en
juin 2002, avec un site bilingue anglais-français.

Toujours en novembre 2000, Amazon ouvre sa librairie numérique, avec
1.000 titres disponibles au départ, et une augmentation rapide du stock
prévue pour les mois suivants.

Même pour le marketing d'une grande librairie en ligne, le papier n'est
pas mort, loin s'en faut. Pour la deuxième année consécutive, en
prévision des fêtes de l'année 2000, Amazon envoie un catalogue imprimé
à 10 millions de clients.

L'année 2001 marque un tournant dans les activités d'Amazon, qui doit
faire face aux secousses de la «nouvelle» économie affectant les
entreprises internet. Suite à un quatrième trimestre déficitaire en
2000, un plan de réduction de 15% des effectifs entraîne 1.300
licenciements aux Etats-Unis et 270 licenciements en Europe fin janvier
2001. Amazon opte aussi pour une plus grande diversification de ses
produits et décide de vendre non seulement des livres, des vidéos, des
CD et des logiciels, mais aussi des produits de santé, des jouets, des
appareils électroniques, des ustensiles de cuisine et des outils de
jardinage. En novembre 2001, la vente des livres, disques et vidéos ne
représente plus que 58% du chiffre d’affaires global, qui est de 4
milliards de dollars US, avec 29 millions de clients.

La société devient bénéficiaire pour la première fois au troisième
trimestre 2003.

En octobre de la même année, Amazon lance un service de recherche plein
texte (Search Inside the Book) après avoir scanné le texte intégral de
120.000 titres, un nombre promis à une croissance rapide. Amazon lance
aussi son propre moteur de recherche, A9.com.

Une sixième filiale est ouverte en Chine sous le nom de Joyo en
septembre 2004.

En 2004, le bénéfice net d’Amazon est de 588 millions de dollars US,
dont 45% généré par ses six filiales, avec un chiffre d’affaires de 6,9
milliards de dollars.

Présent dans sept pays (Etats-Unis, Canada, Royaume-Uni, Allemagne,
France, Japon, Chine) et devenu une référence mondiale du commerce en
ligne, Amazon fête ses dix ans d’existence en juillet 2005, avec 9.000
salariés et 41 millions de clients attirés par des produits culturels,
high-tech et autres aux prix attractifs et une livraison en 48 heures
maximum dans les pays hébergeant une plateforme Amazon.

Amazon poursuit ensuite sa croissance, vend de plus en plus d'ebooks
après avoir racheté la société Mobipocket (en avril 2005) et lance sa
tablette de lecture, le Kindle, en novembre 2007, avec un catalogue de
80.000 ebooks. 538.000 tablettes sont vendues en 2008. Une nouvelle
version du Kindle, le Kindle 2, est lancée en février 2009, avec un
catalogue de 230.000 ebooks.


= Et les petits libraires?

Qu'en est-il des petites librairies, générales et spécialisées? Ces
librairies se débrouillent au mieux avec des moyens limités, comme la
librairie Ulysse, sise au coeur de Paris, dans l’île Saint-Louis, tout
en se faisant peu d'illusions sur le raz-de-marée qui est en train de
les emporter.

Créée en 1971 par Catherine Domain, la librairie Ulysse est la première
librairie au monde uniquement consacrée au voyage. Ses 20.000 livres,
cartes et revues neufs et d’occasion recèlent des documents
introuvables ailleurs. A la fois libraire et grande voyageuse,
Catherine Domain est membre du Syndicat national de la librairie
ancienne et moderne (SLAM), du Club des explorateurs et du Club
international des grands voyageurs.

En 1999, elle décide de se lancer dans un voyage autrement plus ingrat,
virtuel cette fois-ci, à savoir la réalisation d’un site web en
autodidacte. «Mon site est embryonnaire et en construction, raconte-t-
elle en novembre 2000. Il se veut à l’image de ma librairie, un lieu de
rencontre avant d’être un lieu commercial. Il sera toujours en
perpétuel devenir! Internet me prend la tête, me bouffe mon temps et ne
me rapporte presque rien, mais cela ne m’ennuie pas...»

Elle est toutefois pessimiste sur l’avenir des librairies comme la
sienne. «Internet tue les librairies spécialisées. En attendant d’être
dévorée, je l’utilise comme un moyen d’attirer les clients chez moi, et
aussi de trouver des livres pour ceux qui n’ont pas encore internet
chez eux! Mais j’ai peu d’espoir...»



1996: DES EDITEURS SE LANCENT SUR L'INTERNET


= [Résumé]

A partir de 1996, l’édition électronique creuse son sillon à côté de
l’édition traditionnelle, du fait des avantages qu’elle procure: pas de
stock, coût de fonctionnement moins élevé, diffusion plus facile. Elle
amène aussi un souffle nouveau dans le monde de l’édition, et même une
certaine zizanie. On voit des éditeurs traditionnels vendre directement
leurs titres en ligne, des éditeurs électroniques commercialiser les
versions numérisées de livres publiés par des éditeurs traditionnels,
des libraires numériques vendre les versions numérisées de livres
publiés par des éditeurs partenaires, sans parler des auteurs qui
choisissent de s’auto-éditer sur le web ou de promouvoir eux-mêmes
leurs oeuvres publiées, et des nouvelles plateformes d'édition
littéraire qui se chargent de découvrir de nouveaux talents pour
pallier les carences de l’édition traditionnelle. Le numérique pourra-
il à terme rajeunir la structure éditoriale en place, passablement
sclérosée dans certains pays et ne favorisant guère les nouveaux
auteurs dans d'autres pays?


= Editeurs électroniques

# Editel

En avril 1995, Pierre François Gagnon, poète et essayiste québécois,
décide d’utiliser le numérique pour la réception des textes, leur
stockage et leur diffusion. Il crée Editel, premier site d’auto-édition
collective de langue française. En juillet 2000, il relate: «En fait,
tout le monde et son père savent ou devraient savoir que le premier
site d’édition en ligne commercial fut CyLibris (fondé en août 1996,
NDLR), précédé de loin lui-même, au printemps de 1995, par nul autre
qu’Editel, le pionnier d’entre les pionniers du domaine, bien que nous
fûmes confinés à l’action symbolique collective, faute d’avoir les
moyens de déboucher jusqu’ici sur une formule de commerce en ligne
vraiment viable et abordable (...). Nous sommes actuellement trois
mousquetaires (Pierre François Gagnon, Jacques Massacrier et Mostafa
Benhamza, NDLR) à développer le contenu original et inédit du webzine
littéraire qui continuera de servir de façade d’animation gratuite,
offerte personnellement par les auteurs maison à leur lectorat, à
d’éventuelles activités d’édition en ligne payantes, dès que possible
au point de vue technico-financier. Est-il encore réaliste de rêver à
la démocratie économique?»

# CyLibris

Fondé par Olivier Gainon en août 1996, CyLibris (de Cy, cyber et
Libris, livre), basé à Paris, est le pionnier francophone de l’édition
électronique commerciale. CyLibris est en effet la première maison
d’édition à utiliser l’internet et le numérique pour publier de
nouveaux auteurs littéraires et quelques auteurs confirmés, dans divers
genres: littérature générale, policiers, science-fiction, théâtre et
poésie. Vendus uniquement sur le web, les livres sont imprimés à la
commande et envoyés directement au client, ce qui permet d’éviter le
stock et les intermédiaires. Des extraits sont disponibles en
téléchargement libre.

Pendant son premier trimestre d’activité, CyLibris signe des contrats
avec treize auteurs. Fin 1999, CyLibris compte 15.000 visites
mensuelles sur son site et 3.500 livres vendus tous exemplaires
confondus, avec une année 1999 financièrement équilibrée. En 2001,
certains titres sont également vendus en version imprimée par un réseau
de librairies partenaires, notamment la Fnac, et en version numérique
par Mobipocket et Numilog pour lecture sur ordinateur et sur PDA. En
2003, le catalogue de CyLibris comprend une cinquantaine de titres.

«CyLibris a été créé d’abord comme une maison d’édition spécialisée sur
un créneau particulier de l’édition et mal couvert à notre sens par les
autres éditeurs: la publication de premières oeuvres, donc d’auteurs
débutants, explique Olivier Gainon en décembre 2000. Nous nous
intéressons finalement à la littérature qui ne peut trouver sa place
dans le circuit traditionnel: non seulement les premières oeuvres, mais
les textes atypiques, inclassables ou en décalage avec la mouvance et
les modes littéraires dominantes. Ce qui est rassurant, c’est que nous
avons déjà eu quelques succès éditoriaux: le grand prix de la SGDL
(Société des gens de lettres) en 1999 pour 'La Toile' de Jean-Pierre
Balpe, le prix de la litote pour 'Willer ou la trahison' de Jérôme
Olinon en 2000, etc. Ce positionnement de "défricheur" est en soi
original dans le monde de l’édition, mais c’est surtout son mode de
fonctionnement qui fait de CyLibris un éditeur atypique.

Créé dès 1996 autour de l’internet, CyLibris a voulu contourner les
contraintes de l’édition traditionnelle grâce à deux innovations: la
vente directe par l’intermédiaire d’un site de commerce sur internet,
et le couplage de cette vente avec une impression numérique en "flux
tendu". Cela permettait de contourner les deux barrières
traditionnelles dans l’édition: les coûts d’impression (et de stockage)
et les contraintes de distribution. Notre système gérait donc des flux
physiques: commande reçue par internet, impression du livre commandé,
envoi par la poste. Je précise que nous sous-traitons l’impression à
des imprimeurs numériques, ce qui nous permet de vendre des livres de
qualité équivalente à celle de l’offset, et à un prix comparable. Notre
système n’est ni plus cher, ni de moindre qualité, il obéit à une
économie différente qui, à notre sens, devrait se généraliser à terme.»

En quoi consiste l’activité d’un éditeur électronique? «Je décrirais
mon activité comme double, explique Olivier Gainon. D’une part celle
d’un éditeur traditionnel dans la sélection des manuscrits et leur
retravail (je m’occupe directement de la collection science-fiction),
mais également le choix des maquettes, les relations avec les
prestataires, etc. D’autre part, une activité internet très forte qui
vise à optimiser le site de CyLibris et mettre en oeuvre une stratégie
de partenariat permettant à CyLibris d’obtenir la visibilité qui lui
fait parfois défaut. Enfin, je représente CyLibris au sein du SNE
(Syndicat national de l’édition, dont CyLibris fait partie depuis le
printemps 2000, NDLR). CyLibris est aujourd’hui une petite structure.
Elle a trouvé sa place dans l’édition, mais est encore d’une économie
fragile sur internet. Notre objectif est de la rendre pérenne et
rentable et nous nous y employons.»

Le site web se veut aussi un carrefour de la petite édition. Il procure
des informations pratiques aux auteurs en herbe: comment envoyer un
manuscrit à un éditeur, ce que doit comporter un contrat d’édition,
comment protéger ses manuscrits, comment tenter sa chance dans des
revues ou concours littéraires, etc.

Par ailleurs, l’équipe de CyLibris lance en mai 1999 CyLibris Infos,
une lettre d’information électronique gratuite dont l’objectif n’est
pas tant de promouvoir les livres de l’éditeur que de présenter
l’actualité de l’édition francophone. Volontairement décalée et souvent
humoristique sinon décapante, la lettre, d’abord mensuelle, paraît deux
fois par mois à compter de février 2000. Elle compte 565 abonnés en
octobre 2000. Elle change de nom en février 2001 pour devenir Edition-
actu, qui compte 1.500 abonnés en 2003 avant de laisser place au blog
de CyLibris. CyLibris cesse ses activités éditoriales en 2007.

# 00h00

Lui aussi pionnier de l’édition électronique commerciale, 00h00 (qui se
prononce: zéro heure) fait son apparition en mai 1998, un peu moins de
deux ans après CyLibris. Mais le champ d’investigation de 00h00 est
quelque peu différent,  en tant que premier éditeur en ligne. Son
activité est en effet de vendre des livres numériques via l'internet -
et non des livres imprimés comme CyLibris. En 2000, les versions
numériques (au format PDF) représentent 85% des ventes, les 15%
restants étant des versions imprimées à la demande du client, un
service que l'éditeur procure en complément.

00h00 est fondé par Jean-Pierre Arbon et Bruno de Sa Moreira,
respectivement ancien directeur général de Flammarion et ancien
directeur de Flammarion Multimédia. «Aujourd’hui mon activité
professionnelle est 100% basée sur internet, explique Bruno de Sa
Moreira en juillet 1998. Le changement ne s’est pas fait radicalement,
lui, mais progressivement (audiovisuel puis multimédia puis internet).
(...) La gestation du projet a duré un an: brainstorming, faisabilité,
création de la société et montage financier, développement technique du
site et informatique éditoriale, mise au point et production des textes
et préparation du catalogue à l’ouverture. (...) Nous faisons un pari,
mais l’internet me semble un média capable d’une très large
popularisation, sans doute grâce à des terminaux plus faciles d’accès
que le seul micro-ordinateur.»

«La création de 00h00 marque la véritable naissance de l’édition en
ligne, lit-on sur le site web en 1999. C’est en effet la première fois
au monde que la publication sur internet de textes au format numérique
est envisagée dans le contexte d’un site commercial, et qu’une
entreprise propose aux acteurs traditionnels de l’édition (auteurs et
éditeurs) d’ouvrir avec elle sur le réseau une nouvelle fenêtre
d’exploitation des droits. Les textes offerts par 00h00 sont soit des
inédits, soit des textes du domaine public, soit des textes sous
copyright dont les droits en ligne ont fait l’objet d’un accord avec
leurs ayants droit. (...) Avec l’édition en ligne émerge probablement
une première vision de l’édition au 21e siècle. C’est cette idée
d’origine, de nouveau départ qui s’exprime dans le nom de marque,
00h00. (...) Internet est un lieu sans passé, où ce que l’on fait ne
s’évalue pas par rapport à une tradition. Il y faut inventer de
nouvelles manières de faire les choses. (...) Le succès de l’édition en
ligne ne dépendra pas seulement des choix éditoriaux: il dépendra aussi
de la capacité à structurer des approches neuves, fondées sur les
lecteurs autant que sur les textes, sur les lectures autant que sur
l’écriture, et à rendre immédiatement perceptible qu’une aventure
nouvelle a commencé.»

Les collections sont très diverses: inédits, théâtre classique
français, contes et récits fantastiques, contes et récits
philosophiques, souvenirs et mémoires, philosophie classique, réalisme
et naturalisme, cyberculture, romans d’enfance, romans d’amour,
nouvelles et romans d’aventure. Le recherche est possible par auteur,
par titre et par genre. Pour chaque livre, on a un descriptif court, un
descriptif détaillé, la table des matières et une courte présentation
de l’auteur. S’ajoutent ensuite les commentaires des lecteurs. Pas de
stock, pas de contrainte physique de distribution, mais un lien direct
avec le lecteur et entre les lecteurs. Sur le site, les
internautes/lecteurs qui le souhaitent peuvent créer leur espace
personnel pour y rédiger leurs commentaires, participer à des forums ou
recommander des liens vers d’autres sites. Ils peuvent s’abonner à la
lettre d’information de 00h00 pour être tenus au courant des
nouveautés. L'éditeur produit aussi des clips littéraires pour
présenter les ouvrages publiés.

En 2000, le catalogue comprend 600 titres, qui comprennent une centaine
d’oeuvres originales et des rééditions électroniques d’ouvrages publiés
par d’autres éditeurs. Les oeuvres originales sont réparties en
plusieurs collections: nouvelles écritures interactives et
hypertextuelles, premiers romans, documents d’actualité, études sur les
NTIC (nouvelles technologies de l’information et de la communication),
co-éditions avec des éditeurs traditionnels ou de grandes institutions.
Le paiement est effectué en ligne grâce à un système sécurisé mis en
place par la Banque populaire. Ceux que le paiement en ligne rebute
peuvent régler leur commande par carte bancaire (envoi par fax) ou par
chèque (envoi par courrier postal).

En septembre 2000, 00h00 est racheté par Gemstar-TV Guide
International, société américaine spécialisée dans les produits et
services numériques pour les médias. Quelques mois auparavant, en
janvier 2000, Gemstar rachète les deux sociétés californiennes ayant
lancé les premières tablettes de lecture, NuvoMedia, créatrice du
Rocket eBook, et SoftBook Press, créatrice du SoftBook Reader. Selon un
communiqué de Henry Yuen, président de Gemstar, «les compétences
éditoriales dont dispose 00h00 et ses capacités d’innovation et de
créativité sont les atouts nécessaires pour faire de Gemstar un acteur
majeur du nouvel âge de l’édition numérique qui s’ouvre en Europe.» La
communauté francophone ne voit pas ce rachat d’un très bon oeil, la
mondialisation de l’édition semblant justement peu compatible avec
l’innovation et la créativité. Moins de trois ans plus tard, en juin
2003, 00h00 cesse définitivement ses activités, tout comme la branche
eBook et les tablettes de Gemstar.

Il reste le souvenir d’une belle aventure. En octobre 2006, Jean-Pierre
Arbon, devenu chanteur, raconte sur son site: «J’avais fondé, avec
Bruno de Sa Moreira, une maison d’édition d’un genre nouveau, la
première au monde à tenter à grande échelle l’aventure de l’édition en
ligne. Tout était à faire, à inventer. L’édition numérique était terra
incognita: on explorait, on défrichait.»


= Editeurs traditionnels

# Le Choucas, éditeur indépendant

Fondé en 1992 par Nicolas et Suzanne Pewny, alors libraires en Haute-
Savoie, Le Choucas est une petite maison d’édition spécialisée dans les
romans policiers, la littérature, la photographie et les livres d’art.

En juin 1998, Nicolas Pewny raconte: «Le site des éditions du Choucas a
été créé fin novembre 1996. Lorsque je me suis rendu compte des
possibilités qu’internet pouvait nous offrir, je me suis juré que nous
aurions un site le plus vite possible. Un petit problème: nous n’avions
pas de budget pour le faire réaliser. Alors, au prix d’un grand nombre
de nuits sans sommeil, j’ai créé ce site moi-même et l’ai fait
référencer (ce n’est pas le plus mince travail). Le site a alors évolué
en même temps que mes connaissances (encore relativement modestes) en
la matière et s’est agrandi, et a commencé à être un peu connu même
hors France et Europe.

Le changement qu’internet a apporté dans notre vie professionnelle est
considérable. Nous sommes une petite maison d’édition installée en
province. Internet nous a fait connaître rapidement sur une échelle que
je ne soupçonnais pas. Même les médias "classiques" nous ont ouvert un
peu leur portes grâce à notre site. Les manuscrits affluent par le
courrier électronique. Ainsi nous avons édité deux auteurs québécois
(Fernand Héroux et Liz Morency, auteurs de "Affaire de coeurs", paru en
septembre 1997, NDLR). Beaucoup de livres se réalisent (corrections,
illustrations, envoi des documents à l’imprimeur) par ce moyen. Dès le
début du site nous avons reçu des demandes de pays où nous ne sommes
pas (encore) représentés: Etats-Unis, Japon, Amérique latine, Mexique,
malgré notre volonté de ne pas devenir un site "commercial" mais
d’information et à "connotation culturelle". (Nous n’avons pas de
système de paiement sécurisé, nous avons juste référencé sur une page
les libraires qui vendent en ligne).»

En ce qui concerne l’avenir, «j’aurais tendance à répondre par deux
questions: Pouvez vous me dire comment va évoluer internet? Comment
vont évoluer les utilisateurs? Nous voudrions bien rester aussi peu
"commercial" que possible et augmenter l’interactivité et le contact
avec les visiteurs du site. Y réussirons-nous? Nous avons déjà reçu des
propositions qui vont dans un sens opposé. Nous les avons mis "en
veille". Mais si l’évolution va dans ce sens, pourrons-nous résister,
ou trouver une "voie moyenne"? Honnêtement, je n’en sais rien.»

Le Choucas cesse malheureusement ses activités en mars 2001, une
disparition de plus à déplorer chez les petits éditeurs indépendants.
Fort de son expérience dans le domaine de la librairie, de l'édition,
de l'internet et du numérique, Nicolas Pewny devient consultant en
édition électronique et met ses compétences au service d'autres
organismes.

# Editeurs et technologies

Les technologies numériques conduisent les éditeurs scientifiques et
techniques à repenser leur travail et, pour certains, à s’orienter vers
une diffusion en ligne. Les tirages imprimés restent toujours possibles
à titre ponctuel. Certaines universités diffusent désormais des manuels
«sur mesure» composés d’un choix de chapitres et d’articles
sélectionnés dans une base de données, auxquels s’ajoutent les
commentaires des professeurs. Pour un séminaire, un très petit tirage
peut être fait à la demande à partir de documents transmis par voie
électronique à un imprimeur. Quant aux revues spécialisées, certaines
optent pour une publication en ligne complétée par un partenariat avec
une société spécialisée pour une impression à la demande

Enseignante-chercheuse à l’Ecole pratique des hautes études (EPHE,
Paris-Sorbonne), Marie-Joseph Pierre écrit en février 2003: «Il me
paraît évident que la publication des articles et ouvrages au moins
scientifiques se fera de plus en plus sous forme numérique, ce qui
permettra aux chercheurs d’avoir accès à d’énormes banques de données,
constamment et immédiatement évolutives, permettant en outre le contact
direct et le dialogue entre les auteurs. Nos organismes de tutelle,
comme le CNRS (Centre national de la recherche scientifique) par
exemple, ont déjà commencé à contraindre les chercheurs à publier sous
ce mode, et incitent fortement les laboratoires à diffuser ainsi leurs
recherches pour qu’elles soient rapidement disponibles. Nos rapports
d’activité à deux et à quatre ans – ces énormes dossiers peineux
résumant nos labeurs – devraient prochainement se faire sous cette
forme. Le papier ne disparaîtra pas pour autant, et je crois même que
la consommation ne diminuera pas... Car lorsqu'on veut travailler sur
un texte, le livre est beaucoup plus maniable. Je m’aperçois dans mon
domaine que les revues qui ont commencé récemment sous forme numérique
commencent à être aussi imprimées et diffusées sur papier dignement
relié. Le passage de l’un à l’autre peut permettre des révisions et du
recul, et cela me paraît très intéressant.»

Journaliste et infographiste, Marc Autret a derrière lui dix ans de
journalisme multi-tâches et d’hyperformation dans le domaine de
l’édition, du multimédia et du droit d’auteur. Il explique en décembre
2006: «C’est un "socle" irremplaçable pour mes activités d’aujourd’hui,
qui en sont le prolongement technique. Je suis un "artisan" de
l’information et je travaille essentiellement avec des éditeurs. Ils
sont tellement en retard, tellement étrangers à la révolution
numérique, que j’ai du pain sur la planche pour pas mal d’années.
Aujourd’hui je me concentre sur le conseil, l’infographie, la
typographie, le pré-presse et le webdesign, mais je sens que la part du
logiciel va grandir. Des secteurs comme l’animation 3D,
l’automatisation des tâches de production, l’intégration multi-
supports, la base de données et toutes les technologies issues de XML
vont s’ouvrir naturellement. Les éditeurs ont besoin de ces outils,
soit pour mieux produire, soit pour mieux communiquer. C’est là que je
vois l’évolution, ou plutôt l’intensification, de mon travail.»



1997: LA CONVERGENCE MULTIMEDIA EST LE SUJET D'UN COLLOQUE


= [Résumé]

La convergence multimédia peut être définie comme la convergence des
secteurs de l’informatique, du téléphone et de la radiotélévision dans
une industrie utilisant l'internet pour la distribution de cette
information. Cette convergence entraîne l’unification progressive des
secteurs liés à l’information (imprimerie, édition, presse, conception
graphique, enregistrements sonores, films, etc.) suite à l’utilisation
des techniques de numérisation. La numérisation permettant désormais de
traiter des données de manière simple et rapide, le processus matériel
de production s’en trouve considérablement accéléré. Si, dans certains
secteurs, ce phénomène entraîne de nouveaux emplois, par exemple ceux
liés à la production audio-visuelle, d’autres secteurs sont soumis à
des restructurations drastiques. La convergence multimédia a de
nombreux revers, par exemple des contrats précaires pour les salariés,
l’absence de syndicats pour les télétravailleurs ou le droit d’auteur
mis à mal pour les auteurs. Tel est le thème du Colloque sur la
convergence multimédia organisé en janvier 1997 par l'Organisation
internationale du travail (OIT).


= Définition

Depuis bientôt trente ans (en 1997), la chaîne de l’édition est soumise
à de nombreux bouleversements. Dans les années 1970, l’imprimerie
traditionnelle est d’abord ébranlée par les machines de
photocomposition. Le coût de l’impression continue ensuite de baisser
avec les photocopieurs, les photocopieurs couleur, les procédés
d’impression assistée par ordinateur et le matériel d’impression
numérique. Dans les années 1990, l’impression est souvent assurée à bas
prix par des ateliers de PAO (publication assistée par ordinateur).
Tout contenu est désormais systématiquement numérisé pour permettre son
transfert par voie électronique.

La numérisation permet de créer, d’enregistrer, de combiner, de
stocker, de rechercher et de transmettre  des textes, des sons et des
images de manière simple et rapide. Des procédés similaires permettent
le traitement de l’écriture, de la musique et du cinéma alors que, par
le passé, ce traitement était assuré par des procédés différents sur
des supports différents (papier pour l’écriture, bande magnétique pour
la musique, celluloïd pour le cinéma). De plus, des secteurs distincts
comme l’édition (qui produit des livres) et l’industrie musicale (qui
produit des disques) travaillent de concert pour produire des CD-ROM.

La numérisation accélère le processus matériel de production. Dans la
presse, alors qu’auparavant le personnel de production devait
dactylographier les textes du personnel de rédaction, les journalistes
envoient désormais directement leurs textes pour mise en page. Dans
l’édition, le rédacteur, le concepteur artistique et l'infographiste
travaillent souvent simultanément sur le même ouvrage.

On assiste progressivement à la convergence de tous les secteurs liés à
l’information: imprimerie, édition, presse, conception graphique,
enregistrements sonores, films, radiodiffusion, etc.

La convergence multimédia peut être définie comme la convergence des
secteurs de l'informatique, du téléphone et de la radiotélévision dans
une industrie de la communication et de la distribution utilisant les
mêmes autoroutes de l'information. Si, dans certains secteurs, ce
phénomène entraîne de nouveaux emplois, par exemple ceux liés à la
production de films ou de produits audio-visuels, d'autres secteurs
sont soumis à d'inquiétantes restructurations. Ces problèmes sont
suffisamment préoccupants pour pour être débattus lors du Colloque sur
la convergence multimédia organisé en janvier 1997 par l'Organisation
internationale du travail (OIT) à Genève.


= Interventions

Plusieurs interventions de ce colloque soulèvent des problèmes de fond,
dont certains sont toujours d'actualité douze ans plus tard.

Bernie Lunzer, secrétaire-trésorier de la Newspaper Guild (Etats-Unis),
insiste sur les batailles juridiques faisant rage autour des problèmes
de propriété intellectuelle. Ces batailles visent notamment l'attitude
des directeurs de publication, qui amènent les écrivains indépendants à
signer des contrats particulièrement choquants cédant tous leurs droits
au directeur de publication, avec une contrepartie financière ridicule.

Heinz-Uwe Rübenach, de l'Association allemande de directeurs de
journaux (Bundesverband Deutscher Zeitungsverleger), insiste lui aussi
sur la nécessité pour les entreprises de presse de gérer et de
contrôler l'utilisation sur le web des articles de leurs journalistes,
et de demander une contrepartie financière permettant de continuer à
investir dans les nouvelles technologies.

Un problème tout aussi préoccupant est celui de la pression constante
exercée sur les journalistes des salles de rédaction, dont le travail
doit être disponible tout au long de la journée, au lieu d'être utilisé
seulement en fin de journée. Ces tensions à répétition sont encore
aggravées par un travail à l'écran pendant huit à dix heures d'affilée.
Le rythme de travail et l'utilisation intensive de l'ordinateur
entraînent de préoccupants problèmes de sécurité au travail. Après
quelques années de ce régime, des journalistes «craquent» à l'âge de 35
ou 40 ans.

Selon Carlos Alberto de Almeida, président de la Fédération nationale
des journalistes (FENAJ: Federação nacional dos jornalistas) au Brésil,
les nouvelles technologies devaient donner la possibilité de
rationaliser le travail et d'en réduire la durée afin de favoriser
l'enrichissement intellectuel et les loisirs. En pratique, les
professionnels des médias sont obligés d'effectuer un nombre d'heures
de travail de plus en plus grand. La journée légale de cinq heures est
en fait une journée de dix à douze heures. Les heures supplémentaires
ne sont pas payées, comme ne sont pas payées non plus celles effectuées
le week-end par un journaliste pendant sa période de repos.

Si elles accélèrent le processus de production, la numérisation des
documents et l'automatisation des méthodes de travail entraînent une
diminution de l'intervention humaine et donc un accroissement du
chômage. Alors qu'auparavant le personnel de production devait retaper
les textes du personnel de rédaction, la mise en page automatique
permet de combiner les deux tâches de rédaction et de composition.

Etienne Reichel, directeur suppléant de Viscom (Visual Communication),
association suisse pour la communication visuelle, démontre que le
transfert de données via l'internet et la suppression de certaines
phases de production réduisent le nombre d'emplois. Le travail de vingt
typographes est maintenant assuré par six travailleurs qualifiés, alors
que les entreprises de communication visuelle étaient auparavant
génératrices d'emplois. Par contre, l'informatique permet à certains
professionnels de s'installer à leur compte, comme c'est le cas pour
30% des salariés ayant perdu leur emploi suite à la restructuration de
leur entreprise.

Professeur associé en sciences sociales à l’Université d’Utrecht (Pays-
Bas), Peter Leisink précise que la rédaction des textes et la
correction d’épreuves se font désormais à domicile, le plus souvent par
des travailleurs ayant pris le statut d’indépendants à la suite de
licenciements et de délocalisations ou fusions d’entreprises. «Or cette
forme d’emploi tient plus du travail précaire que du travail
indépendant, car ces personnes n’ont que peu d’autonomie et sont
généralement tributaires d’une seule maison d’édition.»

A part quelques cas particuliers mis en avant par les organisations
d’employeurs, la convergence multimédia entraîne des suppressions
massives d’emplois.

Selon Michel Muller, secrétaire général de la FILPAC (Fédération des
industries du livre, du papier et de la communication) en France, les
industries graphiques françaises ont perdu 20.000 emplois en dix ans.
Entre 1987 et 1996, les effectifs passent de de 110.000 à 90.000
salariés. Les entreprises mettent en place des plans sociaux coûteux
pour favoriser le reclassement des personnes licenciées, en créant des
emplois souvent artificiels, alors qu’il aurait été préférable de
financer des études fiables sur la manière d’équilibrer créations et
suppressions d’emplois quand il était encore temps.

Partout dans le monde, de nombreux postes à faible qualification
technique sont remplacés par des postes exigeant des qualifications
techniques élevées. Les personnes peu qualifiées sont licenciées.
D’autres suivent une formation professionnelle complémentaire, parfois
auto-financée et prise sur leur temps libre, et cette formation
professionnelle ne garantit pas pour autant le réemploi.

Directeur de AT&T, géant des télécommunications aux Etats-Unis, Walter
Durling insiste sur le fait que les nouvelles technologies ne
changeront pas fondamentalement la situation des salariés au sein de
l'entreprise. L’invention du film n’a pas tué le théâtre et celle de la
télévision n’a pas fait disparaître le cinéma. Les entreprises
devraient créer des emplois liés aux nouvelles technologies et les
proposer à ceux qui sont obligés de quitter d’autres postes devenus
obsolètes.

Des arguments bien théoriques alors que le problème est plutôt celui du
pourcentage. Combien de créations de postes pour combien de
licenciements?

De leur côté, les syndicats préconisent la création d’emplois par
l’investissement, l’innovation, la formation aux nouvelles
technologies, la reconversion des travailleurs dont les emplois sont
supprimés, des conventions collectives équitables, la défense du droit
d’auteur, une meilleure protection des travailleurs dans le secteur
artistique et enfin la défense des télétravailleurs en tant que
travailleurs à part entière.



1998: LES BIBLIOTHEQUES EMMENAGENT SUR LE WEB


= [Résumé]

A partir de 1998, nombre de bibliothèques «traditionnelles» créent un
site web, qui devient leur vitrine «virtuelle» et permet de proposer
leur catalogue en ligne, des informations pratiques et un choix de
sites pour éviter à leurs lecteurs de se perdre sur la toile. Elles
créent aussi une bibliothèque numérique pour faire connaître leurs
collections à un large public. Qu'elles soient des bibliothèques de
textes, des bibliothèques d'images (fixes ou animées) ou des
bibliothèques sonores, ou qu'elles associent les trois supports, ces
bibliothèques numériques se développent rapidement et permettent
d'avoir accès à des documents jusque-là difficiles - sinon presque
impossibles - à consulter parce qu'appartenant à des fonds anciens, des
fonds régionaux ou des fonds spécialisés.


= Bibliothèques traditionnelles

La première bibliothèque «traditionnelle» présente sur le web est la
Bibliothèque municipale d’Helsinki (Finlande), qui inaugure son site en
février 1994. Des bibliothèques mettent sur pied des «cyberespaces» à
destination de leurs lecteurs. D’autres bibliothèques font connaître
les joyaux de leurs collections par le biais du web. Des bibliothèques
nationales unissent leurs efforts pour créer un portail commun.

Face à un web encyclopédique et des bibliothèques numériques de plus en
plus nombreuses, les jours des bibliothèques traditionnelles sont-ils
comptés? La bibliothèque numérique menace-t-elle vraiment l’existence
de la bibliothèque traditionnelle? Telles sont les questions qu'on se
pose en 1998. A cette date, plusieurs grandes bibliothèques expliquent
sur leur site que, à côté d’un secteur numérique en pleine expansion,
la communication physique des documents reste essentielle. Ces
commentaires disparaissent ensuite. Au début des années 2000, toute
bibliothèque traditionnelle quelque peu dynamique dispose de
collections numériques, soit à usage interne, soit en accès libre sur
le web.

La raison d’être des bibliothèques nationales est de préserver un
patrimoine accumulé au fil des siècles: manuscrits, incunables, livres
imprimés, journaux, périodiques, gravures, affiches, partitions
musicales, images, photos, films, etc. Ceci n’est pas près de changer.
Si le fait de disposer de supports numériques favorise la
communication, il faut bien un endroit pour stocker les documents
physiques originaux, à commencer par les Bibles de Gutenberg.

De plus, les bibliothèques nationales archivent aussi les documents
électroniques et les pages web. A la Bibliothèque nationale de France
(BnF) par exemple, il a été décidé de collecter et d’archiver les sites
dont le nom de domaine se termine en .fr, ou encore les sites dédiés
aux campagnes électorales, d’abord pour les présidentielles de 2002,
puis pour les législatives de 2004, et enfin pour les présidentielles
et législatives de 2007, en copiant et sauvegardant les sites
institutionnels, les sites et blogs officiels des candidats, les sites
d’analyses, les sites des médias traditionnels, les sites
d’associations et de syndicats, etc.

Les bibliothèques publiques ne semblent pas près de disparaître non
plus. Malgré la curiosité suscitée par le livre numérique, les lecteurs
assurent régulièrement lors de sondages divers qu’ils ne sont pas prêts
à lire Zola ou Proust à l’écran. Question de génération peut-être. Les
enfants ayant appris à lire directement à l’écran ne verront sans doute
aucun problème à lire des livres en ligne sur des supports
électroniques en tous genres.

Si les bibliothèques nationales et les bibliothèques publiques restent
toujours utiles, la situation est différente pour les bibliothèques
spécialisées. Dans nombre de domaines où l’information la plus récente
est primordiale, on s’interroge maintenant sur la nécessité d’aligner
des documents imprimés sur des rayonnages, alors qu’il est tellement
plus pratique de rassembler, stocker, archiver, organiser, cataloguer
et diffuser des documents électroniques, et de les imprimer seulement à
la demande.

Fondateur de la bibliothèque numérique Athena, Pierre Perroud insiste
sur la complémentarité du texte électronique et du livre imprimé. Selon
lui, «les textes électroniques représentent un encouragement à la
lecture et une participation conviviale à la diffusion de la culture»,
notamment pour l’étude et la recherche textuelle. Ces textes «sont un
bon complément du livre imprimé - celui-ci restant irremplaçable
lorsqu’il s’agit de lire». Mais le livre imprimé reste «un compagnon
mystérieusement sacré vers lequel convergent de profonds symboles: on
le serre dans la main, on le porte contre soi, on le regarde avec
admiration; sa petitesse nous rassure autant que son contenu nous
impressionne; sa fragilité renferme une densité qui nous fascine; comme
l’homme il craint l’eau et le feu, mais il a le pouvoir de mettre la
pensée de celui-là à l’abri du Temps.» (extraits de la revue
Informatique-Informations, Genève, février 1997)


= Bibliothèques numériques

Objectif poursuivi par des générations de bibliothécaires, la diffusion
du livre devient enfin possible à vaste échelle, puisque celui-ci peut
désormais être converti en fichier électronique et transiter via
l’internet pour toucher un public qui n'a pas toujours accès à une
bibliothèque traditionnelle.

Si certaines bibliothèques numériques naissent directement sur le web,
la plupart émanent de bibliothèques traditionnelles. En 1996, la
Bibliothèque municipale de Lisieux (Normandie) lance la Bibliothèque
électronique de Lisieux, qui offre les versions numériques d'oeuvres
littéraires courtes choisies dans les collections municipales. En 1997,
la Bibliothèque nationale de France (BnF) crée Gallica qui, dans un
premier temps, propose des images et textes du 19e siècle francophone.
Une sélection de 3.000 livres est complétée par un échantillon de la
future iconothèque numérique. En 1998, la Bibliothèque municipale de
Lyon met les enluminures de 200 manuscrits et incunables à la
disposition de tous sur son site web. Trois exemples parmi tant
d’autres.

Les bibliothèques numériques permettent à un large public d’avoir accès
à des documents difficiles à consulter parce qu’appartenant à des fonds
anciens, locaux,  régionaux ou spécialisés, peu accessibles pour des
raisons diverses: souci de conservation des documents rares et
fragiles, heures d’ouverture réduites, nombreux formulaires à remplir,
longs délais de communication, pénurie de personnel, qui sont autant de
barrières à franchir et demandent souvent au lecteur une patience à
toute épreuve et une détermination hors du commun pour arriver jusqu’au
document.

Grâce à la bibliothèque numérique, la bibliothèque traditionnelle peut
enfin rendre compatibles deux objectifs qui jusque-là ne l’étaient
guère, à savoir la conservation des documents et la communication de
ceux-ci. D’une part le document ne quitte son rayonnage qu’une seule
fois pour être scanné, d’autre part le grand public y a enfin accès. Si
le lecteur souhaite consulter le document original, il pourra se lancer
dans le parcours évoqué plus haut, mais en connaissance de cause, grâce
au feuilletage préalable à l’écran.

Selon la British Library, pionnière dans ce domaine, la bibliothèque
numérique peut être définie comme une entité résultant de l’utilisation
des technologies numériques pour acquérir, stocker, préserver et
diffuser des documents. Ces documents sont soit publiés directement
sous forme numérique, soit numérisés à partir d’un document imprimé,
audiovisuel ou autre. Une collection numérique devient une bibliothèque
numérique si elle répond aux quatre critères suivants: 1) elle peut
être créée et/ou produite dans un certain nombre d’endroits différents,
mais elle est accessible en tant qu’entité unique; 2) elle doit être
organisée et indexée pour un accès facile au serveur du lieu; 3) elle
doit être stockée et gérée de manière à avoir une existence assez
longue après sa création; 4) elle doit trouver un équilibre entre le
respect du droit d’auteur et les exigences universitaires.

Hébergée par l’Université Carnegie Mellon (Pittsburgh, Pennsylvanie,
Etats-Unis), l’Universal Library insiste sur les trois avantages de la
bibliothèque numérique: 1) elle occupe moins de place qu’une
bibliothèque traditionnelle et son contenu peut être copié ou
sauvegardé électroniquement; 2) elle est immédiatement accessible à
quiconque sur l’internet; 3) comme toute recherche sur son contenu est
automatisée, elle permet une réduction significative des coûts de
fonctionnement et une meilleure accessibilité des documents.

A titre historique, le site Library 2000 présente un condensé des
recherches menées entre octobre 1995 et octobre 1997 par le MIT/LCS
(Massachusetts Institute of Technology / Laboratory of Computer
Science). Pragmatique, le projet Library 2000 a consisté à étudier
pendant deux ans les problèmes posés par le stockage en ligne d’une
très grande quantité de documents, puis à développer un prototype sensé
économiquement viable en l’an 2000, prototype grâce auquel plusieurs
grandes bibliothèques numériques sont mises en ligne à compter de
novembre 1997.

En ce qui concerne les images, les problèmes de bande passante
s’estompent. Après avoir proposé avec enthousiasme des images en pleine
page très agréables à l’oeil mais excessivement longues à apparaître à
l’écran, nombreux sont les sites qui optent ensuite pour des images de
format réduit, avec possibilité de cliquer ou non sur ces images pour
obtenir un format plus grand. Cette présentation reste souvent la norme
ensuite, même avec la généralisation de l’internet à débit rapide. Le
passage du petit format ou grand format est désormais rapide sinon
immédiat, à la grande satisfaction des iconographes, photographes et
autres amateurs d’images.


= Numérisation: mode texte ou image

Qui dit bibliothèque numérique dit numérisation. Pour pouvoir être
consulté à l’écran, un livre peut être numérisé soit en mode texte soit
en mode image.

La numérisation en mode texte implique la saisie d’un texte. Elle
consiste à patiemment saisir le livre sur un clavier, page après page,
solution souvent adoptée lors de la constitution des premières
bibliothèques numériques, ou alors quand les documents originaux
manquent de clarté, pour les livres anciens par exemple. Les années
passant, la numérisation en mode texte consiste surtout à scanner le
livre en mode image, puis à le convertir en texte grâce à un logiciel
OCR (optical character recognition), avec relecture éventuelle à
l’écran pour corriger le texte obtenu puisqu'un bon logiciel OCR serait
fiable à 90%.

La version informatique du livre ne conserve pas la présentation
originale du livre ou de la page. Le livre devient texte, à savoir un
ensemble de caractères apparaissant en continu à l’écran. A cause du
temps passé au traitement de chaque livre, ce mode de numérisation est
assez long, et donc nettement plus coûteux que la numérisation en mode
image. Dans de nombreux cas, il est toutefois très préférable,
puisqu’il permet l’indexation, la recherche et l’analyse textuelles,
une étude comparative entre plusieurs textes ou plusieurs versions du
même texte, etc. C’est la méthode utilisée par exemple par le Projet
Gutenberg, fondé dès 1971, ou encore la Bibliothèque électronique de
Lisieux, créée en 1996.

La numérisation en mode image correspond à la photographie du livre
page après page. La version informatique est le fac-similé numérique de
la version imprimée. La présentation originale étant conservée, on peut
feuilleter le texte page après page à l’écran. C’est la méthode
employée pour les numérisations à grande échelle, par exemple pour le
programme de numérisation de la Bibliothèque nationale de France (BnF)
et la constitution de sa bibliothèque numérique Gallica. La
numérisation en mode texte est toutefois utilisée pour les tables des
matières, les sommaires et les corpus de documents iconographiques,
afin de faciliter la recherche textuelle.

Pourquoi ne pas tout numériser en mode texte? La BnF répond en 2000 sur
le site de Gallica: «Le mode image conserve l’aspect initial de
l’original y compris ses éléments non textuels. Si le mode texte
autorise des recherches riches et précises dans un document et permet
une réduction significative du volume des fichiers manipulés, sa
réalisation, soit par saisie soit par OCR, implique des coûts de
traitement environ dix fois supérieurs à la simple numérisation. Ces
techniques, parfaitement envisageables pour des volumes limités, ne
pouvaient ici être économiquement justifiables au vu des 50.000
documents (représentant presque 15 millions de pages) mis en ligne.»

Concepteur de Mot@mot, logiciel de remise en page de fac-similés
numériques, Pierre Schweitzer insiste sur l’utilité des deux modes de
numérisation. «Le mode image permet d’avancer vite et à très faible
coût, explique-t-il en janvier 2001. C’est important car la tâche de
numérisation du domaine public est immense. Il faut tenir compte aussi
des différentes éditions: la numérisation du patrimoine a pour but de
faciliter l’accès aux oeuvres, il serait paradoxal qu’elle aboutisse à
se focaliser sur une édition et à abandonner l’accès aux autres. Chacun
des deux modes de numérisation s’applique de préférence à un type de
document, ancien et fragile ou plus récent, libre de droit ou non (pour
l’auteur ou pour l’édition), abondamment illustré ou pas. Les deux
modes ont aussi des statuts assez différents: en mode texte ça peut
être une nouvelle édition d’une oeuvre, en mode image c’est une sorte
d’"édition d’édition", grâce à un de ses exemplaires (qui fonctionne
alors comme une fonte d’imprimerie pour du papier). En pratique, le
choix dépend bien sûr de la nature du fonds à numériser, des moyens et
des buts à atteindre. Difficile de se passer d’une des deux façons de
faire.»


= Gallica

Secteur numérique de la Bibliothèque nationale de France (BnF), Gallica
est inauguré en octobre 1997 avec des images et textes du 19e siècle
francophone, «siècle de l’édition et de la presse moderne, siècle du
roman mais aussi des grandes synthèses historiques et philosophiques,
siècle scientifique et technique». A l’époque, le serveur stocke
2.500 livres numérisés en mode image complétés par les 250 livres
numérisés en mode texte de la base Frantext de l’INaLF (Institut
national de la langue française). Classés par discipline, ces livres
sont complétés par une chronologie du 19e siècle et des synthèses sur
les grands courants en histoire, sciences politiques, droit, économie,
littérature, philosophie, sciences et histoire des sciences. Le site
propose aussi un échantillon de la future iconothèque numérique, à
savoir le fonds du photographe Eugène Atget, une sélection de documents
sur l’écrivain Pierre Loti, une collection d’images de l’Ecole
nationale des ponts et chaussées ayant trait aux grands travaux liés à
la révolution industrielle en France, et enfin un choix de livres
illustrés de la Bibliothèque du Musée de l’homme.

Fin 1997, Gallica se considère moins comme une banque de données
numérisées que comme un «laboratoire dont l’objet est d’évaluer les
conditions d’accès et de consultation à distance des documents
numériques». Le but est d’expérimenter la navigation dans ces
collections, en permettant aussi bien le libre parcours du chercheur ou
du curieux que des recherches textuelles pointues.

Début 1998, Gallica annonce 100.000 volumes et 300.000 images pour la
fin 1999, avec un accroissement rapide des collections ensuite. Sur les
100.000 volumes prévus, qui représenteront 30 millions de pages
numérisées, plus du tiers concerne le 19e siècle. Quant aux 300.000
images fixes, la moitié appartient aux départements spécialisés de la
BnF (Estampes et photographie, Manuscrits, Arts du spectacle, Monnaies
et médailles, etc.). L’autre moitié provient de collections
d’établissements publics (musées et bibliothèques, Documentation
française, Ecole nationale des ponts et chaussées, Institut Pasteur,
Observatoire de Paris, etc.) ou privés (agences de presse dont Magnum,
l’Agence France-Presse, Sygma, Rapho, etc.).

Par ailleurs, à la même date, le site bilingue français-anglais de la
BnF est à la fois solidement ancré dans le passé et résolument ouvert
sur l’avenir, comme en témoigne le menu principal de la page d’accueil,
avec ses neuf rubriques: (1) nouveau (à savoir les nouvelles
manifestations culturelles); (2) connaître la BnF; (3) les actualités
culturelles; (4) les expositions virtuelles (quatre expositions en
septembre 1998: les splendeurs persanes, le roi Charles V et son temps,
naissance de la culture française, tous les savoirs du monde); (5) des
informations pratiques; (6) l’accès aux catalogues de la BnF; (7)
l’information professionnelle (conservation, dépôt légal, produits
bibliographiques, etc.); (8) la bibliothèque en réseau (Francophonie,
coopération nationale, coopération internationale, etc.); (9) les
autres serveurs (bibliothèques nationales, bibliothèques françaises,
universités, etc.). Bien en vue sur la page d’accueil, un logo permet
d’accéder à Gallica.

En mai 1998, la BnF revoit ses espérances à la baisse et modifie
quelque peu ses orientations premières. Jérôme Strazzulla, journaliste
au Figaro, explique dans l'édition du 3 juin 1998 que la BnF est
«passée d’une espérance universaliste, encyclopédique, à la nécessité
de choix éditoriaux pointus». Dans le même article, le président de la
BnF, Jean-Pierre Angremy, rapporte la décision du comité éditorial de
Gallica: «Nous avons décidé d’abandonner l’idée d’un vaste corpus
encyclopédique de cent mille livres, auquel on pourrait sans cesse
reprocher des trous. Nous nous orientons aujourd’hui vers des corpus
thématiques, aussi complets que possibles, mais plus restreints. (...)
Nous cherchons à répondre, en priorité, aux demandes des chercheurs et
des lecteurs.» Le premier corpus aura trait aux voyages en France, avec
mise en ligne prévue en 2000. Ce corpus rassemblera des textes,
estampes et photographies du 16e siècle à 1920. Les corpus envisagés
ensuite auront les thèmes suivants: Paris, les voyages en Afrique des
origines à 1920, les utopies, et les mémoires des Académies des
sciences de province.

En 2003, Gallica rassemble 70.000 ouvrages et 80.000 images allant du
Moyen-Age au début du 20e siècle, tous documents libres de droits.
Mais, de l’avis de nombreux usagers, les fichiers sont très lourds
puisque les livres sont numérisés en mode image, et l’accès en est très
long. Chose tout aussi problématique, la numérisation en mode image
n’autorise pas la recherche textuelle alors que Gallica se trouve être
la plus grande bibliothèque numérique francophone du réseau en nombre
de titres disponibles en ligne. Seule une petite collection de livres
(1.117 livres en février 2004) est numérisée en mode texte, celle de la
base Frantext de l'ATILF (Analyse et traitement informatique de la
langue française, le laboratoire ayant succédé à l'INaLF), intégrée
dans Gallica.

En février 2005, Gallica compte 76.000 ouvrages. A la même date, la BnF
annonce la mise en ligne prochaine (entre 2006 et 2009) de la presse
française parue entre 1826 et 1944, à savoir 22 titres représentant 3,5
millions de pages. Début 2006, les premiers journaux disponibles en
ligne sont les quotidiens Le Figaro (fondé en 1826), La Croix (fondée
en 1883), L'Humanité (fondée en 1904) et Le Temps (fondé en 1861 et
disparu en 1942).

En décembre 2006, les collections comprennent 90.000 ouvrages numérisés
(fascicules de presse compris), 80.000 images et des dizaines d'heures
de ressources sonores. Gallica débute la conversion en mode texte des
livres numérisés en mode image afin de favoriser l'accès à leur contenu
et leur indexation par les moteurs de recherche.

En novembre 2007, la BnF annonce la numérisation de 300.000 ouvrages
supplémentaires d'ici 2010, à savoir 45 millions de pages qui seront
accessibles sur son nouveau site Gallica2, simultanément en mode image
et en mode texte.



1999: LES BIBLIOTHECAIRES DEVIENNENT CYBERTHECAIRES


= [Résumé]

Selon Peter Raggett, bibliothécaire depuis plus de vingt ans,
«l’internet offre aux chercheurs un stock d’informations considérable.
Le problème pour eux est de trouver ce qu’ils cherchent (en 1999).
Jamais auparavant on n’avait senti une telle surcharge d’informations,
comme on la sent maintenant quand on tente de trouver un renseignement
sur un sujet précis en utilisant les moteurs de recherche disponibles
sur l’internet. A mon avis, les bibliothécaires auront un rôle
important à jouer pour améliorer la recherche et l’organisation de
l’information sur le réseau. (...) La tâche du bibliothécaire sera de
filtrer les informations pour le public. Personnellement, je me vois de
plus en plus devenir un bibliothécaire virtuel. Je n’aurai pas
l’occasion de rencontrer les usagers, ils me contacteront plutôt par
courriel, par téléphone ou par fax, j’effectuerai la recherche et je
leur enverrai les résultats par voie électronique.»


= Bibliothécaires et internet

Le bibliothécaire-documentaliste voit son activité professionnelle
frappée de plein fouet par l'informatique puis par l’internet. Dans les
années 1980, l'informatique permet aux bibliothécaires de remplacer des
catalogues de fiches sur bristol par des catalogues consultables à
l’écran, avec un classement alphabétique ou systématique effectué par
la machine. L'informatisation du prêt et des commandes de livres fait
disparaître l’impressionnant stock de fiches et bordereaux nécessaires
lors des opérations manuelles. L’informatique en réseau permet ensuite
la gestion de catalogues collectifs regroupant dans une même base de
données les catalogues des bibliothèques de la même région, du même
pays ou de la même spécialité, entraînant du même coup des services
très facilités pour le prêt inter-bibliothèques et le regroupement des
commandes auprès des fournisseurs. Puis les bibliothèques ouvrent un
serveur minitel pour la consultation de leur catalogue, désormais
disponible au domicile du lecteur. Ces catalogues sont progressivement
transférés sur l’internet, avec une consultation plus souple et plus
attractive que sur minitel. Outre le catalogue en ligne, les sites web
des bibliothèques offrent un ensemble de documents numérisés ou encore
un choix de liens hypertextes vers d’autres sites, évitant ainsi aux
usagers de se perdre sur la toile.

Selon Olivier Bogros, directeur de la Bibliothèque municipale de
Lisieux (Normandie), interviewé en juin 1998, l’internet est «un outil
formidable d’échange entre professionnels (tout ce qui passe par le
courrier électronique, les listes de diffusion et les forums) (...).
C’est aussi pour les bibliothèques la possibilité d’élargir leur public
en direction de toute la Francophonie. Cela passe par la mise en ligne
d’un contenu qui n’est pas seulement la mise en ligne du catalogue,
mais aussi et surtout la constitution de véritables bibliothèques
virtuelles.»

La liste de diffusion Biblio-fr est créée en 1993 par Hervé Le
Crosnier, professeur à l’Université de Caen (Normandie), à l’intention
des «bibliothécaires et documentalistes francophones et [de] toute
personne intéressée par la diffusion électronique de l’information
documentaire». La liste se veut le regard francophone des
documentalistes sur les questions soulevées par le développement de
l’internet, par exemple «la diffusion de la connaissance,
l’organisation de collections de documents électroniques, la
maintenance et l’archivage de l’écrit électronique». Biblio-fr compte
3.329 abonnés le 20 décembre 1998 et 15.136 abonnés le 20 avril 2007.
Une autre liste de diffusion est ADBS-info, gérée par l’Association des
professionnels de l’information et de la documentation (ADBS), avec
7.699 abonnés le 20 avril 2007.

Des portails sont créés à l’intention des bibliothèques, par exemple
Biblio On Line. Jean-Baptiste Rey, son rédacteur et webmestre, relate
en juin 1998: «Le site dans sa première version a été lancé en juin
1996. Une nouvelle version (l’actuelle) a été mise en place à partir du
mois de septembre 1997. Le but de ce site est d’aider les bibliothèques
à intégrer internet dans leur fonctionnement et dans les services
qu’elles offrent à leur public. Le service est décomposé en deux
parties: (a) une partie "professionnelle" où les bibliothécaires
peuvent retrouver des informations professionnelles et des liens vers
les organismes, les institutions, et les projets et réalisations ayant
trait à leur activité; (b) une partie comprenant annuaire, mode
d’emploi de l’internet, villes et provinces, etc... permet au public
des bibliothèques d’utiliser le service Biblio On Line comme un point
d’entrée vers internet.»

Le site de l’ENSSIB (Ecole nationale supérieure des sciences de
l’information et des bibliothèques) héberge la version électronique du
Bulletin des bibliothèques de France (BBF), une revue professionnelle
bimensuelle dans laquelle «professionnels et spécialistes de
l’information discutent de toutes les questions concernant la politique
et le développement des bibliothèques et des centres de documentation:
évolution par secteur, grands projets, informatisation, technologies de
l’information, écrits électroniques, réseaux, coopération, formation,
gestion, patrimoine, usagers et publics, livre et lecture...»

Annie Le Saux, rédactrice de la revue, relate en juillet 1998: «C’est
en 1996 que le BBF a commencé à paraître sur internet (les numéros de
1995). (...) Nous nous servons beaucoup du courrier électronique pour
prendre contact avec nos auteurs et pour recevoir leurs articles. Cela
diminue grandement les délais. Nous avons aussi recours au web pour
prendre connaissance des sites mentionnés lors de colloques, vérifier
les adresses, retrouver des indications bibliographiques dans les
catalogues des bibliothèques...»


= Quelques expériences

# En 1999

Avec cette manne documentaire qu’offre désormais l’internet, que vont
devenir les bibliothécaires-documentalistes? Vont-ils devenir des
cyberthécaires, ou bien vont-ils progressivement disparaître parce que
les usagers n’auront tout simplement plus besoin d’eux? A la fin des
années 1990, il ne semble pas que la profession soit en danger, au
contraire. Piloter les usagers sur l’internet, filtrer et organiser
l’information à leur intention, créer et gérer un site web, rechercher
des documents dans des bases de données spécialisées, telles sont
désormais les tâches de nombreux bibliothécaires. C'est le cas de Peter
Raggett à l'OCDE et de Bruno Didier à l'Institut Pasteur.

Peter Raggett est sous-directeur (puis directeur) de la Bibliothèque
centrale de l’OCDE (Organisation de coopération et de développement
économiques), rebaptisée ensuite Centre d'information et de
documentation (CDI).

Située à Paris, l’OCDE regroupe trente pays membres. Au noyau
d’origine, constitué des pays d’Europe de l’Ouest et d’Amérique du
Nord, viennent s’ajouter le Japon, l’Australie, la Nouvelle-Zélande, la
Finlande, le Mexique, la République tchèque, la Hongrie, la Pologne et
la Corée.

Réservée aux fonctionnaires de l’organisation, la bibliothèque permet
la consultation de 60.000 monographies et 2.500 périodiques imprimés.
En ligne depuis 1996, ses pages intranet deviennent une source
d’information majeure pour le personnel.

«Je dois filtrer l’information pour les usagers de la bibliothèque, ce
qui signifie que je dois bien connaître les sites et les liens qu’ils
proposent, explique Peter Raggett en août 1999. J’ai sélectionné
plusieurs centaines de sites pour en favoriser l’accès à partir de
l’intranet de l’OCDE. Cette sélection fait partie du bureau de
référence virtuel proposé par la bibliothèque à l’ensemble du
personnel. Outre de nombreux liens, ce bureau de référence contient des
pages recensant les articles, monographies et sites web correspondant
aux différents projets de recherche en cours à l’OCDE, l’accès en
réseau aux CD-ROM et une liste mensuelle des nouveaux titres.»

Comment voit-il l’avenir de la profession? «L’internet offre aux
chercheurs un stock d’informations considérable. Le problème pour eux
est de trouver ce qu’ils cherchent. Jamais auparavant on n’avait senti
une telle surcharge d’informations, comme on la sent maintenant quand
on tente de trouver un renseignement sur un sujet précis en utilisant
les moteurs de recherche disponibles sur l’internet. A mon avis, les
bibliothécaires auront un rôle important à jouer pour améliorer la
recherche et l’organisation de l’information sur le réseau. Je prévois
aussi une forte expansion de l’internet pour l’enseignement et la
recherche. Les bibliothèques seront amenées à créer des bibliothèques
numériques permettant à un étudiant de suivre un cours proposé par une
institution à l’autre bout du monde. La tâche du bibliothécaire sera de
filtrer les informations pour le public. Personnellement, je me vois de
plus en plus devenir un bibliothécaire virtuel. Je n’aurai pas
l’occasion de rencontrer les usagers, ils me contacteront plutôt par
courriel, par téléphone ou par fax, j’effectuerai la recherche et je
leur enverrai les résultats par voie électronique.»

En 1999, Bruno Didier est bibliothécaire à l’Institut Pasteur (Paris),
une fondation privée dont le but est la prévention et le traitement des
maladies infectieuses par la recherche, l’enseignement et des actions
de santé publique.

Séduit par les perspectives qu’offre le réseau pour la recherche
documentaire, Bruno Didier crée le site web de la bibliothèque en 1996
et devient son webmestre. «Le site web de la bibliothèque a pour
vocation principale de servir la communauté pasteurienne, relate-t-il
en août 1999. Il est le support d’applications devenues indispensables
à la fonction documentaire dans un organisme de cette taille: bases de
données bibliographiques, catalogue, commande de documents et bien
entendu accès à des périodiques en ligne. C’est également une vitrine
pour nos différents services, en interne mais aussi dans toute la
France et à l’étranger. Il tient notamment une place importante dans la
coopération documentaire avec les instituts du réseau Pasteur à travers
le monde. Enfin j’essaie d’en faire une passerelle adaptée à nos
besoins pour la découverte et l’utilisation d’internet. (...) Je
développe et maintiens les pages du serveur, ce qui s’accompagne d’une
activité de veille régulière. Par ailleurs je suis responsable de la
formation des usagers, ce qui se ressent dans mes pages. Le web est un
excellent support pour la formation, et la plupart des réflexions
actuelles sur la formation des usagers intègrent cet outil.»

Son activité professionnelle a changé de manière radicale, tout comme
celle de ses collègues. «C’est à la fois dans nos rapports avec
l’information et avec les usagers que les changements ont eu lieu,
explique-t-il. Nous devenons de plus en plus des médiateurs, et peut-
être un peu moins des conservateurs. Mon activité actuelle est typique
de cette nouvelle situation: d’une part dégager des chemins d’accès
rapides à l’information et mettre en place des moyens de communication
efficaces, d’autre part former les utilisateurs à ces nouveaux outils.
Je crois que l’avenir de notre métier passe par la coopération et
l’exploitation des ressources communes. C’est un vieux projet
certainement, mais finalement c’est la première fois qu’on dispose
enfin des moyens de le mettre en place.»

# En 2000

En 2000, Bakayoko Bourahima est responsable de la bibliothèque de
l'Ecole nationale supérieure de statistique et d’économie appliquée
(ENSEA) d’Abidjan (Côte d'Ivoire). L'ENSEA est un établissement qui
assure la formation de statisticiens pour les pays africains
d’expression française. Son site web est mis en ligne en avril 1999
dans le cadre du réseau REFER, un réseau mis sur pied par l’Agence
universitaire de la Francophonie (AUF) pour desservir la communauté
scientifique et technique en Afrique, en Asie et en Europe orientale
(24 pays participants en 2002).

En tant que responsable de la bibliothèque, Bakayoko Bourahima s’occupe
de la gestion de l’information et de la diffusion des travaux publiés
par l’ENSEA. Quel est l'apport de l’internet dans son travail? «Mon
service a eu récemment des séances de travail avec l’équipe
informatique pour discuter de l’implication de la bibliothèque dans
l’animation du site, relate-t-il en juillet 2000. Le service de la
bibliothèque travaille aussi à deux projets d’intégration du web pour
améliorer ses prestations. (...) J’espère bientôt pouvoir mettre à la
disposition de mes usagers un accès internet pour l’interrogation de
bases de données. Par ailleurs, j’ai en projet de réaliser et de mettre
sur l’intranet et sur le web un certain nombre de services
documentaires (base de données thématique, informations
bibliographiques, service de références bibliographiques, bulletin
analytique des meilleurs travaux d’étudiants...). Il s’agit donc pour
la bibliothèque, si j’obtiens les financements nécessaires pour ces
projets, d’utiliser pleinement l’internet pour donner à notre Ecole un
plus grand rayonnement et de renforcer sa plateforme de communication
avec tous les partenaires possibles. En intégrant cet outil au plan de
développement de la bibliothèque, j’espère améliorer la qualité et
élargir la gamme de l’information scientifique et technique mise à la
disposition des étudiants, des enseignants et des chercheurs, tout en
étendant considérablement l’offre des services de la bibliothèque.»

En 2000, Emmanuel Barthe est documentaliste juridique et responsable
informatique de Coutrelis & Associés, un cabinet d’avocats parisien.
«Les principaux domaines de travail du cabinet sont le droit
communautaire, le droit de l’alimentation, le droit de la concurrence
et le droit douanier, écrit-il en octobre 2000. Je fais de la saisie
indexation, et je conçois et gère les bases de données internes. Pour
des recherches documentaires difficiles, je les fais moi-même ou bien
je conseille le juriste. Je suis aussi responsable informatique et
télécoms du cabinet: conseils pour les achats, assistance et formation
des utilisateurs. De plus, j’assure la veille, la sélection et le
catalogage de sites web juridiques: titre, auteur et bref descriptif.
Je suis également formateur internet juridique aussi bien à l’intérieur
de mon entreprise qu’à l’extérieur lors de stages de formation.»

Par ailleurs, Emmanuel Barthe est le modérateur de Juriconnexion, une
liste de discussion créée par l’association du même nom. «L’association
Juriconnexion a pour but la promotion de l’électronique juridique,
c’est-à-dire la documentation juridique sur support électronique et la
diffusion des données publiques juridiques. Elle organise des
rencontres entre les utilisateurs et les éditeurs juridiques (et de
bases de données), ainsi qu’une journée annuelle sur un thème. Vis-à-
vis des autorités publiques, Juriconnexion a un rôle de médiateur et de
lobbying à la fois. L’association, notamment, est favorable à la
diffusion gratuite sur internet des données juridiques produites par le
Journal officiel et les tribunaux. Les bibliothécaires-documentalistes
juridiques représentent la majorité des membres de l’association,
suivis par certains représentants des éditeurs et des juristes.»

# En 2001

En 2001, Anissa Rachef est bibliothécaire et professeur à l’Institut
français de Londres. Présents dans de nombreux pays, les instituts
français sont des organismes officiels proposant des cours et
manifestations culturelles. A Londres, 5.000 étudiants environ
s'inscrivent aux cours chaque année. Inaugurée en mai 1996, la
médiathèque utilise l’internet dès sa création.

«L’objectif de la médiathèque est double, explique Anissa Rachef en
avril 2001. Servir un public s’intéressant à la culture et la langue
françaises et "recruter" un public allophone en mettant à disposition
des produits d’appel tels que vidéos documentaires, livres audio, CD-
ROM. La mise en place récente d’un espace multimédia sert aussi à
fidéliser les usagers. L’installation d’un service d’information rapide
a pour fonction de répondre dans un temps minimum à toutes sortes de
questions posées via le courrier électronique, ou par fax. Ce service
exploite les nouvelles technologies pour des recherches très
spécialisées. Nous élaborons également des dossiers de presse destinés
aux étudiants et professeurs préparant des examens de niveau
secondaire. Je m’occupe essentiellement de catalogage, d’indexation et
de cotation. (...) J’utilise internet pour des besoins de base.
Recherches bibliographiques, commande de livres, courrier
professionnel, prêt inter-bibliothèques. C’est grâce à internet que la
consultation de catalogues collectifs, tels SUDOC (Système
universitaire de documentation) et OCLC (Online Computer Library
Center), a été possible. C’est ainsi que j’ai pu mettre en place un
service de fourniture de documents extérieurs à la médiathèque. Des
ouvrages peuvent désormais être acheminés vers la médiathèque pour des
usagers ou bien à destination des bibliothèques anglaises.»



2000: L'INFORMATION DEVIENT MULTILINGUE


= [Résumé]

De pratiquement anglophone à ses débuts, le web, devenu multilingue en
2000, permet une large diffusion des textes électroniques sans
contrainte de frontières. Mais la barrière de la langue est loin
d’avoir disparu. Que préconise Olivier Gainon, fondateur des éditions
CyLibris? «Première étape: le respect des particularismes au niveau
technique, explique-t-il en décembre 2000. Il faut que le réseau
respecte les lettres accentuées, les lettres spécifiques, etc. Je crois
très important que les futurs protocoles permettent une transmission
parfaite de ces aspects - ce qui n’est pas forcément simple (dans les
futures évolutions de l’HTML ou des protocoles IP, etc.). Donc il faut
que chacun puisse se sentir à l’aise avec l’internet et que ce ne soit
pas simplement réservé à des (plus ou moins) anglophones. Il est
anormal aujourd’hui que la transmission d’accents puisse poser problème
dans les courriers électroniques. La première démarche me semble donc
une démarche technique. Si on arrive à faire cela, le reste en découle:
la représentation des langues se fera en fonction du nombre de
connectés, et il faudra envisager à terme des moteurs de recherche
multilingues.»


= Premiers pas

A tort ou à raison, on se plaint souvent de l’hégémonie de l’anglais
sur l’internet. Celle-ci était inévitable au début, puisque le réseau
se développe d’abord en Amérique du Nord avant de s'étendre au monde
entier. En 1997, on note déjà la présence de nombreuses langues, cette
présence dépendant du dynamisme de chaque communauté linguistique. En
décembre 1997, Tim Berners-Lee, inventeur du web, déclare à Pierre
Ruetschi, journaliste à la Tribune de Genève, un quotidien suisse:
«Pourquoi les Francophones ne mettent-ils pas davantage d’informations
sur le web? Est-ce qu’ils pensent que personne ne veut la lire, que la
culture française n’a rien à offrir? C’est de la folie, l’offre est
évidemment énorme.» C’est chose faite dans les années qui suivent.

Consultant en marketing internet de produits et services de traduction,
Randy Hobler écrit en septembre 1998: «Comme l’internet n’a pas de
frontières nationales, les internautes s’organisent selon d’autres
critères propres au médium. En termes de multilinguisme, vous avez des
communautés virtuelles, par exemple ce que j’appelle les "nations des
langues", tous ces internautes qu’on peut regrouper selon leur langue
maternelle quel que soit leur lieu géographique. Ainsi la nation de la
langue espagnole inclut non seulement les internautes d’Espagne et
d’Amérique latine, mais aussi tous les Hispanophones vivant aux Etats-
Unis, ou encore ceux qui parlent espagnol au Maroc.»

En 1998 et 1999, la nécessité d’un web multilingue occupe tous les
esprits. Au début des années 2000, le web, devenu multilingue, permet
une large diffusion des textes électroniques sans contrainte de
frontières, mais la barrière de la langue est loin d’avoir disparu. La
priorité semble être la création de passerelles entre les communautés
linguistiques pour favoriser la circulation des écrits dans d’autres
langues, en améliorant notamment les outils de traduction.

Au cours de l'été 2000, les usagers non anglophones dépassent la barre
des 50%. Ce pourcentage continue ensuite d'augmenter, comme le montrent
les statistiques de la société Global Reach, mises à jour à intervalles
réguliers. Le nombre d’usagers non anglophones est de 52,5% en été
2001, 57% en décembre 2001, 59,8% en avril 2002, 64,4% en septembre
2003 (dont 34,9% d’Européens non anglophones et 29,4% d’Asiatiques) et
64,2% en mars 2004 (dont 37,9% d’Européens non anglophones et 33%
d’Asiatiques).

Bruno Didier, webmestre de la Bibliothèque de l’Institut Pasteur, écrit
en août 1999: «Internet n’est une propriété ni nationale, ni
linguistique. C’est un vecteur de culture, et le premier support de la
culture, c’est la langue. Plus il y a de langues représentées dans leur
diversité, plus il y aura de cultures sur internet. Je ne pense pas
qu’il faille justement céder à la tentation systématique de traduire
ses pages dans une langue plus ou moins universelle. Les échanges
culturels passent par la volonté de se mettre à la portée de celui vers
qui on souhaite aller. Et cet effort passe par l’appréhension de sa
langue. Bien entendu c’est très utopique comme propos. Concrètement,
lorsque je fais de la veille, je peste dès que je rencontre des sites
norvégiens ou brésiliens sans un minimum d’anglais.»

Dès décembre 1997, le moteur de recherche AltaVista lance Babel Fish
Translation, un logiciel de traduction automatique de l’anglais vers
cinq autres langues (allemand, espagnol, français, italien, portugais),
et vice versa. Alimenté par un dictionnaire multilingue de 2,5 millions
de mots, ce service gratuit est l’oeuvre de Systran, société pionnière
en traitement automatique des langues. Le texte à traduire doit être de
trois pages maximum. La page originale et la traduction apparaissent en
vis-à-vis à l’écran. La traduction étant entièrement automatisée, elle
est évidemment approximative. Si cet outil a ses limites, il a le
mérite d’exister et il préfigure ceux des années suivantes, développés
entre autres par Systran, Alis Technologies, Globalink ou Lernout &
Hauspie.


= De l'ASCII à l'Unicode

Communiquer dans plusieurs langues implique d’avoir des systèmes de
codage adaptés à nos alphabets ou idéogrammes respectifs.

Le premier système d'encodage informatique est l’ASCII (American
standard code for information interchange). Publié en 1968 aux Etats-
Unis par l’American National Standards Institute (ANSI), avec
actualisation en 1977 et 1986, l'ASCII est un code standard de
128 caractères traduits en langage binaire sur sept bits (A est traduit
par «1000001», B est traduit par «1000010», etc.). Les 128 caractères
comprennent 33 caractères de contrôle (qui ne représentent donc pas de
symbole écrit) et 95 caractères imprimables: les 26 lettres sans accent
en majuscules (A-Z) et minuscules (a-z), les chiffres, les signes de
ponctuation et quelques symboles, le tout correspondant aux touches du
clavier anglais ou américain.

L'ASCII permet uniquement la lecture de l’anglais et du latin. Il ne
permet pas de prendre en compte les lettres accentuées présentes dans
bon nombre de langues européennes, et à plus forte raison les langues
non alphabétiques (chinois, japonais, coréen, etc.). Ceci ne pose pas
de problème majeur les premières années, tant que l’échange de fichiers
électroniques se limite essentiellement à l’Amérique du Nord. Mais le
multilinguisme devient bientôt une nécessité vitale. Des variantes de
l’ASCII (norme ISO-8859 ou ISO-Latin) prennent en compte les caractères
accentués de quelques langues européennes. Par exemple, la variante
pour le français est définie par la norme ISO-8859-1 (ISO-Latin-1).
Mais le passage de l’ASCII original à ses différentes extensions
devient vite un véritable casse-tête, y compris au sein de l’Union
européenne, les problèmes étant entre autres la multiplication des
variantes, la corruption des données dans les échanges informatiques ou
encore l’incompatibilité des systèmes, les pages ne pouvant être
affichées que dans une seule langue à la fois.

Avec le développement du web, l’échange des données s’internationalise
de plus en plus. On ne peut plus se limiter à l’utilisation de
l’anglais et de quelques langues européennes, traduites par un système
d’encodage datant de 1968.

Publié pour la première fois en janvier 1991, l’Unicode est un système
d'encodage «universel» sur 16 bits spécifiant un nombre unique pour
chaque caractère. Ce nombre est lisible quels que soient la plateforme,
le logiciel et la langue utilisés. L’Unicode peut traiter 65.000
caractères uniques et prendre en compte tous les systèmes d’écriture de
la planète. A la grande satisfaction des linguistes, il remplace
progressivement l’ASCII. L’Unicode dispose de plusieurs variantes en
fonction des besoins, par exemple UTF-8, UTF-16 et UTF-32 (UTF: Unicode
transformation format). Il devient une composante des spécifications du
W3C (World Wide Web Consortium), l'organisme international chargé du
développement du web.

L’utilisation de l’Unicode se généralise en 1998, par exemple pour les
fichiers texte sous plateforme Windows (Windows NT, Windows 2000,
Windows XP et versions suivantes), qui étaient jusque-là en ASCII. Mais
l’Unicode ne peut résoudre tous les problèmes, comme le souligne en
juin 2000 Luc Dall’Armellina, co-auteur et webmestre d’oVosite, un
espace d’écriture hypermédia: «Les systèmes d’exploitation se dotent
peu à peu des kits de langues et bientôt peut-être de polices de
caractères Unicode à même de représenter toutes les langues du monde;
reste que chaque application, du traitement de texte au navigateur web,
emboîte ce pas. Les difficultés sont immenses: notre clavier avec ses ±
250 touches avoue ses manques dès lors qu’il faille saisir des Katakana
ou Hiragana japonais, pire encore avec la langue chinoise. La grande
variété des systèmes d’écriture de par le monde et le nombre de leurs
signes font barrage. Mais les écueils culturels ne sont pas moins
importants, liés aux codes et modalités de représentation propres à
chaque culture ou ethnie.»

Que préconise Olivier Gainon, fondateur de CyLibris et pionnier de
l’édition littéraire en ligne? « Première étape: le respect des
particularismes au niveau technique, explique-t-il en décembre 2000. Il
faut que le réseau respecte les lettres accentuées, les lettres
spécifiques, etc. Je crois très important que les futurs protocoles
permettent une transmission parfaite de ces aspects - ce qui n’est pas
forcément simple (dans les futures évolutions de l’HTML ou des
protocoles IP, etc.). Donc il faut que chacun puisse se sentir à l’aise
avec l’internet et que ce ne soit pas simplement réservé à des (plus ou
moins) anglophones. Il est anormal aujourd’hui que la transmission
d’accents puisse poser problème dans les courriers électroniques. La
première démarche me semble donc une démarche technique. Si on arrive à
faire cela, le reste en découle: la représentation des langues se fera
en fonction du nombre de connectés, et il faudra envisager à terme des
moteurs de recherche multilingues.»

Yoshi Mikami est informaticien à Fujisawa, au Japon. En décembre 1995,
il lance le site "The Languages of the World by Computers and the
Internet", communément appelé Logos Home Page ou Kotoba Home Page. Son
site donne un bref historique de chaque langue, ses caractéristiques,
son système d'écriture, son jeu de caractères et enfin la configuration
du clavier dans la langue donnée. Yoshi Mikami est également co-auteur
(avec Kenji Sekine et Nobutoshi Kohara) de "Pour un web multilingue",
publié en août 1997 en japonais par les éditions O'Reilly avant d'être
traduit en anglais, en allemand et en français (version française parue
en septembre 1998).

Yoshi explique en décembre 1998: «Ma langue maternelle est le japonais.
Comme j'ai suivi mes études de troisième cycle aux Etats-Unis et que
j'ai travaillé dans l'informatique, je suis devenu bilingue
japonais/anglais américain. J'ai toujours été intéressé par différentes
langues et cultures, aussi j'ai appris le russe, le français et le
chinois dans la foulée. A la fin de 1995, j'ai créé sur le web le site
"The Languages of the World by Computers and the Internet" et j'ai
tenté de donner - en anglais et en japonais - un bref historique de
toutes ces langues, ainsi que les caractéristiques propres à chaque
langue et à sa phonétique. Suite à l'expérience acquise, j'ai invité
mes deux associés à écrire un livre sur la conception, la création et
la présentation de pages web multilingues, livre qui fut publié en août
1997 sous le titre "The Multilingual Web Guide", le premier livre au
monde sur un tel sujet.»

Comment voit-il l'évolution vers un web multilingue? «Il y a des
milliers d'années de cela, en Egypte, en Chine et ailleurs, les gens
étaient plus sensibles au fait de communiquer leurs lois et leurs
réflexions non seulement dans une langue mais dans plusieurs. Dans
notre monde moderne, chaque Etat a adopté plus ou moins une seule
langue de communication. A mon avis, l'internet verra l'utilisation
plus grande de langues différentes et de pages multilingues (et pas
seulement une gravitation autour de l'anglais américain) et un usage
plus créatif de la traduction informatique multilingue. 99% des sites
web créés au Japon sont en japonais!»


= De l'anglais au plurilinguisme

Après avoir été anglophone à pratiquement 100%, l’internet est encore
anglophone à plus de 80% en 1998, un pourcentage qui s’explique par
trois facteurs: (a) la création d’un grand nombre de sites web émanant
des Etats-Unis, du Canada et du Royaume-Uni; (b) une proportion
d'usagers particulièrement forte en Amérique du Nord par rapport au
reste du monde; (c) l’usage de l'anglais en tant que principale langue
d’échange internationale.

L’anglais reste en effet prépondérant et ceci n’est pas près de
disparaître. Comme indiqué en janvier 1999 par Marcel Grangier,
responsable de la section française des services linguistiques centraux
de l’Administration fédérale suisse, «cette suprématie n’est pas un mal
en soi, dans la mesure où elle résulte de réalités essentiellement
statistiques (plus de PC par habitant, plus de locuteurs de cette
langue, etc.). La riposte n’est pas de "lutter contre l’anglais" et
encore moins de s’en tenir à des jérémiades, mais de multiplier les
sites en d’autres langues. Notons qu’en qualité de service de
traduction, nous préconisons également le multilinguisme des sites eux-
mêmes. La multiplication des langues présentes sur internet est
inévitable, et ne peut que bénéficier aux échanges multiculturels.»

Professeur en technologies de la communication à la Webster University
de Genève, Henk Slettenhaar insiste lui aussi sur la nécessité de sites
bilingues, dans la langue originale et en anglais. «Les communautés
locales présentes sur le web devraient en tout premier lieu utiliser
leur langue pour diffuser des informations, écrit-il en décembre 1998.
Si elles veulent également présenter ces informations à la communauté
mondiale, celles-ci doivent être aussi disponibles en anglais. Je pense
qu’il existe un réel besoin de sites bilingues. (...) Mais je suis
enchanté qu’il existe maintenant tant de documents disponibles dans
leur langue originale. Je préfère de beaucoup lire l’original avec
difficulté plutôt qu’une traduction médiocre.»

Henk ajoute en août 1999: «A mon avis, il existe deux types de
recherches sur le web. La première est la recherche globale dans le
domaine des affaires et de l’information. Pour cela, la langue est
d’abord l’anglais, avec des versions locales si nécessaire. La seconde,
ce sont les informations locales de tous ordres dans les endroits les
plus reculés. Si l’information est à destination d’une ethnie ou d’un
groupe linguistique, elle doit d’abord être dans la langue de l’ethnie
ou du groupe, avec peut-être un résumé en anglais.»

Philippe Loubière, traducteur littéraire et dramatique, dénonce pour sa
part la main-mise anglophone sur le réseau. «Tout ce qui peut
contribuer à la diversité linguistique, sur internet comme ailleurs,
est indispensable à la survie de la liberté de penser, explique-t-il en
mars 2001. Je n’exagère absolument pas: l’homme moderne joue là sa
survie. Cela dit, je suis très pessimiste devant cette évolution. Les
Anglo-saxons vous écrivent en anglais sans vergogne. L’immense majorité
des Français constate avec une indifférence totale le remplacement
progressif de leur langue par le mauvais anglais des marchands et des
publicitaires, et le reste du monde a parfaitement admis l’hégémonie
linguistique des Anglo-saxons parce qu’ils n’ont pas d’autres horizons
que de servir ces riches et puissants maîtres. La seule solution
consisterait à recourir à des législations internationales assez
contraignantes pour obliger les gouvernements nationaux à respecter et
à faire respecter la langue nationale dans leur propre pays (le
français en France, le roumain en Roumanie, etc.), cela dans tous les
domaines et pas seulement sur internet. Mais ne rêvons pas...»

Richard Chotin, professeur à l’Ecole supérieure des affaires (ESA) de
Lille, rappelle à juste titre que la suprématie de l’anglais a succédé
à celle du français. «Le problème est politique et idéologique: c’est
celui de l’"impérialisme" de la langue anglaise découlant de
l’impérialisme américain, explique-t-il en septembre 2000. Il suffit
d’ailleurs de se souvenir de l’"impérialisme" du français aux 18e et
19e siècles pour comprendre la déficience en langues des étudiants
français: quand on n’a pas besoin de faire des efforts pour se faire
comprendre, on n’en fait pas, ce sont les autres qui les font.»

Guy Antoine, créateur de Windows on Haiti, site de référence sur la
culture haïtienne, croit en la nécessité de l'anglais en tant que
langue commune. Il relate en novembre 1999: «Pour des raisons
pratiques, l'anglais continuera à dominer le web. Je ne pense pas que
ce soit une mauvaise chose, en dépit des sentiments régionalistes qui
s'y opposent, parce que nous avons besoin d'une langue commune
permettant de favoriser les communications à l'échelon international.
Ceci dit, je ne partage pas l'idée pessimiste selon laquelle les autres
langues n'ont plus qu'à se soumettre à la langue dominante. Au
contraire. Tout d'abord l'internet peut héberger des informations
utiles sur les langues minoritaires, qui seraient autrement amenées à
disparaître sans laisser de traces. De plus, à mon avis, l'internet
incite les gens à apprendre les langues associées aux cultures qui les
intéressent. Ces personnes réalisent rapidement que la langue d'un
peuple est un élément fondamental de sa culture. De ce fait, je n'ai
pas grande confiance dans les outils de traduction automatique qui,
s'ils traduisent les mots et les expressions, ne peuvent guère traduire
l'âme d'un peuple. Que sont les Haïtiens, par exemple, sans le kreyòl
(créole pour les non initiés), une langue qui s'est développée et qui a
permis de souder entre elles diverses tribus africaines transplantées à
Haïti pendant la période de l'esclavage? Cette langue représente de
manière la plus palpable l'unité de notre peuple. Elle est toutefois
principalement une langue parlée et non écrite. A mon avis, le web va
changer cet état de fait plus qu'aucun autre moyen traditionnel de
diffusion d'une langue. Dans Windows on Haiti, la langue principale est
l'anglais, mais on y trouve tout aussi bien un forum de discussion
animé conduit en kreyòl. Il existe aussi des documents sur Haïti en
français et dans l'ancien créole colonial, et je suis prêt à publier
d'autres documents en espagnol et dans diverses langues. Je ne propose
pas de traductions, mais le multilinguisme est effectif sur ce site, et
je pense qu'il deviendra de plus en plus la norme sur le web.»

Bakayoko Bourahima, bibliothécaire de l'Ecole nationale supérieure de
statistique et d’économie appliquée (ENSEA) à Abidjan, écrit en juillet
2000: «Pour nous les Africains francophones, le diktat de l’anglais sur
la toile représente pour la masse un double handicap d’accès aux
ressources du réseau. Il y a d’abord le problème de l’alphabétisation
qui est loin d’être résolu et que l’internet va poser avec beaucoup
plus d’acuité, ensuite se pose le problème de la maîtrise d’une seconde
langue étrangère et son adéquation à l’environnement culturel. En
somme, à défaut de multilinguisme, l’internet va nous imposer une
seconde colonisation linguistique avec toutes les contraintes que cela
suppose. Ce qui n’est pas rien quand on sait que nos systèmes éducatifs
ont déjà beaucoup de mal à optimiser leurs performances, en raison,
selon certains spécialistes, des contraintes de l’utilisation du
français comme langue de formation de base. Il est donc de plus en plus
question de recourir aux langues vernaculaires pour les formations de
base, pour "désenclaver" l’école en Afrique et l’impliquer au mieux
dans la valorisation des ressources humaines. Comment faire? Je pense
qu’il n’y a pas de chance pour nous de faire prévaloir une quelconque
exception culturelle sur la toile, ce qui serait de nature tout à fait
grégaire. Il faut donc que les différents blocs linguistiques
s’investissent beaucoup plus dans la promotion de leur accès à la
toile, sans oublier leurs différentes spécificités internes.»

Tôt ou tard, le pourcentage des langues sur le réseau correspondra-t-il
à leur répartition sur la planète? Rien n’est moins sûr à l’heure de la
fracture numérique entre riches et pauvres, entre zones rurales et
zones urbaines, entre régions favorisées et régions défavorisées, entre
l’hémisphère nord et l’hémisphère sud, entre pays développés et pays en
développement.

Selon Zina Tucsnak, ingénieur d’études au laboratoire ATILF (Analyse et
traitement informatique de la langue française), interviewée en octobre
2000, «le meilleur moyen serait l’application d’une loi par laquelle on
va attribuer un "quota" à chaque langue. Mais n’est-ce pas une utopie
de demander l’application d’une telle loi dans une société de
consommation comme la nôtre?»

A la même date, Emmanuel Barthe, documentaliste juridique, exprime un
avis contraire: «Des signes récents laissent penser qu’il suffit de
laisser les langues telles qu’elles sont actuellement sur le web. En
effet, les langues autres que l’anglais se développent avec
l’accroissement du nombre de sites web nationaux s’adressant
spécifiquement aux publics nationaux, afin de les attirer vers
internet. Il suffit de regarder l’accroissement du nombre de langues
disponibles dans les interfaces des moteurs de recherche généralistes.»


= Langues minoritaires

De plus, l’«impérialisme» dénoncé plus haut ne concerne pas seulement
les Etats-Unis et la langue anglaise. La France elle aussi n’est pas
sans exercer pression pour imposer la suprématie de la langue française
sur d’autres langues, comme en témoigne Guy Antoine, créateur du site
Windows on Haiti. Il relate en juin 2001: «J’ai fait de la promotion du
kreyòl (créole haïtien) une cause personnelle, puisque cette langue est
le principal lien unissant tous les Haïtiens, malgré l’attitude
dédaigneuse d’une petite élite haïtienne - à l’influence
disproportionnée - vis-à-vis de l’adoption de normes pour l’écriture du
kreyòl et le soutien de la publication de livres et d’informations
officielles dans cette langue. A titre d’exemple, il y avait récemment
dans la capitale d’Haïti un Salon du livre de deux semaines, à qui on
avait donné le nom de "Livres en folie". Sur les 500 livres d’auteurs
haïtiens qui étaient présentés lors du salon, il y en avait une
vingtaine en kreyòl, ceci dans le cadre de la campagne insistante que
mène la France pour célébrer la Francophonie dans ses anciennes
colonies. A Haïti cela se passe relativement bien, mais au détriment
direct de la Créolophonie.

En réponse à l’attitude de cette minorité haïtienne, j’ai créé sur mon
site web Windows on Haiti deux forums de discussion exclusivement en
kreyòl. Le premier forum regroupe des discussions générales sur toutes
sortes de sujets, mais en fait ces discussions concernent
principalement les problèmes socio-politiques qui agitent Haïti. Le
deuxième forum est uniquement réservé aux débats sur les normes
d’écriture du kreyòl. Ces débats sont assez animés, et un certain
nombre d’experts linguistiques y participent. Le caractère exceptionnel
de ces forums est qu’ils ne sont pas académiques. Je n’ai trouvé nulle
part ailleurs sur l’internet un échange aussi spontané et aussi libre
entre des experts et le grand public pour débattre dans une langue
donnée des attributs et des normes de la même langue.»

En septembre 2000, Guy Antoine rejoint l’équipe dirigeante de Mason
Integrated Technologies, dont l’objectif est de créer des outils
permettant l’accessibilité des documents publiés dans des langues dites
minoritaires. «Etant donné l’expérience de l’équipe en la matière, nous
travaillons d’abord sur le créole haïtien (kreyòl), qui est la seule
langue nationale d’Haïti, et l’une des deux langues officielles,
l’autre étant le français. Cette langue ne peut guère être considérée
comme une langue minoritaire dans les Caraïbes puisqu’elle est parlée
par huit à dix millions de personnes.»

Autre expérience, celle de Caoimhín Ó Donnaíle, professeur
d’informatique à l’Institut Sabhal Mór Ostaig, situé sur l’île de Skye,
en Ecosse. Caoimhín dispense ses cours en gaélique écossais. Il est
aussi le webmestre du site de l’institut, qui est bilingue anglais-
gaélique et se trouve être la principale source d’information mondiale
sur le gaélique écossais. Sur ce site, il tient à jour la page
"European Minority Languages", une liste de langues européennes
minoritaires elle aussi bilingue, avec classement par ordre
alphabétique de langues et par famille linguistique. Interviewé en mai
2001, Caoimhín raconte: «Nos étudiants utilisent un correcteur
d’orthographe en gaélique et une base terminologique en ligne en
gaélique. (...) Il est maintenant possible d’écouter la radio en
gaélique (écossais et irlandais) en continu sur l’internet partout dans
le monde. Une réalisation particulièrement importante a été la
traduction en gaélique du logiciel de navigation Opera. C’est la
première fois qu’un logiciel de cette taille est disponible en
gaélique.»

En février 2000, Robert Beard co-fonde yourDictionary.com en tant que
portail de référence pour toutes les langues sans exception, avec une
section spécifique consacrée aux langues menacées (Endangered Language
Repository). «Les langues menacées sont essentiellement des langues non
écrites, écrit-il en janvier 2000. Un tiers seulement des quelque 6.000
langues existant dans le monde sont à la fois écrites et parlées. Je ne
pense pourtant pas que le web va contribuer à la perte de l’identité
des langues et j’ai même le sentiment que, à long terme, il va
renforcer cette identité. Par exemple, de plus en plus d’Indiens
d’Amérique contactent des linguistes pour leur demander d’écrire la
grammaire de leur langue et de les aider à élaborer des dictionnaires.
Pour eux, le web est un instrument à la fois accessible et très
précieux d’expression culturelle.»

Caoimhín Ó Donnaíle indique pour sa part en mai 2001: «En ce qui
concerne l’avenir des langues menacées, l’internet accélère les choses
dans les deux sens. Si les gens ne se soucient pas de préserver les
langues, l’internet et la mondialisation qui l’accompagne accéléreront
considérablement la disparition de ces langues. Si les gens se soucient
vraiment de les préserver, l’internet constituera une aide
irremplaçable.»


= Traductions

L’internet étant une source d’information à vocation mondiale, il
semble indispensable de favoriser les activités de traduction. Auteur
des Chroniques de Cybérie, une chronique hebdomadaire en ligne des
actualités du réseau, Jean-Pierre Cloutier déplore en août 1999 «qu’il
se fasse très peu de traductions des textes et essais importants qui
sont publiés sur le web, tant de l’anglais vers d’autres langues que
l’inverse. (...) La nouveauté d’internet dans les régions où il se
déploie présentement y suscite des réflexions qu’il nous serait utile
de lire. À quand la traduction des penseurs hispanophones et autres de
la communication?»

Professeur d’espagnol en entreprise et traductrice, Maria Victoria
Marinetti écrit à la même date: «Il est très important de pouvoir
communiquer en différentes langues. Je dirais même que c’est
obligatoire, car l’information donnée sur le net est à destination du
monde entier, alors pourquoi ne l’aurions-nous pas dans notre propre
langue ou dans la langue que nous souhaitons lire? Information
mondiale, mais pas de vaste choix dans les langues, ce serait
contradictoire, pas vrai?»

Une solution résidera peut-être dans l'utilisation à grande échelle des
logiciels de traduction automatique, dont on nous promet l'amélioration
d'ici quelques années. Il va sans dire que, pour le moment, la
traduction automatique n’offre pas la qualité de travail des
professionnels de la traduction, et qu’il est préférable de faire appel
à ces derniers lorsqu'on a le temps et l’argent nécessaires. Mais qui a
l’argent nécessaire pour faire traduire des centaines sinon des
milliers de pages web et, qui plus est, en plusieurs langues? Les
logiciels de traduction sont donc très pratiques pour fournir un
résultat immédiat et à moindres frais, sinon gratuit, même si celui-ci
est très imparfait. Depuis 1998, des logiciels sont en accès libre sur
le web - par exemple ceux de Systran, d'Alis technologies ou de Google
- et permettent de traduire en quelques secondes une page web ou un
texte court, avec plusieurs combinaisons de langues possibles.

Le but d’un logiciel de traduction est d’analyser le texte dans la
langue source (texte à traduire) et de générer automatiquement le texte
correspondant dans la langue cible (texte traduit), en utilisant des
règles précises pour le transfert de la structure grammaticale. Comme
l’explique l’EAMT (European Association for Machine Translation) sur
son site, «il existe aujourd’hui un certain nombre de systèmes
produisant un résultat qui, s’il n’est pas parfait, est de qualité
suffisante pour être utile dans certaines applications spécifiques, en
général dans le domaine de la documentation technique. De plus, les
logiciels de traduction, qui sont essentiellement destinés à aider le
traducteur humain à produire des traductions, jouissent d’une
popularité croissante auprès des organismes professionnels de
traduction.»

La tâche est immense. Comme le souligne en février 2001 Pierre-Noël
Favennec, expert à la direction scientifique de France Télécom R&D,
«les recherches sur la traduction automatique devraient permettre une
traduction automatique dans les langues souhaitées, mais avec des
applications pour toutes les langues et non les seules dominantes (ex.:
diffusion de documents en japonais, si l’émetteur est de langue
japonaise, et lecture en breton, si le récepteur est de langue
bretonne...). Il y a donc beaucoup de travaux à faire dans le domaine
de la traduction automatique et écrite de toutes les langues.»



2001: COPYRIGHT, COPYLEFT ET CREATIVE COMMONS


= [Résumé]

Lancée en 2001 à l'initiative de Lawrence Lessig, professeur de droit à
la Stanford Law School (Californie), la licence Creative Commons a pour
but de favoriser la diffusion d'oeuvres numériques tout en protégeant
le droit d'auteur. L'organisme du même nom propose des licences-type,
qui sont des contrats flexibles de droit d'auteur compatibles avec une
diffusion sur l'internet. Simplement rédigées, ces autorisations non
exclusives permettent aux titulaires des droits d'autoriser le public à
utiliser leurs créations tout en ayant la possibilité de restreindre
les exploitations commerciales et les oeuvres dérivées. L'auteur peut
par exemple choisir d'autoriser ou non la reproduction et la
rediffusion de ses oeuvres. Ces contrats peuvent être utilisés pour
tout type de création: texte, film, photo, musique, site web, etc.
Finalisée en février 2007, la version 3.0 de la Creative Commons
instaure une licence internationale et la compatibilité avec d'autres
licences similaires, dont le copyleft et la GPL (general public
license).


= Internet et droit d'auteur

Si le débat relatif au droit d’auteur sur l’internet est vif à la fin
des années 1990, Philippe Loubière, traducteur littéraire et
dramatique, ramène ce débat aux vrais problèmes. «Ce débat me semble
assez proche sur le fond de ce qu’il est dans les autres domaines où le
droit d’auteur s’exerce, ou devrait s’exercer, écrit-il en mars 2001.
Le producteur est en position de force par rapport à l’auteur dans
pratiquement tous les cas de figure. Les pirates, voire la simple
diffusion libre, ne menacent vraiment directement que les producteurs.
Les auteurs ne sont menacés que par ricochet. Il est possible que l’on
puisse légiférer sur la question, au moins en France où les
corporations se revendiquant de l’exception culturelle sont actives et
résistent encore un peu aux Américains, mais le mal est plus profond.
En effet, en France comme ailleurs, les auteurs étaient toujours les
derniers et les plus mal payés avant l’apparition d’internet, on
constate qu’ils continuent d’être les derniers et les plus mal payés
depuis. Il me semble nécessaire que l’on règle d’abord la question du
respect des droits d’auteur en amont d’internet. Déjà dans le cadre
général de l’édition ou du spectacle vivant, les sociétés d’auteurs -
SACD (Société des auteurs et compositeurs dramatiques), SGDL (Société
des gens de lettres), SACEM (Société des auteurs, compositeurs et
éditeurs de musique), etc. - faillissent dès lors que l’on sort de la
routine ou du vedettariat, ou dès que les producteurs abusent de leur
position de force, ou tout simplement ne payent pas les auteurs, ce qui
est très fréquent.»

Pour nombre d'auteurs, le web est avant tout un espace public basé sur
l'échange. Alain Bron, consultant en systèmes d'information et auteur
de romans, écrit en novembre 1999: «Je considère aujourd'hui le web
comme un domaine public. Cela veut dire que la notion de droit d'auteur
sur ce média disparaît de facto: tout le monde peut reproduire tout le
monde. La création s'expose donc à la copie immédiate si les copyrights
ne sont pas déposés dans les formes usuelles et si les oeuvres sont
exposées sans procédures de revenus.»

Jacques Gauchey, journaliste et spécialiste des technologies de
l'information, exprime un avis différent. «Le droit d'auteur dans son
contexte traditionnel n'existe plus, écrit-il en juillet 1999. Les
auteurs ont besoin de s'adapter à un nouveau paradigme, celui de la
liberté totale du flot de l'information. Le contenu original est comme
une empreinte digitale: il est incopiable. Il survivra et prospérera
donc.»

Selon Xavier Malbreil, auteur multimédia interviewé en mars 2001, «il y
a deux choses. Le web ne doit pas être un espace de non-droit, et c'est
un principe qui doit s'appliquer à tout, et notamment au droit
d'auteur. Toute utilisation commerciale d'une oeuvre doit ouvrir droit
à rétribution. Mais également, le web est un lieu de partage. Echanger
entre amis des passages d'un texte qui vous a plu, comme on peut
recopier des passages d'un livre particulièrement apprécié, pour le
faire aimer, cela ne peut faire que du bien aux oeuvres, et aux
auteurs. La littérature souffre surtout de ne pas être diffusée. Tout
ce qui peut concourir à la faire sortir de son ghetto sera positif.»


= Copyleft et Creative Commons

Des auteurs et autres créateurs souhaitent respecter la vocation
première du web, réseau de diffusion à l’échelon mondial. De ce fait,
les adeptes de contrats flexibles - copyleft, GPL (general public
license) et Creative Commons - sont de plus en plus nombreux.

L'idée du copyleft est lancée dès 1984 par Richard Stallman, ingénieur
en informatique et défenseur inlassable du mouvement Open Source au
sein de la Free Software Foundation (FSF). Conçu à l’origine pour les
logiciels, le copyleft est formalisé par la GPL (general public
license) et étendu par la suite à toute oeuvre de création. Il contient
la déclaration normale du copyright affirmant le droit d'auteur. Son
originalité est de donner au lecteur le droit de librement redistribuer
le document et de le modifier. Le lecteur s’engage toutefois à ne
revendiquer ni le travail original, ni les changements effectués par
d’autres personnes. De plus, tous les travaux dérivés de l’oeuvre
originale sont eux-mêmes soumis au copyleft.

Lancée en 2001 à l'initiative de Lawrence Lessig, professeur de droit à
la Stanford Law School, en Californie, la licence Creative Commons a
elle aussi pour but de favoriser la diffusion d'oeuvres numériques tout
en protégeant le droit d'auteur. L'organisme du même nom propose des
licences-type, qui sont des contrats flexibles de droit d'auteur
compatibles avec une diffusion sur l'internet. Simplement rédigées, ces
autorisations non exclusives permettent aux titulaires des droits
d'autoriser le public à utiliser leurs créations tout en ayant la
possibilité de restreindre les exploitations commerciales et les
oeuvres dérivées. L'auteur peut par exemple choisir d'autoriser ou non
la reproduction et la rediffusion de ses oeuvres. Ces contrats peuvent
être utilisés pour tout type de création: texte, film, photo, musique,
site web, etc. Finalisée en février 2007, la version 3.0 de la Creative
Commons instaure une licence internationale et la compatibilité avec
d'autres licences similaires, dont le copyleft et la GPL.

Une licence Creative Commons est utilisée pour un million d'oeuvres en
2003, 4,7 millions d'oeuvres en 2004, 20 millions d'oeuvres en 2005, 50
millions d'oeuvres en 2006, 90 millions d'oeuvres en 2007 et 130
millions d'oeuvres en 2008.

En complément, Science Commons est fondé en 2005 pour définir les
stratégies et les outils nécessaires à la diffusion sur le web de la
recherche scientifique, et ccLearn est fondé en 2007 en vue de définir
des stratégies et outils pour l'enseignement.


= Domaine public et copyright

Chose inquiétante à l’heure d’une société dite de l’information, le
domaine public se réduit comme peau de chagrin. A une époque qui n'est
pas si lointaine, 50% des oeuvres appartenaient au domaine public, et
pouvaient donc être librement utilisées par tous. D'ici 2100, 99% des
oeuvres seraient régies par le droit d’auteur, avec un maigre 1% laissé
au domaine public. Un problème épineux pour tous ceux qui gèrent des
bibliothèques numériques, et qui affecte aussi bien le Projet Gutenberg
que Google Books.

Si le Projet Gutenberg s’est donné pour mission de diffuser
gratuitement par voie électronique le plus grand nombre possible
d’oeuvres du domaine public, sa tâche n’est guère facilitée par les
coups de boutoir portés au domaine public. Michael Hart, son fondateur,
se penche sur la question depuis plus de trente ans, avec l’aide d’un
groupe d’avocats spécialisés dans le droit d’auteur.

Dans la section Copyright HowTo, le Projet Gutenberg détaille les
calculs à faire pour déterminer si un titre publié aux Etats-Unis
appartient ou non au domaine public. Les oeuvres publiées avant 1923
sont soumises au droit d’auteur pendant 75 ans à partir de leur date de
publication (elles sont donc maintenant dans le domaine public). Les
oeuvres publiées entre 1923 et 1977 sont soumises au droit d’auteur
pendant 95 ans à partir de leur date de publication (rien ne tombera
dans le domaine public avant 2019). Une oeuvre publiée en 1998 et les
années suivantes est soumise au droit d’auteur pendant 70 ans à partir
de la date du décès de l’auteur s’il s’agit d’un auteur personnel (rien
dans le domaine public avant 2049), ou alors pendant 95 ans à partir de
la date de publication - ou 120 ans à partir de la date de création -
s’il s’agit d’un auteur collectif (rien dans le domaine public avant
2074). Tout ceci dans les grandes lignes. D’autres règles viennent
s’ajouter à ces règles de base, la loi sur le copyright ayant été
amendée plusieurs fois fois depuis 1971, date de fondation du Projet
Gutenberg.

Nettement plus contraignant que l'amendement précédent, une nouveau
renforcement du copyright est entériné par le Congrès le 27 octobre
1998 pour contrer le formidable véhicule de diffusion qu'est
l'internet. Au fil des siècles, chaque avancée technique est
accompagnée d'un durcissement du copyright, qui semble être la réponse
des éditeurs à un accès plus facile au savoir, et la peur afférente de
perdre des royalties. «Le copyright a été augmenté de 20 ans, explique
Michael Hart en juillet 1999. Auparavant on devait attendre 75 ans, on
est maintenant passé à 95 ans. Bien avant, le copyright durait 28 ans
(plus une extension de 28 ans si on la demandait avant l’expiration du
délai) et, avant cela, le copyright durait 14 ans (plus une extension
de 14 ans si on la demandait avant l’expiration du délai). Comme on le
voit, on assiste à une dégradation régulière et constante du domaine
public.»

Les dates évoquées par Michael sont les suivantes, comme expliqué en
détail dans son blog:

(a) 1790 est la date de la main-mise de la Guilde des imprimeurs (les
éditeurs de l’époque en Angleterre) sur les auteurs, qui entraîne la
naissance du copyright. Le 1790 Copyright Act institue un copyright de
14 ans après la date de publication de l’oeuvre, plus une extension de
28 ans si celle-ci est demandée avant l’expiration du délai. Les
oeuvres pouvant être légalement imprimées passent subitement de 6.000 à
600, et neuf titres sur dix disparaissent des librairies. Quelque 335
ans après les débuts de l'imprimerie, censée ouvrir les portes du
savoir à tous, le monde du livre est désormais contrôlé par les
éditeurs et non plus par les auteurs. Cette nouvelle législation est
également effective aux Etats-Unis et en France.

(b) 1831 est la date d'un premier renforcement du copyright pour
contrer la réédition de vastes collections du domaine public sur les
nouvelles presses à vapeur. Le 1831 Copyright Act institue un copyright
de 28 ans après la date de publication de l’oeuvre, plus une extension
de 14 ans si celle-ci est demandée avant l’expiration du délai, à
savoir un total de 42 ans.

(c) 1909 est la date d'un deuxième renforcement du copyright pour
contrer une réédition des collections du domaine public sur les
nouvelles presses électriques. Le 1909 Copyright Act double la période
de l’extension, qui passe à 28 ans, le tout représentant un total de 56
ans.

(d) 1976 est la date d’un nouveau durcissement du copyright suite
l’apparition de la photocopieuse lancée par Xerox. Le 1976 Copyright
Act institue un copyright de 50 ans après le décès de l’auteur. De ce
fait, tout copyright en cours avant le 19 septembre 1962 n’expire pas
avant le 31 décembre 1976.

(e) 1998 est la date d’un durcissement supplémentaire du copyright
suite au développement rapide des technologies numériques et aux
centaines de milliers d'oeuvres désormais disponibles sur CD et DVD et
sur le web, gratuitement ou à un prix très bas. Le 1998 Copyright Act
allonge la durée du copyright qui est désormais de 70 ans après le
décès de l’auteur, pour protéger l'empire Disney (raison pour laquelle
on parle souvent de Mickey Mouse Copyright Act) et nombre de
multinationales culturelles.

Pour ne prendre qu'un exemple, le classique mondial "Gone With the
Wind" ("Autant en emporte le vent") de Margaret Mitchell, publié en
1939, aurait dû tomber dans le domaine public au bout de 56 ans, en
1995, conformément à la législation de l'époque, libérant ainsi les
droits pour les adaptations en tous genres. Suite aux législations de
1976 et 1998, ce classique ne devrait désormais tomber dans le domaine
public qu'en 2035.

La législation de 1998 porte un coup très rude aux bibliothèques
numériques, en plein essor avec le développement du web, et
scandalisent ceux qui les gèrent, à commencer par Michael Hart et John
Mark Ockerbloom, créateur de l'Online Books Page. Mais comment faire le
poids vis-à-vis des majors de l’édition? Nombre de titres doivent être
retirés des collections.

Michael raconte en juillet 1999: «J’ai été le principal opposant aux
extensions du copyright, mais Hollywood et les grands éditeurs ont fait
en sorte que le Congrès ne mentionne pas mon action en public. Les
débats actuels sont totalement irréalistes. Ils sont menés par
“l’aristocratie terrienne de l’âge de l’information” et servent
uniquement ses intérêts. Un âge de l’information? Et pour qui?»

En effet. Les instances politiques ne cessent de parler d’âge de
l’information alors que, en parallèle, elles durcissent la
réglementation relative à la mise à disposition de cette information.
La contradiction est flagrante. Le copyright est passé d'une durée de
30 ans en moyenne en 1909 à une durée de 95 ans en moyenne en 1998,
explique aussi Michael dans son blog. En 89 ans, de 1909 à 1998, le
copyright a subi une extension de 65 ans qui affecte les trois quarts
de la production du 20e siècle. Seul un livre publié avant 1923 peut
être considéré avec certitude comme du domaine public.

Un durcissement similaire touche les pays de l'Union européenne. La
règle générale est désormais un copyright de 70 ans après le décès de
l’auteur, alors qu’il était auparavant de 50 ans, suite aux pressions
exercées par les éditeurs de contenu sous le prétexte d’«harmoniser»
les lois nationales relatives au copyright pour répondre à la
mondialisation du marché.

A ceci s'ajoute la législation sur le copyright des éditions numériques
en application des traités internationaux de l'OMPI (Organisation
mondiale de la propriété intellectuelle). Ces traités sont signés en
1996 dans l'optique de contrôler la gestion des droits numériques. Le
Digital Millenium Copyright Act (DMCA) est entériné en octobre 1998 aux
Etats-Unis. La directive EUCD (European Union Copyright Directive) est
entérinée en mai 2001 par la Communauté européenne. Cette directive
s'intitule très précisément «Directive 2001/29/EC du Parlement européen
et du Conseil sur l'harmonisation de certains aspects du droit d'auteur
et des droits voisins dans la société de l'information». Elle fait
suite à la directive de février 1993 (Directive 93/98/EEC) qui visait à
harmoniser les législations des différents pays en matière de
protection du droit d'auteur. La directive EUCD entre peu à peu en
vigueur dans tous les pays de l'Union européenne, avec mise en place de
législations nationales, le but officiel étant de renforcer le respect
du droit d'auteur sur l'internet et de contrer ainsi le piratage. En
France, par exemple, la loi DADVSI (Droit d'auteur et droits voisins
dans la société de l'information) est promulguée en août 2006, et n'est
pas sans susciter de nombreux remous.



2002: LE WEB DEVIENT UNE VASTE ENCYCLOPEDIE


= [Résumé]

En 2002, le MIT (Massachusetts Institute of Technology) décide de
publier le contenu de ses cours en ligne, avec accès libre et gratuit,
en privilégiant la diffusion libre du savoir. Mise en ligne en
septembre 2002, la version pilote du MIT OpenCourseWare (MIT OCW) offre
en accès libre le matériel d’enseignement de 32 cours représentatifs
des cinq facultés du MIT. Ce matériel d’enseignement comprend des
textes de conférences, des travaux pratiques, des exercices et
corrigés, des bibliographies, des documents audio et vidéo, etc.
Parallèlement, la Public Library of Science (PLoS) met sur pied des
revues scientifiques en ligne de haut niveau. En ce qui concerne les
encyclopédies, Wikipédia ouvre la voie en 2001, en lançant une
encyclopédie écrite collectivement et dont le contenu est librement
réutilisable. Wikipédia est rédigé par des milliers de volontaires,
avec possibilité de corriger et de compléter les articles, aussi bien
les leurs que ceux d'autres contributeurs. Suivent d'autres
encyclopédies comme Citizendium et l'Encyclopedia of Life.


= Vers un savoir numérique

Sur le site de l'Internet Society (ISOC), qu'il fonde en 1992 pour
promouvoir le développement de l’internet, Vinton Cerf explique: «Le
réseau fait deux choses (...): comme les livres, il permet d’accumuler
de la connaissance. Mais, surtout, il la présente sous une forme qui la
met en relation avec d’autres informations. Alors que, dans un livre,
l’information est maintenue isolée.»

De plus, l’information contenue dans les livres reste la même, au moins
pendant une période donnée, alors que l'internet privilégie
l’information la plus récente et constamment actualisée.

Lors d'une conférence organisée par l'IFIP (International Federation of
Information Processing) en septembre 1996, Dale Spender, professeur et
chercheuse, tente de cerner les changements fondamentaux apportés par
l'internet dans l'acquisition du savoir et les méthodes d'enseignement.
Voici son argumentation résumée en deux paragraphes.

Pendant plus de cinq siècles, l'enseignement est principalement basé
sur l'information donnée par les livres. Or les habitudes liées à
l'imprimé ne peuvent être transférées au monde numérique.
L'enseignement en ligne offre des possibilités tellement nouvelles
qu'il n'est guère possible d'effectuer les distinctions traditionnelles
entre enseignant et enseigné. Le passage de la culture imprimée à la
culture numérique exige d'entièrement repenser le processus
d'enseignement, puisque nous avons maintenant l'opportunité sans
précédent de pouvoir influer sur le genre d'enseignement que nous
souhaitons.

Dans la culture imprimée, l'information contenue dans les livres
restait la même un certain temps, ce qui nous a encouragé à penser que
l'information était stable. La nature même de l'imprimé est liée à la
notion de vérité, stable elle aussi. Cette stabilité et l'ordre qu'elle
engendre ont été un des fondements de l'âge industriel et de la
révolution scientifique. Les notions de vérité, de lois, d'objectivité
et de preuve ont été les éléments de référence de nos croyances et de
nos cultures. Mais la révolution numérique change tout ceci. Soudain
l'information en ligne supplante l'information imprimée pour devenir la
plus fiable et la plus utile, et l'usager est prêt à la payer en
conséquence. C'est cette transformation radicale dans la nature de
l'information qui doit être au coeur du débat concernant les méthodes
d'enseignement.

En témoigne l'expérience de Russon Wooldridge, professeur au
département des études françaises de l'Université de Toronto (Canada),
qui relate en février 2001: «Tout mon enseignement exploite au maximum
les ressources d'internet (le web et le courriel): les deux lieux
communs d'un cours sont la salle de classe et le site du cours, sur
lequel je mets tous les matériaux des cours. Je mets toutes les données
de mes recherches des vingt dernières années sur le web (réédition de
livres, articles, textes intégraux de dictionnaires anciens en bases de
données interactives, de traités du 16e siècle, etc.). Je publie des
actes de colloques, j'édite un journal, je collabore avec des collègues
français, mettant en ligne à Toronto ce qu'ils ne peuvent pas publier
en ligne chez eux. En mai 2000 j'ai organisé à Toronto un colloque
international sur "Les études françaises valorisées par les nouvelles
technologies". (...)

Je me rends compte que sans internet mes activités seraient bien
moindres, ou du moins très différentes de ce qu'elles sont
actuellement. Donc je ne vois pas l'avenir sans. Mais il est crucial
que ceux qui croient à la libre diffusion des connaissances veillent à
ce que le savoir ne soit pas bouffé, pour être vendu, par les intérêts
commerciaux. Ce qui se passe dans l'édition du livre en France, où on
n'offre guère plus en librairie que des manuels scolaires ou pour
concours (c'est ce qui s'est passé en linguistique, par exemple), doit
être évité sur le web. Ce n'est pas vers les amazon.com qu'on se tourne
pour trouver la science désintéressée. Sur mon site, je refuse toute
sponsorisation.»


= Quelques projets pilotes

# Cours du MIT

Professeur à l’Université d’Ottawa (Canada), Christian Vandendorpe
salue en mai 2001 «la décision du MIT (Massachusetts Institute of
Technology) de placer tout le contenu de ses cours sur le web d’ici dix
ans, en le mettant gratuitement à la disposition de tous. Entre les
tendances à la privatisation du savoir et celles du partage et de
l’ouverture à tous, je crois en fin de compte que c’est cette dernière
qui va l’emporter.» Le MIT décide en effet de publier le contenu de ses
cours en ligne, avec accès libre et gratuit, une initiative menée avec
le soutien financier de la Hewlett Foundation et de la Mellon
Foundation.

Mise en ligne en septembre 2002, la version pilote du MIT
OpenCourseWare (MIT OCW) offre en accès libre le matériel
d’enseignement de 32 cours représentatifs des cinq facultés du MIT. Ce
matériel d’enseignement comprend des textes de conférences, des travaux
pratiques, des exercices et corrigés, des bibliographies, des documents
audio et vidéo, etc. Le lancement officiel du site a lieu un an plus
tard, en septembre 2003, avec accès à quelques centaines de cours. En
mars 2004, 500 cours sont disponibles dans 33 disciplines. En mai 2006,
1.400 cours sont disponibles dans 34 disciplines. La totalité des 1.800
cours dispensés par le MIT est en ligne en novembre 2007, avec
actualisation régulière. Certains cours sont traduits en espagnol, en
portugais et en chinois avec l’aide d’autres organismes.

Le MIT espère que cette expérience de publication électronique - la
première du genre - va permettre de définir un standard et une méthode
de publication, et inciter d’autres universités à créer un
«opencourseware» pour la mise à disposition gratuite de leurs propres
cours. Un «opencourseware» peut être défini comme la publication
électronique en accès libre du matériel d’enseignement d'un ensemble de
cours. A cet effet, le MIT lance l’OpenCourseWare Consortium (OCW
Consortium) en décembre 2005, avec accès libre et gratuit au matériel
d’enseignement de cent universités dans le monde un an plus tard.

# Public Library of Science

A l’heure de l’internet, il paraît assez scandaleux que le résultat de
travaux de recherche – travaux originaux et demandant de longues années
d’efforts – soit détourné par des éditeurs spécialisés s’appropriant ce
travail et le monnayant à prix fort. L’activité des chercheurs est
souvent financée par les deniers publics, et de manière substantielle
en Amérique du Nord. Il semblerait donc normal que la communauté
scientifique et le grand public puissent bénéficier librement du
résultat de ces recherches.

Dans le domaine scientifique et médical par exemple, 1.000 nouveaux
articles sont publiés chaque jour, en ne comptant que les articles
révisés par les pairs. Se basant sur ce constat, la Public Library of
Science (PLoS) est fondée en octobre 2000 à San Francisco à
l’initiative de Harold Varmus, Patrick Brown et Michael Eisen,
chercheurs dans les universités de Stanford et Berkeley (Californie).
Le but est de contrer les pratiques de l’édition spécialisée en
regroupant tous les articles scientifiques et médicaux au sein
d’archives en ligne en accès libre. Au lieu d’une information
disséminée dans des millions de rapports et des milliers de périodiques
en ligne ayant chacun des conditions d’accès différentes, un point
d’accès unique permettrait de lire le contenu intégral de ces articles,
avec moteur de recherche multicritères et système d’hyperliens entre
les articles.

Pour ce faire, la PLoS fait circuler une lettre ouverte demandant que
les articles publiés par les éditeurs spécialisés soient distribués
librement dans un service d’archives en ligne, et incitant les
signataires de cette lettre à promouvoir les éditeurs prêts à soutenir
ce projet. La réponse de la communauté scientifique internationale est
remarquable. Au cours des deux années suivantes, la lettre ouverte est
signée par 30.000 chercheurs de 180 pays. Bien que la réponse des
éditeurs soit nettement moins enthousiaste, plusieurs éditeurs donnent
également leur accord pour une distribution immédiate des articles
publiés par leurs soins, ou alors une distribution dans un délai de six
mois. Mais dans la pratique, même les éditeurs ayant donné leur accord
formulent nombre d’objections au nouveau modèle proposé, si bien que le
projet d’archives en ligne ne voit finalement pas le jour.

Un autre objectif de la Public Library of Science est de devenir elle-
même éditeur. La PLoS fonde donc une maison d’édition scientifique non
commerciale qui reçoit en décembre 2002 une subvention de 9 millions de
dollars US de la part de la Moore Foundation. Une équipe éditoriale de
haut niveau est constituée en janvier 2003 pour lancer des périodiques
de qualité selon un nouveau modèle d’édition en ligne basé sur la
diffusion libre du savoir.

Le premier numéro de PLoS Biology sort en octobre 2003, avec une
version en ligne gratuite et une version imprimée au prix coûtant
(couvrant uniquement les frais de fabrication et de distribution). PLoS
Medicine est lancé en octobre 2004. Trois nouveaux titres voient le
jour en 2005: PLoS Genetics, PLoS Computational Biology et PLoS
Pathogens. PLoS Clinical Trials voit le jour en 2006. PloS Neglected
Tropical Diseases est lancé à l’automne 2007 en tant que première
publication scientifique consacrée aux maladies tropicales négligées.
Ces maladies affectent les populations pauvres, aussi bien dans les
zones rurales que dans les zones urbaines.

Tous les articles de ces périodiques sont librement accessibles en
ligne, sur le site de la PLoS et dans PubMed Central, le service
d’archives en ligne public et gratuit de la National Library of
Medicine (Etats-Unis), avec moteur de recherche multicritères. Les
versions imprimées sont abandonnées en 2006 pour laisser place à un
service d’impression à la demande proposé par la société Odyssey Press.
Ces articles peuvent être librement diffusés et réutilisés ailleurs, y
compris pour des traductions, selon les termes de la licence Creative
Commons, la seule contrainte étant la mention des auteurs et de la
source. La PLoS lance aussi PLoS ONE, un forum en ligne permettant la
publication d’articles sur tout sujet scientifique et médical.

Le succès est total. Trois ans après les débuts de la Public Library of
Science en tant qu’éditeur, PLoS Biology et PLos Medicine ont la même
réputation d’excellence que les grandes revues Nature, Science ou The
New England Journal of Medicine. La PLoS reçoit le soutien financier de
plusieurs fondations tout en mettant sur pied un modèle économique
viable, avec des revenus émanant des frais de publication payés par les
auteurs, et émanant aussi de la publicité, des sponsors et des
activités destinées aux membres de la PLoS. De plus, la PLoS souhaite
que ce modèle économique d’un genre nouveau inspire d’autres éditeurs
pour créer des revues du même type ou pour mettre des revues existantes
en accès libre.

# Wikipédia

Issu du terme hawaïen «wiki» (qui signifie: vite, rapide), un wiki est
un site web permettant à plusieurs utilisateurs de collaborer en ligne
sur un même projet. A tout moment, ces utilisateurs peuvent contribuer
à la rédaction du contenu, modifier ce contenu et l'enrichir en
permanence. Le wiki est utilisé par exemple pour créer et gérer des
dictionnaires, des encyclopédies ou encore des sites d'information sur
un sujet donné. Le programme présent derrière l'interface d'un wiki est
plus ou moins élaboré. Un programme simple gère du texte et des
hyperliens. Un programme élaboré permet d'inclure des images, des
graphiques, des tableaux, etc. L’encyclopédie wiki la plus connue est
Wikipédia.

Créée en janvier 2001 à l’initiative de Jimmy Wales et de Larry Sanger,
Wikipédia est une encyclopédie gratuite écrite collectivement et dont
le contenu est librement réutilisable. Elle est immédiatement très
populaire. Sans publicité et financée par des dons, cette encyclopédie
coopérative est rédigée par des milliers de volontaires - appelés
Wikipédiens, et qui s'inscrivent en prenant un pseudonyme - avec
possibilité de corriger et de compléter les articles, aussi bien les
leurs que ceux d'autres contributeurs. Les articles restent la
propriété de leurs auteurs, et leur libre utilisation est régie par la
licence GFDL (GNU free documentation license).

En décembre 2004, Wikipédia compte 1,3 million d'articles rédigés dans
100 langues par 13.000 contributeurs. En décembre 2006, elle compte 6
millions d'articles dans 250 langues, et elle est un de dix sites les
plus visités du web. En mai 2007, la version francophone fête ses
500.000 articles. A la même date, Wikipédia compte 7 millions
d'articles dans 192 langues, dont 1,8 million en anglais, 589.000 en
allemand, 260.000 en portugais et 236.000 en espagnol.

Fondée en juin 2003, la Wikimedia Foundation gère non seulement
Wikipédia mais aussi Wiktionary, un dictionnaire et thésaurus
multilingue lancé en décembre 2002, puis Wikibooks (livres et manuels
en cours de rédaction) lancé en juin 2003, auxquels s'ajoutent ensuite
Wikiquote (répertoire de citations), Wikisource (textes appartenant au
domaine public), Wikimedia Commons (sources multimédia), Wikispecies
(répertoire d'espèces animales et végétales), Wikinews (site
d'actualités) et enfin Wikiversity (matériel d'enseignement), lancé en
août 2006. La fin 2007 voit le lancement d'un moteur de recherche
dénommé Wiki Search, qui utilise le réseau de contributeurs de
Wikipédia pour classer les sites en fonction de leur qualité.

# Citizendium

Une nouvelle étape s’ouvre avec les débuts de Citizendium (abrégé de:
The Citizens’ Compendium), une grande encyclopédie collaborative en
ligne conçue en novembre 2006 et lancée en mars 2007 (version bêta) par
Larry Sanger, co-fondateur de Wikipédia, mais qui quitte ensuite
l’équipe de Wikipédia suite à des problèmes de qualité de contenu.

Citizendium est basé sur le même modèle que Wikipédia - collaborative
et gratuite - tout en évitant ses travers - vandalisme et manque de
rigueur. Les auteurs signent leurs articles de leur vrai nom, et ces
articles sont édités par des experts («editors») âgés d'au moins 25 ans
et titulaires d'une licence universitaire. De plus, des «constables»
sont chargés de la bonne marche du projet et du respect du règlement.
Le jour de son lancement le 25 mars 2007, Citizendium comprend 1.100
articles, 820 auteurs et 180 experts. 9.800 articles sont disponibles
en janvier 2009.

Dans "Why Make Room for Experts in Web 2.0?", une communication datée
d’octobre 2006 et actualisée depuis, Larry Sanger voit dans Citizendium
l’émergence d’un nouveau modèle de collaboration massive de dizaines de
milliers d’intellectuels et scientifiques, non seulement pour les
encyclopédies, mais aussi pour les manuels d’enseignement, les ouvrages
de référence, le multimédia et les applications en 3D. Cette
collaboration est basée sur le partage des connaissances, dans la
lignée du web 2.0, un concept lancé en 2004 pour caractériser les
notions de communauté et de partage et qui se manifeste d’abord par une
floraison de wikis, blogs et sites sociaux. D’après Larry Sanger, il
importe aussi de créer des structures permettant des collaborations
scientifiques, et Citizendium pourrait servir de prototype dans ce
domaine.

# Encyclopedia of Life

Cet appel semble se concrétiser rapidement avec le lancement en mai
2007 du projet de l’Encyclopedia of Life. Cette vaste encyclopédie
collaborative en ligne rassemblera les connaissances existantes sur
toutes les espèces animales et végétales connues (1,8 million), y
compris les espèces en voie d’extinction, avec l’ajout de nouvelles
espèces au fur et à mesure de leur identification, ce qui
représenterait entre 8 et 10 millions d'espèces en tout.

Il s’agira d’une encyclopédie multimédia permettant de ressembler
textes, photos, cartes, bandes sonores et vidéos, avec une page web par
espèce, et permettant aussi d’offrir un portail unique à des millions
de documents épars, en ligne et hors ligne. Outil d’apprentissage et
d’enseignement pour une meilleure connaissance de notre planète, cette
encyclopédie sera à destination de tous: scientifiques, enseignants,
étudiants, scolaires, médias, décideurs et grand public.

Ce projet collaboratif est mené par plusieurs grandes institutions:
Field Museum of Natural History, Harvard University, Marine Biological
Laboratory, Missouri Botanical Garden, Smithsonian Institution et
Biodiversity Heritage Library.

Le directeur honoraire du projet est Edward Wilson, professeur émérite
à l’Université de Harvard, qui - dans un essai daté de 2002 - est le
premier à émettre le voeu d’une telle encyclopédie. Cinq ans après - en
2007 - c'est désormais chose possible grâce aux avancées technologiques
récentes, notamment les outils logiciels permettant l’agrégation de
contenu, le mash-up (à savoir le fait de rassembler un contenu donné à
partir de nombreuses sources différentes), les wikis de grande taille
et la gestion de contenu à vaste échelle.

En tant que consortium des dix plus grandes bibliothèques des sciences
de la vie (d’autres suivront), la Biodiversity Heritage Library a
d’ores et déjà débuté la numérisation de 2 millions de documents, dont
les dates de publication s’étalent sur 200 ans. En mai 2007, date du
lancement officiel du projet, on compte déjà 1,25 million de pages
traitées dans les centres de numérisation de Londres, Boston et
Washington DC, et disponibles sur le site de l’Internet Archive.

Le financement initial est assuré par la MacArthur Foundation (10
millions de dollars US) et la Sloan Foundation (2,5 millions de
dollars). 100 millions de dollars sont nécessaires pour un financement
sur dix ans, avant que l'encyclopédie ne puisse s'autofinancer. La
réalisation des pages web débute courant 2007. L’encyclopédie fait ses
débuts à la mi-2008. Opérationnelle d'ici 2012, elle devrait être
complète - c'est-à-dire à jour - en 2017.

Dans la lignée du "Human Genome Project" (Séquencage du génome humain),
publié pour la première fois en février 2001 et appartenant d'emblée au
domaine public, l’Encyclopedia of Life permettra de proposer toutes les
connaissances disponibles à ce jour sur les espèces animales et
végétales. La version initiale sera d’abord en anglais avant d’être
traduite en plusieurs langues par de futurs organismes partenaires.

L'encyclopédie sera aussi un «macroscope» permettant de déceler les
grandes tendances à partir d’un stock d’informations considérable, à la
différence du microscope permettant l’étude de détail.

En plus de sa flexibilité et de sa diversité, elle permettra à chacun
de contribuer au contenu sous une forme s’apparentant au wiki, ce
contenu étant ensuite validé ou non par des scientifiques.



2003: LES NOUVEAUTES SONT EN VERSION NUMERIQUE


= [Résumé]

En 2003, les livres numériques prennent peu à peu une place
significative à côté de leurs correspondants imprimés, avec des livres
aux formats PDF (pour l'Adobe Reader), LIT (pour le Microsoft Reader)
et PRC (pour le Mobipocket Reader), OeB (pour de nombreux logiciels de
lecture), entre autres. Des centaines de best-sellers sont vendus en
version numérique sur Amazon.com, Barnesandnoble.com, Yahoo! eBook
Store ou sur des sites d’éditeurs (Random House, PerfectBound, etc.),
pour lecture sur ordinateur ou sur assistant personnel (PDA). Numilog
distribue 3.500 titres numériques (livres et périodiques) en français
et en anglais. Mobipocket distribue 6.000 titres numériques dans
plusieurs langues, soit sur son site soit dans des librairies
partenaires. Le catalogue de Palm Digital Media approche les 10.000
titres, lisibles sur les gammes Palm et Pocket PC, avec 15 à 20
nouveaux titres par jour et 1.000 nouveaux clients par semaine.


= Adobe Reader

Le format PDF (portable document format) est lancé en juin 1993 par la
société Adobe, en même temps que l'Acrobat Reader (gratuit), premier
logiciel de lecture du marché, téléchargeable gratuitement pour lecture
des fichiers au format PDF. Le but de ce format est de figer les
documents numériques dans une présentation donnée, pour conserver la
présentation originale du document source, quelle que soit la
plateforme utilisée pour le créer et pour le lire. Le format PDF
devient au fil des ans un standard international de diffusion des
documents. Tout document peut être converti au format PDF à l’aide du
logiciel Adobe Acrobat (payant).

Dix ans plus tard, 10% des documents disponibles sur l'internet sont au
format PDF. Des millions de fichiers PDF sont présents sur le web pour
lecture ou téléchargement, ou bien transitent par courriel. L’Acrobat
Reader pour ordinateur est progressivement disponible dans plusieurs
langues et pour diverses plateformes (Windows, Mac, Linux).

Adobe annonce en août 2000 l’acquisition de la société Glassbook,
spécialisée dans les logiciels de distribution de livres numériques à
l'intention des éditeurs, libraires, diffuseurs et bibliothèques. Adobe
passe aussi un partenariat avec Amazon.com et Barnes & Noble.com afin
de proposer des titres lisibles sur l’Acrobat Reader et le Glassbook
Reader.

En janvier 2001, Adobe lance deux nouveaux logiciels.

Le premier logiciel, gratuit, est l’Acrobat eBook Reader. Il permet de
lire les fichiers PDF de livres numériques sous droits, avec gestion
des droits par l’Adobe Content Server. Il permet aussi d’ajouter des
notes et des signets, de choisir l’orientation de lecture des livres
(paysage ou portrait), ou encore de visualiser leur couverture dans une
bibliothèque personnelle. Il utilise la technique d’affichage CoolType
et comporte un dictionnaire intégré.

Le deuxième logiciel, payant, est l’Adobe Content Server, destiné aux
éditeurs et distributeurs. Il s’agit d’un logiciel serveur de contenu
assurant le conditionnement, la protection, la distribution et la vente
sécurisée de livres numériques au format PDF. Ce système de gestion des
droits numériques (DRM: digital rights management) permet de contrôler
l’accès aux livres numériques sous droits, et donc de gérer les droits
d’un livre selon les consignes données par le gestionnaire des droits,
par exemple en autorisant ou non l’impression ou le prêt.

En avril 2001, Adobe conclut un partenariat avec Amazon, qui met en
vente 2.000 livres numériques lisibles sur l’Acrobat eBook Reader:
titres de grands éditeurs, guides de voyages, livres pour enfants, etc.

L'Acrobat Reader s'enrichit d'une version PDA, pour le Palm Pilot (en
mai 2001) puis pour le Pocket PC (en décembre 2001).

En dix ans, entre 1993 et 2003, l’Acrobat Reader aurait été téléchargé
500 millions de fois. Ce logiciel gratuit est désormais disponible dans
de nombreuses langues et pour de nombreuses plateformes (Windows, Mac,
Linux, Palm OS, Pocket PC, Symbian OS, etc.).

En mai 2003, l’Acrobat Reader (5e version) fusionne avec l’Acrobat
eBook Reader (2e version) pour devenir l’Adobe Reader (débutant à la
version 6), qui permet de lire aussi bien les fichiers PDF standard que
les fichiers PDF sécurisés comme ceux des livres numériques sous
droits.

Fin 2003, Adobe ouvre sa librairie en ligne, Digital Media Store, avec
les titres au format PDF de grands éditeurs - HarperCollins Publishers,
Random House, Simon & Schuster, etc. - ainsi que les versions
électroniques de journaux et magazines comme le New York Times, Popular
Science, etc. Adobe lance aussi Adobe eBooks Central, un service
permettant de lire, publier, vendre et prêter des livres numériques, et
l’Adobe eBook Library, qui se veut un prototype de bibliothèque de
livres numériques.

En novembre 2004, l’Adobe Content Server est remplacé par l’Adobe
LiveCycle Policy Server.

Les versions récentes d’Adobe Acrobat permettent de créer des PDF
compatibles avec les formats OeB (open ebook) puis ePub (format ayant
succédé au format OeB), devenus eux aussi des standards du livre
numérique.


= Open eBook et ePub

Les années 1998 et 1999 sont marquées par la prolifération des formats,
chacun lançant son propre format de livre numérique dans le cadre d’un
marché naissant promis à une expansion rapide.

Aux formats classiques - formats TXT (texte), DOC (Microsoft Word),
HTML (hypertext markup language), XML (extensible markup language) et
PDF (portable document format) - s’ajoutent des formats propriétaires
créés par plusieurs sociétés pour lecture sur leurs propres logiciels -
Glassbook Reader, Peanut Reader, Rocket eBook Reader (pour lecture sur
le Rocket eBook), Franklin Reader (pour lecture sur le eBookMan),
logiciel de lecture Cytale (pour lecture sur le Cybook), Gemstar eBook
Reader (pour lecture sur le Gemstar eBook), Palm Reader (pour lecture
sur le Palm Pilot), etc. -, ces logiciels correspondant le plus souvent
à un appareil donné et ne pouvant être utilisés sur d'autres appareils.

Inquiets pour l’avenir du livre numérique qui, à peine né, propose
presque autant de formats que de titres, certains insistent sur
l’intérêt - sinon la nécessité - d’un format unique. A l’instigation du
NIST (National Institute of Standards & Technology) aux Etats-Unis,
l’Open eBook Initiative voit le jour en juin 1998 et constitue un
groupe de travail de 25 personnes sous le nom d'Open eBook Authoring
Group. Ce groupe élabore l’OeB (open ebook), un format de livre
numérique basé sur le langage XML et destiné à normaliser le contenu,
la structure et la présentation des livres numériques.

Le format OeB est défini par l’OeBPS (open ebook publication
structure), dont la version 1.0 est disponible en septembre 1999.
Téléchargeable gratuitement, l’OeBPS dispose d'une version ouverte et
gratuite appartenant au domaine public. La version originale est
destinée aux professionnels de la publication puisqu'elle doit être
associée à une technologie normalisée de gestion des droits numériques,
et donc à un système de DRM (digital rights management) permettant de
contrôler l’accès des livres numériques sous droits.

Fondé en janvier 2000 pour prendre la suite de l’Open eBook Initiative,
l’OeBF (Open eBook Forum) est un consortium industriel international
regroupant constructeurs, concepteurs de logiciels, éditeurs, libraires
et spécialistes du numérique (85 participants en 2002) dans l'optique
de développer le format OeB et l’OeBPS. Le format OeB devient un
standard qui sert lui-même de base à de nombreux formats, par exemple
le format LIT (pour le Microsoft Reader) ou le format PRC (pour le
Mobipocket Reader).

En avril 2005, l’Open eBook Forum devient l’International Digital
Publishing Forum (IDPF), et le format OeB laisse la place au format
ePub.


= Microsoft Reader

Lancé en avril 2000, le Microsoft Reader est un logiciel permettant la
lecture de livres numériques au format LIT (abrégé du terme anglais
«literature»), lui-même basé sur le format OeB. Le Microsoft Reader
équipe d'abord le Pocket PC, l’assistant personnel lancé à la même date
par Microsoft. Quatre mois plus tard, en août 2000, le Microsoft Reader
est utilisable sur toute plateforme Windows, et donc aussi bien sur
ordinateur que sur assistant personnel. Ses caractéristiques sont un
affichage utilisant la technologie ClearType, le choix de la taille des
caractères, la mémorisation des mots-clés pour des recherches
ultérieures, et l’accès d’un clic au Merriam-Webster Dictionary.

Ce logiciel étant téléchargeable gratuitement, Microsoft facture les
éditeurs et distributeurs pour l’utilisation de sa technologie de
gestion des droits numériques (DRM), et touche une commission sur la
vente de chaque titre. La gestion des droits numériques s’effectue au
moyen du Microsoft DAS Server (DAS: digital asset server). Microsoft
passe aussi des partenariats avec les grandes librairies en ligne -
Barnes & Noble.com en janvier 2000 puis Amazon.com en août 2000 – pour
la vente de livres numériques lisibles sur le Microsoft Reader. Barnes
& Noble.com ouvre son secteur eBooks en août 2000, suivi par Amazon.com
en novembre 2000.

En novembre 2002, le Microsoft Reader est disponible pour tablette PC,
dès la commercialisation de cette nouvelle machine par 14 fabricants.


= Mobipocket Reader

Face à Adobe avec son format PDF (lisible sur l'Acrobat Reader) et
Microsoft avec son format LIT (lisible sur le Microsoft Reader), un
nouvel acteur s’impose rapidement sur le marché, sur un créneau bien
spécifique, celui des appareils mobiles. Fondé à Paris en mars 2000 par
Thierry Brethes et Nathalie Ting, Mobipocket se spécialise d’emblée
dans la lecture et la distribution sécurisée de livres pour assistant
personnel. La société est financée en partie par Viventures, branche de
la multinationale française Vivendi.

Mobipocket conçoit le Mobipocket Reader, logiciel de lecture permettant
la lecture de fichiers au format PRC. Gratuit et disponible en
plusieurs langues (français, anglais, allemand, espagnol, italien), ce
logiciel est «universel», c’est-à-dire utilisable sur tout assistant
personnel. En octobre 2001, le Mobipocket Reader reçoit l’eBook
Technology Award de la Foire internationale du livre à Francfort. A la
même date, Franklin passe un partenariat avec Mobipocket pour
l’installation du Mobipocket Reader sur l’eBookMan, l’assistant
personnel multimédia de Franklin, au lieu du partenariat prévu à
l’origine entre Franklin et Microsoft pour l’installation du Microsoft
Reader.

Si le Mobipocket Reader est gratuit, d’autres logiciels Mobipocket sont
payants. Le Mobipocket Web Companion est un logiciel d’extraction
automatique de contenu pour les sites de presse partenaires de la
société. Le Mobipocket Publisher permet aux particuliers (version
privée gratuite ou version standard payante) et aux éditeurs (version
professionnelle payante) de créer des livres numériques sécurisés
utilisant la technologie Mobipocket DRM, afin de contrôler l’accès aux
livres numériques sous droits. Dans un souci d’ouverture aux autres
formats, le Mobipocket Publisher permet aussi de créer des livres
numériques au format LIT, lu par le Microsoft Reader.

Déjà utilisable sur n’importe quel PDA, le Mobipocket Reader peut être
utilisé sur tout ordinateur et pour toute plateforme en avril 2002,
avec le lancement de nouvelles versions pour ordinateur personnel.

Au printemps 2003, le Mobipocket Reader équipe tous les PDA du marché,
à savoir les gammes Palm Pilot, Pocket PC, eBookMan et Psion, auxquels
s'ajoutent les smartphones de Nokia et de Sony Ericsson. A la même
date, le nombre de livres lisibles sur le Mobipocket Reader se chiffre
à 6.000 titres dans plusieurs langues (français, anglais, allemand et
espagnol), distribués soit sur le site de Mobipocket soit dans les
librairies partenaires.

Mobipocket est racheté par Amazon.com en avril 2005. Ce rachat permet à
Amazon de beaucoup étoffer son catalogue d'ebooks, en prévision du
lancement de sa tablette de lecture Kindle en novembre 2007. Le site de
Mobipocket propose 70.000 ebooks en 2008.


= Numilog

Numilog ouvre ses portes «virtuelles» en octobre 2000 pour devenir en
quelques années la plus grande librairie numérique francophone du
réseau.

En février 2001, Denis Zwirn, président de Numilog, relate: «Dès 1995,
j’avais imaginé et dessiné des modèles de lecteurs électroniques
permettant d’emporter sa bibliothèque avec soi et pesant comme un livre
de poche. Début 1999, j’ai repris ce projet avec un ami spécialiste de
la création de sites internet, en réalisant la formidable synergie
possible entre des appareils de lecture électronique mobiles et le
développement d’internet, qui permet d’acheminer les livres
dématérialisés en quelques minutes dans tous les coins du monde. (...)
Nous avons créé une base de livres accessible par un moteur de
recherche. Chaque livre fait l’objet d’une fiche avec un résumé et un
extrait. En quelques clics, il peut être acheté en ligne par carte
bancaire, puis reçu par email ou téléchargement.»

Le site offre ensuite «des fonctionnalités nouvelles, comme
l’intégration d’une "authentique vente au chapitre" (les chapitres
vendus isolément sont traités comme des éléments inclus dans la fiche-
livre, et non comme d’autres livres) et la gestion très ergonomique des
formats de lecture multiples».

Fondée en avril 2000 (six mois avant l'ouverture de la librairie), la
société Numilog a en fait une triple activité: librairie en ligne,
studio de fabrication et diffuseur. «Numilog est d’abord une librairie
en ligne de livres numériques, explique Denis en 2001. Notre site
internet est dédié à la vente en ligne de ces livres, qui sont envoyés
par courrier électronique ou téléchargés après paiement par carte
bancaire. Il permet aussi de vendre des livres par chapitres. Numilog
est également un studio de fabrication de livres numériques:
aujourd’hui, les livres numériques n’existent pas chez les éditeurs, il
faut donc d’abord les fabriquer avant de pouvoir les vendre, dans le
cadre de contrats négociés avec les éditeurs détenteurs des droits. Ce
qui signifie les convertir à des formats convenant aux différents
"readers" du marché. (...) Enfin Numilog devient aussi progressivement
un diffuseur. Car, sur internet, il est important d’être présent en de
très nombreux points du réseau pour faire connaître son offre. Pour les
livres en particulier, il faut les proposer aux différents sites
thématiques ou de communautés, dont les centres d’intérêt correspondent
à leur sujet (sites de fans d’histoire, de management, de science-
fiction...). Numilog facilitera ainsi la mise en oeuvre de multiples
"boutiques de livres numériques" thématiques.»

Répartis à l’origine en trois grandes catégories - savoir, guides
pratiques et littérature - les livres sont disponibles en plusieurs
formats: format PDF pour lecture sur l’Acrobat Reader (devenu l’Adobe
Reader en mai 2003), format LIT pour lecture sur le Microsoft Reader et
format PRC pour lecture sur le Mobipocket Reader.

En septembre 2003, le catalogue comprend 3.500 titres (livres et
périodiques) en français et en anglais, grâce à un partenariat avec une
quarantaine d’éditeurs, le but à long terme étant de «permettre à un
public d’internautes de plus en plus large d’avoir progressivement
accès à des bases de livres numériques aussi importantes que celles des
livres papier, mais avec plus de modularité, de richesse d’utilisation
et à moindre prix».

Au fil des ans, Numilog devient la principale librairie francophone de
livres numériques, suite à des accords avec de nombreux éditeurs:
Gallimard, Albin Michel, Eyrolles, Hermès Science, Pearson Education
France, etc. Numilog propose aussi des livres audionumériques lisibles
sur synthèse vocale. Une librairie anglophone est lancée suite à des
accords de diffusion conclus avec plusieurs éditeurs anglo-saxons:
Springer-Kluwer, Oxford University Press, Taylor & Francis, Kogan Page,
etc. Les différents formats proposés permettent la lecture des livres
sur tout appareil électronique: ordinateur, assistant personnel,
téléphone portable, smartphone et tablette de lecture.

La société est également prestataire de services pour les technologies
DRM (digital rights management), à savoir les systèmes de gestion des
droits numériques permettant de contrôler l’accès aux livres numériques
sous droits, et donc de gérer les droits d’un livre selon les consignes
données par le gestionnaire des droits, par exemple en autorisant ou
non l’impression ou le prêt.

En 2004, Numilog met sur pied un système de bibliothèque en ligne pour
le prêt de livres numériques. Ce système est surtout destiné aux
bibliothèques, aux administrations et aux entreprises.

En janvier 2006, Numilog s'associe avec la ville de Boulogne-
Billancourt (en région parisienne) pour lancer la version expérimentale
de la Bibliothèque numérique pour le handicap (BnH).

En décembre 2006, le catalogue de Numilog comprend 35.000 livres grâce
à un partenariat avec 60 éditeurs francophones et des éditeurs
anglophones.

En janvier 2009, Numilog, devenue filiale du groupe Hachette Livre (en
mai 2008), est un distributeur-diffuseur numérique représentant 100
éditeurs francophones et anglophones, avec un catalogue de 50.000
livres numériques distribués auprès des particuliers et des
bibliothèques. Numilog propose également aux librairies un service de
vente de livres numériques sur leur propre site.



2004: DES AUTEURS SONT CREATIFS SUR LE NET


= [Résumé]

En 2004, nombre d’auteurs s’accordent à reconnaître les bienfaits de
l'internet, que ce soit pour la recherche d’information, la diffusion
de leurs oeuvres, les échanges avec les lecteurs ou la collaboration
avec d’autres créateurs. Des écrivains férus de nouvelles technologies
font un travail de défricheur en explorant les possibilités offertes
par l’hyperlien. Les technologies numériques donnent naissance à
plusieurs genres: roman multimédia, roman hypertexte, roman hypermédia,
site d’écriture hypermédia, mail-roman, etc. Une véritable littérature
numérique - appelée aussi littérature électronique ou cyber-littérature
- bouscule désormais la littérature traditionnelle en lui apportant un
souffle nouveau, tout en s’intégrant à d’autres formes artistiques
puisque le support numérique favorise la fusion de l’écrit avec l’image
et le son.


= Poésie

Poète et plasticienne, Silvaine Arabo vit en France, dans la région
Poitou-Charentes. En mai 1997, elle crée l’un des premiers sites
francophones consacrés à la poésie, "Poésie d’hier et d’aujourd’hui",
sur lequel elle propose de nombreux poèmes, y compris les siens.

En juin 1998, elle raconte: «Je suis poète, peintre et professeur de
lettres (13 recueils de poèmes publiés, ainsi que deux recueils
d’aphorismes et un essai sur le thème "poésie et transcendance"; quant
à la peinture, j’ai exposé mes toiles à Paris - deux fois - et en
province). (...) Pour ce qui est d’internet, je suis autodidacte (je
n’ai reçu aucune formation informatique quelle qu’elle soit). J’ai eu
l’idée de construire un site littéraire centré sur la poésie: internet
me semble un moyen privilégié pour faire circuler des idées, pour
communiquer ses passions aussi. Je me suis donc mise au travail, très
empiriquement, et ai finalement abouti à ce site sur lequel j’essaye de
mettre en valeur des poètes contemporains de talent, sans oublier la
nécessaire prise de recul (rubrique "Réflexions sur la poésie") sur
l’objet considéré. (...)

Par ailleurs, internet m’a mis en contact avec d’autres poètes, dont
certains fort intéressants. Cela rompt le cercle de la solitude et
permet d’échanger des idées. On se lance des défis aussi. Internet peut
donc pousser à la créativité et relancer les motivations des poètes
puisqu’ils savent qu’ils seront lus et pourront même, dans le meilleur
des cas, correspondre avec leurs lecteurs et avoir les points de vue de
ceux-ci sur leurs textes. Je ne vois personnellement que des aspects
positifs à la promotion de la poésie par internet, tant pour le lecteur
que pour le créateur.»

Très vite, Poésie d’hier et d’aujourd’hui prend la forme d’une cyber-
revue. Quatre ans plus tard, en mars 2001, Silvaine Arabo crée une
deuxième revue, "Saraswati: revue de poésie, d’art et de réflexion",
cette fois sur papier. Les deux revues «se complètent et sont vraiment
à placer en regard l’une de l’autre».


= Fables

Fondé en 1992 par Nicolas et Suzanne Pewny, alors libraires en Haute-
Savoie, Le Choucas est une petite maison d’édition spécialisée dans les
romans policiers, la littérature, la photographie et les livres d’art.
Bien qu’étant d’abord un éditeur à vocation commerciale, Nicolas Pewny
tient aussi à avoir des activités non commerciales pour faire connaître
des auteurs peu diffusés, par exemple Raymond Godefroy, écrivain-paysan
normand, qui désespérait de trouver un éditeur pour son recueil de
fables, "Fables pour l’an 2000". Quelques jours avant l'an 2000,
Nicolas Pewny réalise un beau design pour ces fables et publie le
recueil en ligne sur le site du Choucas.

«Internet représente pour moi un formidable outil de communication qui
nous affranchit des intermédiaires, des barrages doctrinaires et des
intérêts des médias en place, écrit Raymond Godefroy en décembre 1999.
Soumis aux mêmes lois cosmiques, les hommes, pouvant mieux se
connaître, acquerront peu à peu cette conscience du collectif,
d’appartenir à un même monde fragile pour y vivre en harmonie sans le
détruire. Internet est absolument comme la langue d’Esope, la meilleure
et la pire des choses, selon l’usage qu’on en fait, et j’espère qu’il
me permettra de m’affranchir en partie de l’édition et de la
distribution traditionnelle qui, refermée sur elle-même, souffre d’une
crise d’intolérance pour entrer à reculons dans le prochain
millénaire.»

Très certainement autobiographique, la fable "Le poète et l’éditeur"
(sixième fable de la troisième partie du recueil) relate on ne peut
mieux les affres du poète à la recherche d’un éditeur. Raymond Godefroy
restant très attaché au papier, il auto-publie la version imprimée de
ses fables en juin 2001, avec un titre légèrement différent, "Fables
pour les années 2000", puisque le cap du 21e siècle est désormais
franchi.


= Romans policiers

Michel Benoît habite Montréal, au Québec. Auteur de nouvelles
policières, de récits noirs et d’histoires fantastiques, il utilise
l’internet pour élargir ses horizons et pour abolir le temps et la
distance. Il relate en juin 2000: «L’internet s’est imposé à moi comme
outil de recherche et de communication, essentiellement. Non, pas
essentiellement. Ouverture sur le monde aussi. Si l’on pense
"recherche", on pense "information". Voyez-vous, si l’on pense
"écriture", "réflexion", on pense "connaissance", "recherche". Donc on
va sur la toile pour tout, pour une idée, une image, une explication.
Un discours prononcé il y a vingt ans, une peinture exposée dans un
musée à l’autre bout du monde. On peut donner une idée à quelqu’un
qu’on n’a jamais vu, et en recevoir de même. La toile, c’est le monde
au clic de la souris. On pourrait penser que c’est un beau cliché.
Peut-être bien, à moins de prendre conscience de toutes les
implications de la chose. L’instantanéité, l’information tout de suite,
maintenant. Plus besoin de fouiller, de se taper des heures de
recherche. On est en train de faire, de produire. On a besoin d’une
information. On va la chercher, immédiatement. De plus, on a accès aux
plus grandes bibliothèques, aux plus importants journaux, aux musées
les plus prestigieux. (...)

Mon avenir professionnel en inter-relation avec le net, je le vois
exploser. Plus rapide, plus complet, plus productif. Je me vois faire
en une semaine ce qui m’aurait pris des mois. Plus beau, plus
esthétique. Je me vois réussir des travaux plus raffinés, d’une facture
plus professionnelle, même et surtout dans des domaines connexes à mon
travail, comme la typographie, où je n’ai aucune compétence. La
présentation, le transport de textes, par exemple. Le travail simultané
de plusieurs personnes qui seront sur des continents différents.
Arriver à un consensus en quelques heures sur un projet, alors qu’avant
le net, il aurait fallu plusieurs semaines, parlons de mois entre les
Francophones. Plus le net ira se complexifiant, plus l’utilisation du
net deviendra profitable, nécessaire, essentielle.»

Autre expérience, celle d'Alain Bron, consultant en systèmes
d'information et écrivain. L'internet est un des «personnages» de son
deuxième roman, "Sanguine sur toile", disponible en version imprimée
aux éditions du Choucas en 1999, puis en version numérique (format PDF)
aux éditions 00h00.com en 2000.

Quel est le thème de ce roman? «La "toile", c'est celle du peintre,
c'est aussi l'autre nom d'internet: le web - la toile d'araignée,
raconte l'auteur en novembre 1999. "Sanguine" évoque le dessin et la
mort brutale. Mais l'amour des couleurs justifierait-il le meurtre?
Sanguine sur toile évoque l'histoire singulière d'un internaute pris
dans la tourmente de son propre ordinateur, manipulé à distance par un
très mystérieux correspondant qui n'a que vengeance en tête. J'ai voulu
emporter le lecteur dans les univers de la peinture et de l'entreprise,
univers qui s'entrelacent, s'échappent, puis se rejoignent dans la
fulgurance des logiciels. Le lecteur est ainsi invité à prendre
l'enquête à son propre compte pour tenter de démêler les fils tressés
par la seule passion. Pour percer le mystère, il devra répondre à de
multiples questions. Le monde au bout des doigts, l'internaute n'est-il
pas pour autant l'être le plus seul au monde? Compétitivité oblige,
jusqu'où l'entreprise d'aujourd'hui peut-elle aller dans la violence?
La peinture tend-elle à reproduire le monde ou bien à en créer un
autre? Enfin, j'ai voulu montrer que les images ne sont pas si sages.
On peut s'en servir pour agir, voire pour tuer. (...) Dans le roman,
internet est un personnage en soi. Plutôt que de le décrire dans sa
complexité technique, le réseau est montré comme un être tantôt
menaçant, tantôt prévenant, maniant parfois l'humour. N'oublions pas
que l'écran d'ordinateur joue son double rôle: il montre et il cache.
C'est cette ambivalence qui fait l'intrigue du début à la fin. Dans ce
jeu, le grand gagnant est bien sûr celui ou celle qui sait s'affranchir
de l'emprise de l'outil pour mettre l'humanisme et l'intelligence au-
dessus de tout.»


= Oeuvres de fiction

Murray Suid vit à Palo Alto, dans la Silicon Valley, en Californie. Il
est l’auteur de livres pédagogiques, de livres pour enfants, d’oeuvres
multimédia et de scénarios. Dès septembre 1998, il préconise une
solution choisie depuis par de nombreux auteurs: «Un livre peut avoir
un prolongement sur le web – et donc vivre en partie dans le
cyberespace. L’auteur peut ainsi aisément l’actualiser et le corriger,
alors qu’auparavant il devait attendre longtemps, jusqu’à l’édition
suivante, quand il y en avait une. (...) Je ne sais pas si je publierai
des livres sur le web, au lieu de les publier en version imprimée.
J’utiliserai peut-être ce nouveau support si les livres deviennent
multimédia. Pour le moment, je participe au développement de matériel
pédagogique multimédia. C’est un nouveau type de matériel qui me plaît
beaucoup et qui permet l’interactivité entre des textes, des films, des
bandes sonores et des graphiques qui sont tous reliés les uns aux
autres.»

Un an après, en août 1999, il ajoute: «En plus des livres complétés par
un site web, je suis en train d’adopter la même formule pour mes
oeuvres multimédia – qui sont sur CD-ROM – afin de les actualiser et
d’enrichir leur contenu.»

Quelques mois plus tard, l’intégralité de ses oeuvres multimédia est
sur le réseau. Le matériel pédagogique auquel il contribue est conçu
non plus pour diffusion sur CD-ROM, mais pour diffusion sur le web.
D’entreprise multimédia, la société de logiciels éducatifs qui
l’emploie s'est reconvertie en entreprise internet.

Autre expérience, celle d'Anne-Bénédicte Joly, romancière et essayiste,
qui habite en région parisienne. En avril 2000, elle décide d’auto-
publier ses oeuvres en utilisant l’internet pour les faire connaître.
«Mon site a plusieurs objectifs, relate-t-elle en juin 2000. Présenter
mes livres (essais, nouvelles et romans auto-édités) à travers des
fiches signalétiques (dont le format est identique à celui que l’on
trouve dans la base de données Electre) et des extraits choisis,
présenter mon parcours (de professeur de lettres et d’écrivain),
permettre de commander mes ouvrages, offrir la possibilité de laisser
des impressions sur un livre d’or, guider le lecteur à travers des
liens vers des sites littéraires. (...) Créer un site internet me
permet d’élargir le cercle de mes lecteurs en incitant les internautes
à découvrir mes écrits. Internet est également un moyen pour élargir la
diffusion de mes ouvrages. Enfin, par une politique de liens, j’espère
susciter des contacts de plus en plus nombreux.»


= Romans numériques

Lucie de Boutiny est l’auteur de "NON", roman multimédia débuté en août
1997 et publié en feuilleton par Synesthésie, une revue en ligne d’art
contemporain. «"NON" est un roman comique qui fait la satire de la vie
quotidienne d’un couple de jeunes cadres supposés dynamiques, raconte-
t-elle en juin 2000. Bien qu’appartenant à l’élite high-tech d’une
industrie florissante, Monsieur et Madame sont les jouets de la dite
révolution numérique. (...) "NON" prolonge les expériences du roman
post-moderne (récits tout en digression, polysémie avec jeux sur les
registres - naturaliste, mélo, comique... - et les niveaux de langues,
etc.). Cette hyperstylisation permet à la narration des développements
inattendus et offre au lecteur l’attrait d’une navigation dans des
récits multiples et multimédia, car l’écrit à l’écran s’apparente à un
jeu et non seulement se lit mais aussi se regarde.»

Les romans précédents de Lucie de Boutiny sont publiés sous forme
imprimée. Un roman numérique requiert-il une démarche différente?
«D’une manière générale, mon humble expérience d’apprentie auteur m’a
révélé qu’il n’y a pas de différence entre écrire de la fiction pour le
papier ou le pixel: cela demande une concentration maximale, un
isolement à la limite désespéré, une patience obsessionnelle dans le
travail millimétrique avec la phrase, et bien entendu, en plus de la
volonté de faire, il faut avoir quelque chose à dire! Mais avec le
multimédia, le texte est ensuite mis en scène comme s’il n’était qu’un
scénario. Et si, à la base, il n’y a pas un vrai travail sur le langage
des mots, tout le graphisme et les astuces interactives qu’on peut y
mettre fera gadget. Par ailleurs, le support modifie l’appréhension du
texte, et même, il faut le souligner, change l’oeuvre originale.»

Autre roman numérique, "Apparitions inquiétantes" est né sous la plume
d’Anne-Cécile Brandenbourger. Il s’agit d’«une longue histoire à lire
dans tous les sens, un labyrinthe de crimes, de mauvaises pensées et de
plaisirs ambigus». Pendant deux ans, cette histoire se construit sous
forme de feuilleton sur le site d’Anacoluthe, en collaboration avec
Olivier Lefèvre. En février 2000, l’histoire est publiée en version
numérique (au format PDF) aux éditions 00h00, en tant que premier titre
de la Collection 2003, consacrée aux écritures numériques, avec version
imprimée à la demande.

00h00 présente l'ouvrage comme «un cyber-polar fait de récits
hypertextuels imbriqués en gigogne. Entre personnages de feuilleton
américain et intrigue policière, le lecteur est - hypertextuellement -
mené par le bout du nez dans cette saga aux allures borgésiennes. (...)
C’est une histoire de meurtre et une enquête policière; des textes
écrits court et montés serrés; une balade dans l’imaginaire des séries
télé; une destructuration (organisée) du récit dans une transposition
littéraire du zapping; et par conséquent, des sensations de lecture
radicalement neuves.»

Suite au succès du livre, les éditions Florent Massot publient en août
2000 une deuxième version imprimée (la première étant celle de 00h00,
imprimée uniquement à la demande), avec une couverture en 3D, un
nouveau titre - "La malédiction du parasol" - et une maquette d’Olivier
Lefèvre restituant le rythme de la version originale.

Anne-Cécile Brandenbourger relate en juin 2000: «Les possibilités
offertes par l’hypertexte m’ont permis de développer et de donner libre
cours à des tendances que j’avais déjà auparavant. J’ai toujours adoré
écrire et lire des textes éclatés et inclassables (comme par exemple
"La vie mode d’emploi" de Perec ou "Si par une nuit d’hiver un
voyageur" de Calvino) et l’hypermédia m’a donné l’occasion de me
plonger dans ces formes narratives en toute liberté. Car, pour créer
des histoires non linéaires et des réseaux de textes qui s’imbriquent
les uns dans les autres, l’hypertexte est évidemment plus approprié que
le papier. Je crois qu’au fil des jours, mon travail hypertextuel a
rendu mon écriture de plus en plus intuitive. Plus "intérieure" aussi
peut-être, plus proche des associations d’idées et des mouvements
désordonnés qui caractérisent la pensée lorsqu’elle se laisse aller à
la rêverie. Cela s’explique par la nature de la navigation
hypertextuelle, le fait que presque chaque mot qu’on écrit peut être un
lien, une porte qui s’ouvre sur une histoire.»

A la même date, Lucie de Boutiny raconte: «Mes "conseillers
littéraires", des amis qui n’ont pas ressenti le vent de liberté qui
souffle sur le web, aimeraient que j’y reste, engluée dans la pâte à
papier. Appliquant le principe de demi-désobéissance, je fais des
allers-retours papier-pixel. L’avenir nous dira si j’ai perdu mon temps
ou si un nouveau genre littéraire hypermédia va naître. (...) Si les
écrivains français classiques en sont encore à se demander s’ils ne
préfèrent pas le petit carnet Clairefontaine, le Bic ou le Mont-Blanc
fétiche, et un usage modéré du traitement de texte, plutôt que
l’ordinateur connecté, voire l’installation, c’est que l’HTX (hypertext
literature) nécessite un travail d’accouchement visuel qui n’est pas la
vocation originaire de l’écrivain papier. En plus des préoccupations du
langage (syntaxe, registre, ton, style, histoire...), le techno-
écrivain - collons-lui ce label pour le différencier - doit aussi
maîtriser la syntaxe informatique et participer à l’invention de codes
graphiques car lire sur un écran est aussi regarder.»


= Mail-romans

Le premier mail-roman francophone est lancé en 2001 par Jean-Pierre
Balpe, chercheur, écrivain et directeur du département hypermédia de
l’Université Paris 8. Pendant très exactement cent jours, entre le 11
avril et le 19 juillet 2001, il diffuse quotidiennement par courriel un
chapitre de "Rien n’est sans dire" auprès de cinq cents personnes - sa
famille, ses amis, ses collègues, etc. - en y intégrant les réponses et
réactions des lecteurs. Racontée par un narrateur, l’histoire est celle
de Stanislas et Zita, qui vivent une passion tragique déchirée par une
sombre histoire politique. « Cette idée d’un mail-roman m’est venue
tout naturellement, relate l’auteur en février 2002. D’une part en me
demandant depuis quelque temps déjà ce qu’internet peut apporter sur le
plan de la forme à la littérature (...) et d’autre part en lisant de la
littérature "épistolaire" du 18e siècle, ces fameux "romans par
lettres". Il suffit alors de transposer: que peut être le "roman par
lettres" aujourd’hui?»

Jean-Pierre Balpe tire plusieurs conclusions de cette expérience:
«D’abord c’est un "genre": depuis, plusieurs personnes m’ont dit lancer
aussi un mail-roman. Ensuite j’ai aperçu quantité de possibilités que
je n’ai pas exploitées et que je me réserve pour un éventuel travail
ultérieur. La contrainte du temps est ainsi très intéressante à
exploiter: le temps de l’écriture bien sûr, mais aussi celui de la
lecture: ce n’est pas rien de mettre quelqu’un devant la nécessité de
lire, chaque jour, une page de roman. Ce "pacte" a quelque chose de
diabolique. Et enfin le renforcement de ma conviction que les
technologies numériques sont une chance extraordinaire du
renouvellement du littéraire.»


= Sites hypermédia

Principe de base du web, le lien hypertexte permet de relier entre eux
des documents textuels et des images. Quant au lien hypermédia, il
permet l’accès à des graphiques, des images animées, des bandes sonores
et des vidéos. Des écrivains férus de nouvelles technologies ne tardent
pas à en explorer les possibilités, dans des sites d’écriture
hypermédia et des oeuvres d’hyperfiction.

Mis en ligne en juin 1997, oVosite est un espace d’écriture conçu par
un collectif de six auteurs issus du département hypermédia de
l’Université Paris 8: Chantal Beaslay, Laure Carlon, Luc Dall’Armellina
(qui est aussi webmestre), Philippe Meuriot, Anika Mignotte et Claude
Rouah. «oVosite est un site web conçu et réalisé (...) autour d’un
symbole primordial et spirituel, celui de l’oeuf, explique Luc
Dall’Armellina en juin 2000. Le site s’est constitué selon un principe
de cellules autonomes qui visent à exposer et intégrer des sources
hétérogènes (littérature, photo, peinture, vidéo, synthèse) au sein
d’une interface unifiante.»

Les possibilités offertes par l’hyperlien ont-elles changé son mode
d’écriture? Sa réponse est à la fois négative et positive.

Négative d’abord: «Non - parce qu’écrire est de toute façon une affaire
très intime, un mode de relation qu’on entretient avec son monde, ses
proches et son lointain, ses mythes et fantasmes, son quotidien et
enfin, appendus à l’espace du langage, celui de sa langue d’origine.
Pour toutes ces raisons, je ne pense pas que l’hypertexte change
fondamentalement sa manière d’écrire, qu’on procède par touches, par
impressions, associations, quel que soit le support d’inscription, je
crois que l’essentiel se passe un peu à notre insu.»

Positive ensuite: «Oui - parce que l’hypertexte permet sans doute de
commencer l’acte d’écriture plus tôt: devançant l’activité de lecture
(associations, bifurcations, sauts de paragraphes) jusque dans l’acte
d’écrire. L’écriture (ceci est significatif avec des logiciels comme
StorySpace) devient peut-être plus modulaire. On ne vise plus tant la
longue horizontalité du récit, mais la mise en espace de ses fragments,
autonomes. Et le travail devient celui d’un tissage des unités entre
elles. L’autre aspect lié à la modularité est la possibilité
d’écritures croisées, à plusieurs auteurs. Peut-être s’agit-il
d’ailleurs d’une méta-écriture, qui met en relation les unités de sens
(paragraphes ou phrases) entre elles.»

Luc ajoute aussi: «La couverture du réseau autour de la surface du
globe resserre les liens entre les individus distants et inconnus. Ce
qui n’est pas simple puisque nous sommes placés devant des situations
nouvelles: ni vraiment spectateurs, ni vraiment auteurs, ni vraiment
lecteurs, ni vraiment interacteurs. Ces situations créent des nouvelles
postures de rencontre, des postures de "spectacture" ou de "lectacture"
(Jean-Louis Weissberg). Les notions de lieu, d’espace, de temps,
d’actualité sont requestionnées à travers ce médium qui n’offre plus
guère de distance à l’événement mais se situe comme aucun autre dans le
présent en train de se faire. L’écart peut être mince entre l’envoi et
la réponse, parfois immédiat (cas de la génération de textes).

Mais ce qui frappe et se trouve repérable ne doit pas masquer les
aspects encore mal définis tels que les changements radicaux qui
s’opèrent sur le plan symbolique, représentationnel, imaginaire et plus
simplement sur notre mode de relation aux autres. "Plus de proximité"
ne crée pas plus d’engagement dans la relation, de même "plus de liens"
ne créent pas plus de liaisons, ou encore "plus de tuyaux" ne créent
pas plus de partage. Je rêve d’un internet où nous pourrions écrire à
plusieurs sur le même dispositif, une sorte de lieu d’atelier
d’écritures permanent et qui autoriserait l’écriture personnelle (c’est
en voie d’exister), son partage avec d’autres auteurs, leur mise en
relation dans un tissage d’hypertextes et un espace commun de notes et
de commentaires sur le travail qui se crée.»

L’avenir de la cyber-littérature est tracé par sa technologie même,
comme l'explique en août 1999 Jean-Paul, webmestre du site hypermédia
cotres.net: «Il est maintenant impossible à un(e) auteur(e) seul(e) de
manier à la fois les mots, leur apparence mouvante et leur sonorité.
Maîtriser aussi bien Director, Photoshop et Cubase, pour ne citer que
les plus connus, c’était possible il y a dix ans, avec les versions 1.
Ça ne l’est plus. Dès demain (matin), il faudra savoir déléguer les
compétences, trouver des partenaires financiers aux reins autrement
plus solides que Gallimard, voir du côté d’Hachette-Matra, Warner,
Pentagone, Hollywood. Au mieux, le statut de... l’écrivaste? du
multimédiaste? sera celui du vidéaste, du metteur en scène, du
directeur de produit: c’est lui qui écope des palmes d’or à Cannes,
mais il n’aurait jamais pu les décrocher seul. Soeur jumelle (et non
pas clone) du cinématographe, la cyber-littérature (= la vidéo + le
lien) sera une industrie, avec quelques artisans isolés dans la
périphérie off-off (aux droits d’auteur négatifs, donc).»

Quelques mois plus tard, en juin 2000, Jean-Paul s'interroge sur
l'apport de l'internet dans son écriture: «La navigation par hyperliens
se fait en rayon (j’ai un centre d’intérêt et je clique méthodiquement
sur tous les liens qui s’y rapportent) ou en louvoiements (de clic en
clic, à mesure qu’ils apparaissent, au risque de perdre de vue mon
sujet). Bien sûr, les deux sont possibles avec l’imprimé. Mais la
différence saute aux yeux: feuilleter n’est pas cliquer. L’internet n’a
donc pas changé ma vie, mais mon rapport à l’écriture. On n’écrit pas
de la même manière pour un site que pour un scénario, une pièce de
théâtre, etc. (...) Depuis, j’écris (compose, mets en page, en scène)
directement à l’écran. L’état "imprimé" de mon travail n’est pas le
stade final, le but; mais une forme parmi d’autres, qui privilégie la
linéarité et l’image, et qui exclut le son et les images animées. (...)
C’est finalement dans la publication en ligne (l’entoilage?) que j’ai
trouvé la mobilité, la fluidité que je cherchais. Le maître mot y est
"chantier en cours", sans palissades. Accouchement permanent, à vue,
comme le monde sous nos yeux. Provisoire, comme la vie qui tâtonne, se
cherche, se déprend, se reprend. Avec évidemment le risque souligné par
les gutenbergs, les orphelins de la civilisation du livre: plus rien
n’est sûr. Il n’y a plus de source fiable, elles sont trop nombreuses,
et il devient difficile de distinguer un clerc d’un gourou. Mais c’est
un problème qui concerne le contrôle de l’information. Pas la
transmission des émotions.»

Jean-Paul fait à nouveau le point sur son activité d’entoileur beaucoup
plus tard, en janvier 2007: «J’ai gagné du temps. J’utilise moins de
logiciels, dont j’intègre le résultat dans Flash. Ce dernier m’assure
de contrôler à 90% le résultat à l’affichage sur les écrans de
réception (au contraire de ceux qui préfèrent présenter des oeuvres
ouvertes, où l’intervention tantôt du hasard tantôt de l’internaute est
recherchée). Je peux maintenant me concentrer sur le coeur de la chose:
l’architecture et le développement du récit. (...) Les deux points
forts des trois ou quatre ans à venir sont: (1) la généralisation du
très haut débit (c’est-à-dire en fait du débit normal), qui va
m’affranchir des limitations purement techniques, notamment des soucis
de poids et d’affichage des fichiers (mort définitive, enfin, des
histogrammes de chargement); (2) le développement de la 3 D. C’est le
récit en hypermédia (= le multimédia + le clic) qui m’intéresse. Les
pièges que pose un récit en 2 D sont déjà passionnants. Avec la 3 D, il
va falloir chevaucher le tigre pour éviter la simple prouesse technique
et laisser la priorité au récit.»



2005: GOOGLE S'INTERESSE A L'EBOOK


= [Résumé]

En octobre 2004, Google lance la première partie de son programme
Google Print, établi en partenariat avec les éditeurs pour consulter à
l’écran des extraits de livres, puis commander les livres auprès d’une
librairie en ligne. En décembre 2004, Google lance la deuxième partie
de son programme Google Print, cette fois à destination des
bibliothèques, le but étant de numériser 15 millions de livres, à
commencer par ceux des bibliothèques de plusieurs universités
partenaires (Harvard, Stanford, Michigan, Oxford) et de la ville de New
York. La version bêta de Google Print est mise en ligne en mai 2005. En
août 2005, le programme est suspendu pour cause de conflit avec les
éditeurs de livres sous droits. Il reprend en août 2006 sous le nom de
Google Books. La numérisation des fonds de grandes bibliothèques se
poursuit, tout comme le développement de partenariats avec les éditeurs
qui le souhaitent. En octobre 2008, Google clôt le conflit avec les
associations d'auteurs et d'éditeurs en annonçant un accord avec eux,
accord qui serait effectif courant 2009.


= Google Print

En 2005, alors que le Projet Gutenberg poursuit la mise en ligne
gratuite des oeuvres du domaine public, tâche immense entreprise depuis
nombre d’années, le livre devient un objet convoité par les géants de
l’internet que sont Google, Yahoo! et Microsoft, d’une part par souci
méritoire de mettre le patrimoine mondial à la disposition de tous,
d’autre part à cause de l’enjeu représenté par les recettes
publicitaires générées par les liens commerciaux accolés aux résultats
des recherches.

Google décide de mettre son expertise au service du livre, et lance la
version bêta de Google Print en mai 2005. Ce lancement est précédé de
deux étapes.

En octobre 2004, Google lance la première partie de son programme
Google Print, établi en partenariat avec les éditeurs pour pouvoir
consulter à l’écran des extraits de livres, puis commander les livres
auprès d’une librairie en ligne.

En décembre 2004, Google lance la deuxième partie de son programme
Google Print, cette fois à destination des bibliothèques. Il s’agit
d’un projet de bibliothèque numérique de 15 millions de livres
consistant à numériser plusieurs grandes bibliothèques partenaires, à
commencer par la bibliothèque de l’Université du Michigan (dans sa
totalité, à savoir 7 millions d’ouvrages), les bibliothèques des
universités de Harvard, de Stanford et d’Oxford, et celle de la ville
de New York. Le coût estimé au départ se situe entre 150 et 200
millions de dollars US, avec la numérisation de 10 millions de livres
sur six ans et un chantier d'une durée totale de dix ans.

En août 2005, soit trois mois après son lancement, Google Print est
suspendu pour un temps indéterminé suite à un conflit grandissant avec
les associations d'auteurs et d'éditeurs de livres sous droits, celles-
ci reprochant à Google de numériser les livres sans l'accord préalable
des ayants droit.


= Google Books

Le programme reprend en août 2006 sous le nom de Google Books (Google
Livres). Google Books permet de rechercher les livres par date, titre
ou éditeur. La numérisation des fonds de grandes bibliothèques se
poursuit, axée cette fois sur les livres libres de droit, tout comme le
développement de partenariats avec les éditeurs qui le souhaitent.

Les livres libres de droit sont consultables à l’écran en texte
intégral, leur contenu est copiable et l’impression est possible page à
page. Ils sont également téléchargeables sous forme de fichiers PDF et
imprimables dans leur entier. Les liens publicitaires associés aux
pages de livres sont situés en haut et à droite de l’écran.

Le conflit avec les associations d'auteurs et d'éditeurs se poursuit
lui aussi, puisque Google continue de numériser des livres sous droits
sans l’autorisation préalable des ayants droit, en invoquant le droit
de citation pour présenter des extraits sur le web. L’Authors Guild et
l’Association of American Publishers (AAP) invoquent pour leur part le
non respect de la législation relative au copyright pour attaquer
Google en justice. Le feuilleton judiciaire dure de nombreux mois.

Fin 2006, d'après le buzz médiatique, Google scannerait 3.000 livres
par jour, ce qui représenterait un million de livres par an. Le coût
estimé serait de 30 dollars par livre - d'autres sources mentionnent un
coût double. Google Books comprendrait 3 millions de livres. Tous
chiffres à prendre avec précaution, la société ne communiquant pas de
statistiques à ce sujet.

A l’exception de la New York Public Library, les collections en cours
de numérisation appartiennent toutes à des bibliothèques universitaires
(Harvard, Stanford, Michigan, Oxford, Californie, Virginie, Wisconsin-
Madison, Complutense de Madrid). S’y ajoutent début 2007 les
bibliothèques des universités de Princeton et du Texas (Austin), ainsi
que la Biblioteca de Catalunya (Catalogne, Espagne) et la Bayerische
Staatbibliothek (Bavière, Allemagne). En mai 2007, Google annonce la
participation de la première bibliothèque francophone, la Bibliothèque
cantonale et universitaire (BCU) de Lausanne (Suisse), pour la
numérisation de 100.000 titres en français, allemand et italien publiés
entre le 17e et le 19e siècle. Suit ensuite un partenariat avec la
Bibliothèque municipale de Lyon (France) signé en juillet 2008 pour
numériser 500.000 livres.

En octobre 2008, après trois ans de conflit, Google met fin aux
poursuites à son encontre par les associations d'auteurs et d'éditeurs.
La société annonce un accord qui serait effectif courant 2009. Cet
accord serait basé sur un partage des revenus générés par Google Books
ainsi qu'un large accès aux ouvrages épuisés, et le paiement de 125
millions de dollars US à l'Authors Guild et l'Association of American
Publishers (AAP) pour clôturer ce conflit.

Suite à cet accord, Google devrait proposer de plus larges extraits des
livres, jusqu'à 20% d'un même ouvrage, avec un lien commercial pour
acheter une copie - numérique ou non - de l'oeuvre. Les ayants droit
auront la possibilité de participer ou non au projet Google Books, et
de retirer leurs livres des collections. Par ailleurs, les
bibliothèques universitaires et publiques (des États-Unis) pourront
accéder à un portail gratuit géré par Google et donnant accès aux
textes de millions de livres épuisés. Un abonnement permettra aux
universités et aux écoles de consulter les collections des
bibliothèques les plus renommées.

En novembre 2008, Google Books comprend 7 millions d'ouvrages
numérisés, en partenariat avec 24 bibliothèques et 2.000 éditeurs
partenaires. Les 24 bibliothèques partenaires se situent principalement
aux Etats-Unis (16), mais aussi en Allemagne (1), en Belgique (1), en
Espagne (2), en France (1), au Japon (1), au Royaume-Uni (1) et en
Suisse (1).

En février 2009, Google Books lance un portail spécifique pour lecture
sur téléphone mobile et smartphone, par exemple sur l'iPhone 3G d'Apple
ou sur le G1 de T-Mobile (ce dernier utilisant Android, la plateforme
de Google). Le catalogue comprend 1,5 million de livres du domaine
public, auxquels s'ajoutent 500.000 autres titres téléchargeables hors
des Etats-Unis, du fait d'une législation du copyright moins
restrictive dans certains pays.



2006: VERS UNE BIBLIOTHEQUE NUMERIQUE PLANETAIRE


= [Résumé]

Lancée en octobre 2005 à l’instigation de l’Internet Archive, l’Open
Content Alliance (OCA) - – qui débute véritablement en 2006 - est un
vaste projet public et coopératif de bibliothèque numérique mondiale
proposant un répertoire multilingue de livres et documents multimédia
pour consultation et téléchargement sur tout moteur de recherche. L'OCA
regroupe de nombreux  partenaires: bibliothèques, universités,
organisations gouvernementales, associations à but non lucratif,
organismes culturels, sociétés informatiques (Adobe, Hewlett Packard,
Microsoft, Yahoo!, Xerox, etc.). Les premiers participants sont les
bibliothèques des universités de Californie et de Toronto, l'European
Archive, les Archives nationales du Royaume-Uni, O'Reilly Media et les
Prelinger Archives. L’OCA souhaite s’inspirer de l’initiative de Google
tout en évitant ses travers, à savoir la numérisation de livres sous
droits sans l’accord préalable des éditeurs, tout comme la consultation
et le téléchargement impossibles sur un autre moteur de recherche.


= L'Internet Archive

En réaction au projet Google Books, l’Internet Archive pense qu'une
bibliothèque à vocation mondiale ne doit pas être liée à des enjeux
commerciaux. Elle s'associe en janvier 2005 à Yahoo! pour mettre sur
pied l’Open Content Alliance (OCA), dans l'optique de fédérer un grand
nombre de partenaires pour créer une bibliothèque planétaire publique
respectueuse du copyright et sur un modèle ouvert.

Qu’est-ce exactement que l’Internet Archive? Fondée en avril 1996 par
Brewster Kahle à San Francisco (Californie), l’Internet Archive a pour
but de constituer, stocker, préserver et gérer une «bibliothèque» de
l’internet, en archivant la totalité du web tous les deux mois, afin
d’offrir un outil de travail aux universitaires, chercheurs et
historiens, et de préserver un historique de l’internet pour les
générations futures.

En octobre 2001, l’Internet Archive met ses archives en accès libre sur
le web grâce à la Wayback Machine, qui permet à tout un chacun de
consulter l’historique d’un site web, à savoir le contenu et la
présentation d’un site web à différentes dates, théoriquement tous les
deux mois à partir de 1996.

L’Internet Archive débute aussi la constitution de collections
numériques telles que le Million Book Project (10.520 livres en avril
2005), des archives de films de la période 1903-1973, des archives de
concerts live récents, des archives de logiciels, etc. Toutes ces
collections sont en consultation libre sur le web.


= L'Open Content Alliance

En janvier 2005, l’Internet Archive s’associe à Yahoo! pour mettre sur
pied l’Open Content Alliance (OCA), une initiative visant à créer un
répertoire libre et multilingue de livres numérisés et de documents
multimédia pour consultation sur n’importe quel moteur de recherche.

L’OCA est officiellement lancée en octobre 2005 et débute véritablement
en 2006. Le but de l’initiative est de s’inspirer de Google Books tout
en évitant ses travers, à savoir la numérisation des livres sous droits
sans l’accord préalable des éditeurs, tout comme la consultation et le
téléchargement impossibles sur un autre moteur de recherche.

L’OCA regroupe de nombreux partenaires: des bibliothèques et des
universités bien sûr, mais aussi des organisations gouvernementales,
des associations à but non lucratif, des organismes culturels et des
sociétés informatiques (Adobe, Hewlett Packard, Microsoft, Yahoo!,
Xerox, etc.).

Les premiers partenaires sont les bibliothèques des universités de
Californie et de Toronto, l’European Archive, les Archives nationales
du Royaume-Uni, O’Reilly Media et les Prelinger Archives. Seuls les
livres appartenant au domaine public sont numérisés, pour éviter les
problèmes de copyright auxquels se heurte Google. Les collections
numérisées sont intégrées à la section Text Archive de l’Internet
Archive.

En décembre 2006, l’Open Content Alliance franchit la barre des 100.000
livres numérisés, avec un rythme de 12.000 nouveaux livres par mois. A
la même date, l’Internet Archive reçoit une subvention d'un million de
dollars de la part de la Sloan Foundation pour numériser les
collections du Metropolitan Museum of Art (l’ensemble des livres et
plusieurs milliers d’images) ainsi que certaines collections de la
Boston Public Library (les 3.800 livres de la bibliothèque personnelle
de John Adams, deuxième président des Etats-Unis), du Getty Research
Institute (une série de livres d'art), de la John Hopkins University
(une série de documents liés au mouvement anti-esclavagiste) et de
l’Université de Californie à Berkeley (une série de documents relatifs
à la ruée vers l’or).

En mai 2007, l’Open Content Alliance franchit la barre des 200.000
livres numérisés.

La barre du million de livres numérisés est atteinte en décembre 2008.

Si Microsoft est un des partenaires de l'OCA, il se lance également
dans l’aventure à titre personnel. En décembre 2006 est mise en ligne
aux Etats-Unis la version bêta de Live Search Books, qui permet une
recherche par mots-clés dans les livres du domaine public. Ces livres
sont numérisés par Microsoft suite à des accords passés avec de grandes
bibliothèques, les premières étant la British Library et les
bibliothèques des universités de Californie et de Toronto, suivies en
janvier 2007 par celles de la New York Public Library et de
l’Université Cornell. Microsoft compte aussi ajouter des livres sous
droits, mais uniquement avec l’accord préalable des éditeurs.

Tout comme Google Books, Live Search Books permet de consulter des
extraits comportant les mots-clés, qui sont eux-même surlignés. Mais
les collections sont moins riches que celles de Google Books, le moteur
de recherche est plus rudimentaire, et il n'est pas possible de
télécharger les livres au format PDF dans leur entier.

En mai 2007, Microsoft annonce des accords avec plusieurs grands
éditeurs, dont Cambridge University Press et McGraw Hill.

Microsoft met finalement un terme à ce projet en mai 2008 pour
concentrer ses efforts sur d'autres activités. Les 750.000 livres déjà
numérisés sont versés dans les collections de l'Open Content Alliance.


= Europeana

En Europe, certains s’inquiètent de l'«hégémonie américaine» que
représente Google Books. Il existe sur le web une Bibliothèque
européenne, qui est en fait un portail commun aux 43 bibliothèques
nationales, lancé en janvier 2004 par la CENL (Conference of European
National Librarians) et hébergé sur le site de la Bibliothèque
nationale des Pays-Bas.

En septembre 2005, la Commission européenne lance une vaste
consultation sur un projet de bibliothèque numérique européenne, avec
réponse requise en janvier 2006, suite à quoi le projet est
officiellement lancé en mars 2006.

«Le plan de la Commission européenne visant à promouvoir l’accès
numérique au patrimoine de l’Europe prend forme rapidement, lit-on dans
le communiqué de presse. Dans les cinq prochaines années, au moins six
millions de livres, documents et autres oeuvres culturelles seront mis
à la disposition de toute personne disposant d’une connexion à
l’internet, par l’intermédiaire de la "bibliothèque numérique
européenne". Afin de stimuler les initiatives de numérisation
européennes, la Commission va cofinancer la création d’un réseau
paneuropéen de centres de numérisation. La Commission abordera
également, dans une série de documents stratégiques, la question du
cadre approprié à adopter pour assurer la protection des droits de
propriété intellectuelle dans le cadre des bibliothèques numériques.»

Europeana et ses deux millions de documents sont disponibles en
novembre 2008, avec un serveur qui déclare rapidement forfait suite à
la très forte demande des premières heures, puis une période
expérimentale avec consultation partielle des collections suite au
«renforcement» de la capacité de ce serveur.



2007: NOUS LISONS SUR DIVERS APPAREILS ELECTRONIQUES


= [Résumé]

Si le Kindle, le lecteur de livres d'Amazon, est lancé en novembre
2007, la lecture sur appareil mobile débute dix ans auparavant. On lit
d'abord sur son ordinateur - portable ou non - avant de lire sur les
agendas électroniques que sont le Psion et les appareils de Franklin.
La société Palm lance en mars 1996 le Palm Pilot, premier PDA du
marché. Le Pocket PC de Microsoft est lancé en avril 2000. Suivent
ensuite les premiers smartphones de Nokia et Sony Ericsson.
Parallèlement, on voit l'émergence de tablettes de lecture dédiées. Les
premières sont le Rocket eBook, le SoftBook Reader ou le Gemstar eBook,
et ne durent pas. Après une période morose, des tablettes plus légères
gagnent en puissance et qualité d'écran, par exemple le Cybook
(nouvelle version) ou le Sony Reader, auquel s'ajoute le Kindle
d'Amazon.com en novembre 2007. L'écran LCD laisse la place à un écran
utilisant la technologie E Ink. On parle maintenant d'un support souple
ultra-fin appelé papier électronique, qui serait lancé par E Ink,
Plastic Logic et d'autres en 2010.


= Tablettes de lecture

# Premiers pas

Les livres numériques sont d’abord lisibles uniquement sur l’écran de
l'ordinateur, que celui-ci soit un ordinateur de bureau ou un
ordinateur portable sinon ultra-portable. Outre le stockage d’un
millier de livres sinon plus - en fonction de la taille du disque dur -
l'ordinateur permet l’utilisation d’outils bureautiques standard,
l’accès au web, l’écoute de fichiers musicaux et le visionnement de
vidéos ou de films. Certains usagers sont également tentés par le
webpad, un ordinateur-écran sans disque dur disposant d’une connexion
sans fil à l’internet, apparu en 2001, ou alors la tablette PC, une
tablette informatique pourvue d’un écran tactile, apparue fin 2002.

En 1999, pour plus de mobilité, on voit apparaître des appareils dédiés
de la taille d'un (gros) livre, souvent appelés ebooks, livres
électroniques, tablettes de lecture ou même liseuses. Les premiers
appareils suscitent un engouement certain, même si peu de gens vont
jusqu'à les acheter, vu leur prix prohibitif et un choix de livres
restreint. Le catalogue de livres numériques est encore ridicule par
rapport à la production imprimée.

Les premières tablettes de lecture sont conçues et développées dans la
Silicon Valley, en Californie. Elles disposent d'un écran à cristaux
liquides (LCD: liquid crystal display) rétro-éclairé ou non, noir et
blanc ou en couleur. Elles fonctionnent sur batterie et disposent d’un
modem intégré et d’un port USB, pour connexion à l’internet et
téléchargement des livres à partir de librairies numériques.

Le modèle le plus connu, le Rocket eBook, est créé par la société
NuvoMedia, financée par la chaîne de librairies Barnes & Noble et le
géant des médias Bertelsmann. Un deuxième modèle, le SoftBook Reader,
est développé par la société SoftBook Press, financée par les deux
grandes maisons d’édition Random House et Simon & Schuster. Plusieurs
autres modèles ont une durée de vie assez courte, par exemple
l’EveryBook, appareil à double écran créé par la société du même nom,
ou encore le Millennium eBook, créé par la société Librius.com. A cette
époque, qui n’est pas si lointaine, toutes ces tablettes électroniques
pèsent entre 700 grammes et 2 kilos et peuvent stocker une dizaine de
livres.

# Gemstar eBook

Lancés en octobre 2000 à New York, les deux premiers modèles de Gemstar
eBook sont les successeurs du Rocket eBook (conçu par NuvoMedia) et du
SoftBook Reader (conçu par SoftBook Press), suite au rachat de
NuvoMedia et de SoftBook Press en janvier 2000 par Gemstar-TV Guide
International, une grosse société spécialisée dans les produits et
services numériques pour les médias.

Commercialisés en novembre 2000 aux Etats-Unis, ces deux modèles - le
REB 1100 (écran noir et blanc, successeur du Rocket eBook) et le REB
1200 (écran couleur, successeur du SoftBook Reader) - sont construits
et vendus sous le label RCA, appartenant à Thomson Multimedia. Le
système d’exploitation, le navigateur et le logiciel de lecture sont
spécifiques au produit, tout comme le format de lecture, basé sur le
format OeB (open ebook).

Les ventes sont très inférieures aux pronostics. En avril 2002, un
article du New York Times annonce l’arrêt de la fabrication de ces
appareils par RCA. En automne 2002, leurs successeurs - le GEB 1150 et
le GEB 2150 - sont produits sous le label Gemstar et vendus par SkyMall
à un prix beaucoup plus compétitif (199 et 349 dollars US), avec ou
sans abonnement annuel ou bisannuel à la librairie numérique Gemstar
eBook. Mais les ventes restent peu concluantes et Gemstar décide de
mettre fin à ses activités eBook. La société cesse la vente de ses
tablettes de lecture en juin 2003 et celle de ses livres numériques le
mois suivant.

# Cybook

Première tablette de lecture européenne, le Cybook (21 x 16 cm, 1 kilo)
est conçu et développé par la société française Cytale, et
commercialisé en janvier 2001. Sa mémoire - 32 Mo (méga-octets) de
mémoire SDRAM (synchronous dynamic random access memory) et 16 Mo de
mémoire flash - permet de stocker 15.000 pages de texte, soit 30 livres
de 500 pages.

«J’ai croisé il y a deux ans le chemin balbutiant d’un projet
extraordinaire, le livre électronique, écrit en décembre 2000 Olivier
Pujol, PDG de Cytale. Depuis ce jour, je suis devenu le promoteur
impénitent de ce nouveau mode d’accès à l’écrit, à la lecture, et au
bonheur de lire. La lecture numérique se développe enfin, grâce à cet
objet merveilleux: bibliothèque, librairie nomade, livre "adaptable",
et aussi moyen d’accès à tous les sites littéraires (ou non), et à
toutes les nouvelles formes de la littérature, car c’est également une
fenêtre sur le web.»

Mais les ventes sont très inférieures aux pronostics et forcent la
société à se déclarer en cessation de paiement. Cytale est mis en
liquidation judiciaire en juillet 2002 et cesse ses activités à la même
date. La commercialisation du Cybook est reprise quelques mois plus
tard par la société Bookeen, créée en 2003 à l’initiative de Michael
Dahan et Laurent Picard, deux ingénieurs de Cytale. En juillet 2007,
Bookeen dévoile la nouvelle version de sa tablette, baptisée Cybook
Gen3, avec un écran utilisant la technologie E Ink.

# Sony Reader

En avril 2004, Sony lance au Japon son premier Reader, le Librié 1000-
EP, produit en partenariat avec les sociétés Philips et E Ink. Cette
tablette est d'ailleurs la première à utiliser la technologie
d’affichage développée par E Ink et dénommée encre électronique.
L’appareil pèse 300 grammes (avec piles et protection d’écran), pour
une taille de 12,6 x 19 x 1,3 centimètres. Sa mémoire est de 10 Mo
(méga-octets) - avec possibilité d’extension - et sa capacité de
stockage de 500 livres. Son écran de 6 pouces a une définition de 170
DPI (dots per inch) et une résolution de 800 x 600 pixels. Un port USB
permet le téléchargement des livres à partir de l’ordinateur.
L’appareil comprend aussi un clavier, une fonction enregistrement et
une synthèse vocale. Il fonctionne avec quatre piles alcalines, qui
permettraient la consultation de 10.000 pages. Son prix est de 375
dollars US. Le Librié cède ensuite la place au Sony Reader, lancé en
octobre 2006 aux Etats-Unis au prix de 350 dollars, avec plusieurs
nouveaux modèles sortis depuis avec succès.

# Kindle

La librairie en ligne Amazon.com lance en novembre 2007 sa propre
tablette de lecture, le Kindle, qui a le format d'un livre (19 x 13 x
1,8 cm, 289 grammes), avec un écran noir et blanc (6 pouces, 800 x 600
pixels), un clavier, une mémoire de 256 Mo (extensible par carte SD) et
enfin une connexion sans fil (wifi) et un port USB. Vendu 400 dollars
US (273 euros), il peut contenir jusqu'à 200 livres parmi les 88.000
disponibles dans le catalogue d'Amazon. 538.000 Kindle sont vendus en
2008. En janvier 2009, Amazon rachète la société Audible.com et sa
collection de livres, journaux et magazines audio - 80.000 titres -
téléchargeables sur baladeurs, téléphones et smartphones. En février
2009, Amazon lance une nouvelle version du Kindle, le Kindle 2, avec un
catalogue de 230.000 titres.


= PDA

# Psion

Le Psion Organiser est le vétéran des agendas électroniques. Le premier
modèle est lancé dès 1984 par la société britannique Psion. Au fil des
ans, la gamme des appareils s’étend et la société se développe à
l’international. En 2000, les divers modèles (Série 7, Série 5mx, Revo,
Revo Plus) sont concurrencés par le Palm Pilot et le Pocket PC. Les
ventes baissent et la société décide de diversifier ses activités.
Suite au rachat de Teklogix par Psion, Psion Teklogix est fondé en
septembre 2000 pour développer des solutions mobiles sans fil à
destination des entreprises. Psion Software est fondé en 2001 pour
développer les logiciels de la nouvelle génération d’appareils mobiles
utilisant la plateforme Symbian OS, par exemple ceux du smartphone
Nokia 9210, modèle précurseur commercialisé la même année.

Enseignante-chercheuse à l’Ecole pratique des hautes études (EPHE,
Paris-Sorbonne), Marie-Joseph Pierre utilise un Psion depuis plusieurs
années pour lire et étudier dans le train lors de ses fréquents
déplacements entre Argentan (Normandie), sa ville de résidence, et
Paris. Elle achète son premier Psion en 1997, un Série 3, remplacé
ensuite par un Série 5, remplacé lui-même par un Psion 5mx en juin
2001.

En février 2002, elle raconte: «J’ai chargé tout un tas de trucs
littéraires – dont mes propres travaux et dont la Bible entière – sur
mon Psion 5mx (16 + 16 Mo), que je consulte surtout dans le train ou
pour mes cours, quand je ne peux pas emporter toute une bibliothèque.
J’ai mis les éléments de programme qui permettent de lire page par page
comme sur un véritable ebook. Ce qui est pratique, c’est de pouvoir
charger une énorme masse documentaire sur un support minuscule. Mais ce
n’est pas le même usage qu’un livre, surtout un livre de poche qu’on
peut feuilleter, tordre, sentir..., et qui s’ouvre automatiquement à la
page qu’on a aimée. C’est beaucoup moins agréable à utiliser, d’autant
que sur PDA, la page est petite: on n’a pas de vue d’ensemble. Mais
avec une qualité appréciable: on peut travailler sur le texte
enregistré, en rechercher le vocabulaire, réutiliser des citations,
faire tout ce que permet le traitement informatique du document, et
cela m’a pas mal servi pour mon travail, ou pour mes activités
associatives. Je fais par exemple partie d’une petite société poétique
locale, et nous faisons prochainement un récital poétique. J’ai voulu
rechercher des textes de Victor Hugo, que j’ai maintenant pu lire et
même charger à partir du site de la Bibliothèque nationale de France:
c’est vraiment extra.»

# eBookMan (Franklin)

Basée dans le New Jersey (Etats-Unis), la société Franklin
commercialise dès 1986 le premier dictionnaire consultable sur une
machine de poche. Quinze ans plus tard, Franklin distribue 200 ouvrages
de référence sur des machines de poche: dictionnaires unilingues et
bilingues, encyclopédies, bibles, manuels d’enseignement, ouvrages
médicaux et livres de loisirs.

En octobre 2000, Franklin lance l’eBookMan, un assistant personnel
multimédia qui - entre autres fonctionnalités (agenda, dictaphone,
etc.) - permet la lecture de livres numériques sur le logiciel de
lecture Franklin Reader. A la même date, l’eBookMan reçoit l’eBook
Technology Award de la Foire internationale du livre de Francfort.
Trois modèles (EBM-900, EBM-901 et EBM-911) sont disponibles début
2001. Leurs prix respectifs sont de 130, 180 et 230 dollars US. Le prix
est fonction de la taille de la mémoire vive (8 ou 16 Mo) et de la
qualité de l’écran à cristaux liquides (écran LCD), rétro-éclairé ou
non selon les modèles. Nettement plus grand que celui de ses
concurrents, l’écran n’existe toutefois qu’en noir et blanc,
contrairement à la gamme Pocket PC ou à certains modèles Palm avec
écran couleur. L’eBookMan permet l’écoute de livres audio et de
fichiers musicaux au format MP3.

En octobre 2001, Franklin décide de ne pas intégrer le Microsoft Reader
à l’eBookMan, mais de lui préférer le Mobipocket Reader, logiciel de
lecture jugé plus performant, et primé à la même date par l’eBook
Technology Award de la Foire internationale du livre de Francfort.
Parallèlement, le Franklin Reader est progressivement disponible pour
les gammes d'appareils mobiles Psion, Palm, Pocket PC et Nokia.
Franklin développe aussi une librairie numérique sur son site en
passant des partenariats avec plusieurs sociétés, notamment avec
Audible.com pour avoir accès à sa collection de 4.500 livres
audionumériques.

# Palm Pilot et Pocket PC

Lorsque le livre numérique commence à se généraliser en 2000, tous les
fabricants de PDA décident d’intégrer un logiciel de lecture dans leur
machine, en plus des fonctionnalités standard (agenda, dictaphone,
lecteur de MP3, etc.). En parallèle, ils négocient les droits de
diffusion numérique de centaines de titres, soit directement soit par
le biais de librairies numériques. Si certains professionnels du livre
s’inquiètent de la petitesse de l’écran, les adeptes de la lecture sur
PDA assurent que la taille de l’écran n’est pas un problème. Les grands
favoris du marché sont les gammes Palm Pilot et Pocket PC.

La société Palm lance le premier Palm Pilot en mars 1996 et vend 23
millions de machines entre 1996 et 2002. Le système d’exploitation du
Palm Pilot est le Palm OS et son logiciel de lecture le Palm Reader. En
mars 2001, la gamme Palm Pilot propose plusieurs modèles et permet la
lecture de livres numériques sur le Mobipocket Reader.

Commercialisé par Microsoft en avril 2000, le Pocket PC utilise le
système d’exploitation Windows CE, qui intègre le nouveau logiciel de
lecture Microsoft Reader. En octobre 2001, Windows CE est remplacé par
Pocket PC 2002, qui permet entre autres de lire des livres numériques
sous droits. Ces livres sont protégés par un système de gestion des
droits numériques, le Microsoft DAS Server (DAS: digital asset server).
En 2002, la gamme Pocket PC propose plusieurs modèles et permet la
lecture sur trois logiciels: le Microsoft Reader bien sûr, le
Mobipocket Reader et le Palm Reader.

Le marché des PDA poursuit sa croissance. D’après The Seybold Report,
on compte 17 millions de PDA dans le monde en avril 2001 (et seulement
100.000 tablettes de lecture). 13,2 millions de PDA sont vendus en
2001, et 12,1 millions en 2002. En 2002, la gamme Palm Pilot est
toujours le leader du marché (36,8% des machines vendues), suivi par la
gamme Pocket PC de Microsoft et les modèles de Hewlett-Packard, Sony,
Handspring, Toshiba et Casio. Les systèmes d'exploitation utilisés sont
essentiellement le Palm OS (pour 55% des machines) et le Pocket PC
(pour 25,7% des machines).

En 2004, on note une plus grande diversité des modèles et une baisse
des prix chez tous les fabricants. Les trois principaux fabricants sont
Palm, Sony et Hewlett-Packard. Suivent Handspring, Toshiba, Casio et
d'autres. Mais le PDA est de plus en plus concurrencé par le
smartphone, qui est un téléphone portable doublé d'un PDA, et les
ventes commencent à baisser. En février 2005, Sony décide de se retirer
complètement du marché des PDA.


= Smartphones

Le premier smartphone est le Nokia 9210, modèle précurseur lancé en
2001 par la société finlandaise Nokia, grand fabricant mondial de
téléphones portables. Apparaissent ensuite le Nokia Series 60, le Sony
Ericsson P800, puis les modèles de Motorola et de Siemens. Ces
différents modèles permettent de lire des livres numériques sur le
Mobipocket Reader.

Appelé aussi téléphone multimédia, téléphone multifonctions ou encore
téléphone intelligent, le smartphone dispose d’un écran couleur, du son
polyphonique et de la fonction appareil photo, qui viennent s'ajouter
aux fonctions habituelles de l’assistant personnel: agenda, dictaphone,
lecteur de livres numériques, lecteur de musique, etc.

Les smartphones représentent 3,7% des ventes de téléphones portables en
2004 et 9% des ventes en 2006, à savoir 90 millions d'unités sur un
milliard.

Si les livres numériques ont une longue vie devant eux, les appareils
de lecture risquent de muer régulièrement. Denis Zwirn, président de
Numilog, grande librairie en ligne francophone, explique en février
2003: «L’équipement des individus et des entreprises en matériel
pouvant être utilisé pour la lecture numérique dans une situation de
mobilité va continuer de progresser très fortement dans les dix
prochaines années sous la forme de machines de plus en plus
performantes (en terme d’affichage, de mémoire, de fonctionnalités, de
légèreté...) et de moins en moins chères. Cela prend dès aujourd’hui la
forme de PDA (Pocket PC et Palm Pilot), de tablettes PC et de
smartphones, ou de smart displays (écrans tactiles sans fil). Trois
tendances devraient être observées: la convergence des usages
(téléphone/PDA), la diversification des types et tailles d’appareils
(de la montre-PDA-téléphone à la tablette PC waterproof), la
démocratisation de l’accès aux machines mobiles (des PDA pour enfants à
15 euros). Si les éditeurs et les libraires numériques savent en saisir
l’opportunité, cette évolution représente un environnement
technologique et culturel au sein duquel les livres numériques, sous
des formes variées, peuvent devenir un mode naturel d’accès à la
lecture pour toute une génération.»

On se demande si des tablettes dédiées peuvent vraiment réussir à
s’imposer face aux smartphones multifonctions.  On se demande aussi
s'il existe une clientèle spécifique pour les deux machines, la lecture
sur téléphone portable et smartphone étant destinée au grand public, et
la lecture sur tablette étant réservée aux gros consommateurs de
documents que sont les lycéens, les étudiants, les professeurs, les
chercheurs ou les juristes. Le débat n'est pas prêt d'être clos.

La compétition risque d’être rude sur un marché très prometteur. Reste
à voir quels modèles seront retenus par l'usager parce que solides,
légers, économiques et procurant un véritable «confort de lecture»,
sans oublier l'aspect esthétique et les possibilités de lecture en 3 D.
Selon Jean-Paul, webmestre du site hypermédia cotres.net, interviewé en
janvier 2007, «on progresse. Les PDA et autres baladeurs multimédia ont
formé le public à manipuler des écrans tactiles de dimension
individuelle (par opposition aux bornes publiques de circulation et
autres tirettes-à-sous). L’hypermédia est maintenant une évidence. Il
ne reste plus qu’à laisser se bousculer les ingénieurs et les
marketteurs pour voir sortir un objet rentable, léger, attirant, peu
fragile, occupant au mieux l’espace qui sépare les deux mains d’un
terrien assis dans le bus ou sur sa lunette WC: la surface d’une
feuille A4 en format italien, soit ± 800 x 600 pixels. Bien sûr, ce que
montrera cette surface ne sera pas en 2 D mais en 3 D. Comme les GPS
prochaine génération, ou les écrans de visée sur le cockpit d’un A-
Win.»

On nous parle de papier électronique pour 2010, avec les sociétés E Ink
et Plastic Logic en tête de file pour nous proposer des supports de
lecture souples et ultra-fins.



2008: LES EBOOKS SONT PARTOUT


= [Résumé]

En 2008, offrir un livre numérique devient «tendance», et le lire sur
son smartphone l'est encore plus. Preuve que les choses ont bien évolué
depuis la panique ayant saisi les éditeurs et libraires à la fin des
années 1990. Trois termes paraissent essentiels en 2008: stockage,
organisation et diffusion. Dans un proche avenir, on devrait disposer
de l’ensemble du patrimoine mondial stocké sous forme numérique, d’une
organisation effective de l’information et d'un réseau internet
omniprésent. Confidentiel en 2000, puis parent pauvre des fichiers
musicaux et vidéo, le livre numérique est en bonne place à côté de la
musique et des films. Editeur puis consultant en édition électronique,
Nicolas Pewny voit «le livre numérique du futur comme un "ouvrage
total" réunissant textes, sons, images, vidéo, interactivité: une
nouvelle manière de concevoir et d’écrire et de lire, peut-être sur un
livre unique, sans cesse renouvelable, qui contiendrait tout ce qu’on a
lu, unique et multiple compagnon.»


= [Texte]

Fondateur du Projet Gutenberg en 1971, Michael Hart précise souvent
dans ses écrits que, si Gutenberg a permis à chacun d'avoir ses propres
livres - jusque-là réservés à une élite -, le Projet Gutenberg permet à
chacun d'avoir une bibliothèque complète - jusque-là réservée à la
collectivité -, sur un support qu'on peut glisser dans sa poche, le
support optimal actuel étant la clé USB. Le Projet Gutenberg compte
près de 30.000 livres en novembre 2008, soit la taille d'une
bibliothèque publique de quartier.

Le futur sera-t-il le cyberespace décrit par le philosophe Timothy
Leary en 1994 dans son livre "Chaos et cyberculture"? «Toute
l’information du monde est à l’intérieur (de gigantesques bases de
données, NDLR). Et grâce au cyberespace, tout le monde peut y avoir
accès. Tous les signaux humains contenus jusque-là dans les livres ont
été numérisés. Ils sont enregistrés et disponibles dans ces banques de
données, sans compter tous les tableaux, tous les films, toutes les
émissions de télé, tout, absolument tout.» Nous n'en sommes pas encore
là. Mais sur les 30 millions de livres du domaine public que
compteraient les bibliothèques (sans compter les différentes éditions),
5 millions seraient déjà librement disponibles sur le web.

Tim Berners-Lee est l'inventeur du web en 1990. A la question de Pierre
Ruetschi, journaliste à la Tribune de Genève, un quotidien suisse:
«Sept ans plus tard, êtes-vous satisfait de la façon dont le web a
évolué?», il répond en décembre 1997 que, s’il est heureux de la
richesse et de la variété de l’information disponible, le web n’a pas
encore la puissance prévue dans sa conception d’origine. Il aimerait
«que le web soit plus interactif, que les gens puissent créer de
l’information ensemble», et pas seulement consommer celle qui leur est
proposée. Le web doit devenir «un média de collaboration, un monde de
connaissance que nous partageons».

Son souhait commence à se concrétiser sept ans après, avec ce qu'on
appelle le web 2.0. La paternité de l'expression «web 2.0» revient
d’ailleurs à un éditeur, Tim O’Reilly, qui utilise cette expression
pour la première fois en 2004 comme titre d'une série de conférences.
Le web ne vise plus seulement à utiliser l’information. Il incite aussi
les usagers à échanger et collaborer en ligne, sur des blogs, des wikis
ou des encyclopédies coopératives comme Wikipédia et Citizendium.

Un enjeu tout aussi important est l'accessibilité de l'internet pour
tous. Mis en ligne en septembre 2000 par l’association du même nom, le
site Handicapzéro devient en février 2003 un portail généraliste
offrant un accès adapté à l’information pour les Francophones ayant un
problème visuel, à savoir plus de 10% de la population. Le portail
offre des informations dans nombre de domaines: actualités, programmes
de télévision, météo, santé, emploi, consommation, loisirs, sports,
téléphonie, etc. Les personnes aveugles peuvent accéder au site au
moyen d’une plage braille ou d’une synthèse vocale. Les personnes
malvoyantes peuvent paramétrer sur la page d’accueil la taille et la
police des caractères ainsi que la couleur du fond d’écran pour une
navigation confortable. Les personnes voyantes peuvent correspondre en
braille avec des aveugles par le biais du site. En octobre 2006, le
portail adopte une nouvelle présentation en enrichissant encore son
contenu, en adoptant une navigation plus intuitive pour la page
d’accueil, en proposant des raccourcis de clavier, en offrant un
service amélioré pour l’affichage «confort de lecture», etc. Plus de 2
millions de visiteurs utilisent les services du portail au cours de
l'année 2006. Handicapzéro entend ainsi démontrer «que, sous réserve du
respect de certaines règles élémentaires, l’internet peut devenir enfin
un espace de liberté pour tous».

Un autre enjeu est l'infrastructure de l'internet. La connexion au
réseau est désormais plus facile, avec la DSL (digital subscriber
line), le câble ou la fibre optique, tout comme les technologies WiFi
(wireless fidelity) pour un secteur géographique limité et WiMAX
(worldwide interoperability for microwave access) pour un secteur
géographique étendu. Jean-Paul, webmestre du site hypermédia
cotres.net, résume la situation en janvier 2007: «J’ai l’impression que
nous vivons une période "flottante", entre les temps héroïques, où il
s’agissait d’avancer en attendant que la technologie nous rattrape, et
le futur, où le très haut débit va libérer les forces qui commencent à
bouger, pour l’instant dans les seuls jeux.»

La prochaine génération de l’internet serait un réseau pervasif
permettant de se connecter en tout lieu et à tout moment sur tout type
d’appareil à travers un réseau unique et omniprésent. Le concept de
réseau pervasif est développé par Rafi Haladjian, fondateur de la
société Ozone. «La nouvelle vague touchera notre monde physique, notre
environnement réel, notre vie quotidienne dans tous les instants,
explique-t-il en 2007. Nous n’accéderons plus au réseau, nous
l’habiterons. Les composantes futures de ce réseau (parties filiaires,
parties non filiaires, opérateurs) seront transparentes à l’utilisateur
final. Il sera toujours ouvert, assurant une permanence de la connexion
en tout lieu. Il sera également agnostique en terme d’application(s),
puisque fondé sur les protocoles mêmes de l’internet.» (extrait du site
web d'Ozone)

Pierre Schweitzer, inventeur du projet @folio, une tablette de lecture
nomade, écrit en décembre 2006: «La chance qu’on a tous est de vivre
là, ici et maintenant cette transformation fantastique. Quand je suis
né en 1963, les ordinateurs avaient comme mémoire quelques pages de
caractères à peine. Aujourd’hui, mon baladeur de musique pourrait
contenir des milliards de pages, une vraie bibliothèque de quartier.
Demain, par l’effet conjugué de la loi de Moore et de l’omniprésence
des réseaux, l’accès instantané aux oeuvres et aux savoirs sera de
mise. Le support de stockage lui-même n’aura plus beaucoup d’intérêt.
Seules importeront les commodités fonctionnelles d’usage et la poétique
de ces objets.»

Pierre ajoute: «La lecture numérique dépasse de loin, de très loin
même, la seule question du "livre" ou de la presse, Le livre et le
journal restent et resteront encore, pour longtemps, des supports de
lecture techniquement indépassables pour les contenus de valeur ou pour
ceux dépassant un seuil critique de diffusion. Bien que leur modèle
économique puisse encore évoluer (comme pour les "gratuits" la presse
grand public), je ne vois pas de bouleversement radical à l’échelle
d’une seule génération. Au-delà de cette génération, l’avenir nous le
dira. On verra bien. Pour autant, d’autres types de contenus se
développent sur les réseaux. Internet défie l’imprimé sur ce terrain-
là: celui de la diffusion en réseau (dématérialisée = coût marginal
nul) des oeuvres et des savoirs. Là où l’imprimé ne parvient pas à
équilibrer ses coûts. Là où de nouveaux acteurs peuvent venir prendre
leur place.

Or, dans ce domaine nouveau, les équilibres économiques et les logiques
d’adoption sont radicalement différents de ceux que l’on connaît dans
l’empire du papier - voir par exemple l’évolution des systèmes de
validation pour les archives ouvertes dans la publication scientifique.
Ou les modèles économiques émergents de la presse en ligne. Il est donc
vain, dangereux même, de vouloir transformer au forceps l’écologie du
papier - on la ruinerait à vouloir le faire! À la marge, certains
contenus très spécifiques, certaines niches éditoriales, pourraient
être transformées - l’encyclopédie ou la publication scientifique le
sont déjà: de la même façon, les guides pratiques, les livres
d’actualité quasi-jetables et quelques autres segments qui envahissent
les tables des librairies pourraient l’être, pour le plus grand bonheur
des libraires. Mais il n’y a là rien de massif ou brutal selon moi: nos
habitudes de lecture ne seront pas bouleversées du jour au lendemain,
elles font partie de nos habitudes culturelles, elles évoluent
lentement, au fur et à mesure de leur adoption (= acceptation) par les
générations nouvelles.»

Marc Autret, journaliste et infographiste, écrit pour sa part à la même
date: «Sans vouloir faire dans la divination, je suis convaincu que
l’e-book (ou "ebook": impossible de trancher!) a un grand avenir dans
tous les secteurs de la non-fiction. Je parle ici de livre numérique en
termes de "logiciel", pas en terme de support physique dédié (les
conjectures étant plus incertaines sur ce dernier point). Les éditeurs
de guides, d’encyclopédies et d’ouvrages informatifs en général
considèrent encore l’e-book comme une déclinaison très secondaire du
livre imprimé, sans doute parce que le modèle commercial et la sécurité
de cette exploitation ne leur semblent pas tout à fait stabilisés
aujourd’hui. Mais c’est une question de temps. Les e-books non
commerciaux émergent déjà un peu partout et opèrent d’une certaine
façon un défrichage des possibles. Il y a au moins deux axes qui
émergent: (a) une interface de lecture/consultation de plus en plus
attractive et fonctionnelle (navigation, recherche, restructuration à
la volée, annotations de l’utilisateur, quizz interactif...); (b) une
intégration multimédia (vidéo, son, infographie animée, base de
données, etc.) désormais fortement couplée au web. Aucun livre physique
n’offre de telles fonctionnalités. J’imagine donc l’e-book de demain
comme une sorte de wiki cristallisé, empaqueté dans un format. Quelle
sera alors sa valeur propre? Celle d’un livre: l’unité et la qualité du
travail éditorial!»

Denis Zwirn, président de Numilog, grande librairie en ligne
francophone, voit 2008 comme une date essentielle dans la courbe de
croissance du marché des livres numériques, avec la conjonction de
trois facteurs:

«(1) le développement de vastes catalogues en ligne utilisant
pleinement les fonctionnalités de la recherche plein texte dans les
livres numérisés, comme ceux de la future Bibliothèque numérique
européenne, de VollTextSuche Online, de Google et d'Amazon. Une fois le
contenu trouvé dans un des ouvrages ainsi "sondé" par ce type de
recherche révolutionnaire pour le grand public, il est naturel de
vouloir accéder à la totalité de l'ouvrage... dans sa version
numérique.

(2) Des progrès techniques cruciaux tels que la proposition commerciale
d'appareils de lecture à base d'encre électronique améliorant
radicalement l'expérience de lecture finale pour l'usager en la
rapprochant de celle du papier. Par exemple l'iLiad d'Irex ou le Sony
Reader, mais bien d'autres appareils s'annoncent. Le progrès concerne
toutefois tout autant le développement des nouveaux smartphones
multifonctions comme les BlackBerry ou l'iPhone, ou la proposition de
logiciels de lecture à l'interface fortement améliorée et pensée pour
les ebooks sur PC, comme Adobe Digital Edition.

(3) Enfin, le changement important d'attitude de la part des
professionnels du secteur, éditeurs, et probablement bientôt aussi
libraires. Les éditeurs anglo-saxons universitaires ont massivement
tracé une route que tous les autres sont en train de suivre, en tout
cas aux Etats-Unis, en Europe du Nord et en France: proposer une
version numérique de tous les ouvrages. Même pour les plus réticents
encore il y a quelques années, ce n'est plus une question de
"pourquoi?", c'est simplement devenu une question de "comment?". Les
libraires ne vont pas tarder à considérer que vendre un livre numérique
fait partie de leur métier normal.

Selon Denis, «le livre numérique n'est plus une question de colloque,
de définition conceptuelle ou de divination par certains "experts":
c'est un produit commercial et un outil au service de la lecture. Il
n'est pas besoin d'attendre je ne sais quel nouveau mode de lecture
hypermoderne et hypertextuel enrichi de multimédia orchestrant
savamment sa spécificité par rapport au papier, il suffit de proposer
des textes lisibles facilement sur les supports de lecture électronique
variés qu'utilisent les gens, l'encre électronique pouvant
progressivement envahir tous ces supports. Et de les proposer de
manière industrielle. Ce n'est pas et ne sera jamais un produit de
niche (les dictionnaires, les guides de voyage, les non voyants...):
c'est en train de devenir un produit de masse, riche de formes
multiples comme l'est le livre traditionnel.»



2009: CYBERESPACE ET SOCIETE DE L'INFORMATION


= [Résumé]

En 2009 (date de publication de ce livre), il semblerait que le
cyberespace devienne omniprésent dans une société dite de
l'information, si ce n'est déjà fait. Comment définir cyberespace et
société de l'information?  Voici les réponses des professionnels du
livre interviewés au fil des ans, qui remplaceront une conclusion pour
ouvrir au contraire des perspectives. Pour mémoire, la paternité du
terme «cyberespace» revient à William Gibson, qui utilise ce terme dans
son roman "Neuromancien", paru en 1984: «Cyberespace: une hallucination
consensuelle expérimentée quotidiennement par des milliards
d'opérateurs réguliers, dans chaque nation, par des enfants à qui on
enseigne des concepts mathématiques... Une représentation graphique des
données extraites des banques de tous les ordinateurs dans le système
humain. Complexité incroyable. Des lignes de lumière qui vont dans le
non-espace de l'esprit, des agglomérats et des constellations de
données. Et qui fuient, comme les lumières de la ville.» Quant à la
société de l'information, elle n'est pas si récente. On annonce
régulièrement son avènement depuis plus de trente ans, comme le
rappelle Jacques Pataillot, conseiller en management chez Cap Gemini
Ernst & Young: «C'est un vieux concept, dont on parlait déjà en 1975!
Seules les technologies ont changé.»


= Cyberespace

# Auteurs

Alex Andrachmes, producteur audiovisuel, écrivain et explorateur
d'hypertexte, se demande de quel cyberespace on parle: «Celui des
Gibson, inventeur de la formule, des Spinrad ou des Clarke, utopies
scientifiques pas toujours traitées comme elles devraient l'être? Ou
celui des AOL/Time-Warner, des Microsoft ou des... J6M-
Canal/Universal... Tout ce qu'on peut dire à l'heure actuelle, c'est
que ce qu'on peut encore appeler le cyberspace est multiforme, et qu'on
ne sait pas qui le domptera. Ni s'il faut le dompter d'ailleurs... En
tout cas, les créateurs, artistes, musiciens, les sites scientifiques,
les petites "start-up" créatives, voire les millions de pages perso,
les chats, les forums, et tout ce qui donne au net sa matière propre ne
pourra être ignoré par les grands mangeurs de toile. Sans eux, ils
perdraient leurs futurs "abonnés". Ce paradoxe a son petit côté
subversif qui me plaît assez.»

D'après Lucie de Boutiny, romancière multimédia, le cyberespace est «le
délire SF du type: "bienvenue dans la 3e dimension, payez-vous du sexe,
des voyages et des vies virtuels" a toujours existé. La méditation,
l'ésotérisme, les religions y pourvoient, etc. Maintenant, on est dans
le cyberspace.»

Jean-Pierre Cloutier, auteur des Chroniques de Cybérie, une chronique
hebdomadaire des actualités de l'internet, définit le cyberespace comme
«un monde parallèle, un espace où se déroule l'ensemble des activités
d'information, de communication, et d'échanges (y compris échanges
commerciaux) désormais permises par le réseau. Il y a un centre,
autonome, très interconnecté qui vit par et pour lui-même. Puis des
collectivités plus ou moins ouvertes, des espaces réservés (intranets),
des sous-ensembles (AOL, CompuServe). Il y a ensuite de très longues
frontières où règne une culture mixte, hybride, issue du virtuel et du
réel (on pense aux imprimés qui ont des versions web, aux sites
marchands). Il y a aussi un sentiment d'appartenance à l'une ou l'autre
de ces régions du cyberespace, et un sentiment d'identité.»

Selon Luc Dall'Armellina, co-auteur et webmestre d'oVosite, un espace
d'écriture hypermédia, «ce pourrait-être quelque chose comme l'ensemble
électrique mouvant, le système invisible mais cohérent des êtres
humains sensibles et des interfaces intelligentes dont les activités
sont tout ou en partie réglées, conditionnées ou co-régulées à travers
leurs machines connectées ensemble. Peut-être plus simplement: la
virtualisation sensible et numérique de l'inconscient collectif...»

Jean-Paul, webmestre du site hypermédia cotres.net, définit le
cyberespace comme «un lieu isotrope en expansion pour l'instant
infinie. Un modèle de la vision que nous avons aujourd'hui de
l'univers. Jusqu'à l'invention du clic, le savoir humain était senti
comme un espace newtonien, avec deux repères absolus: le temps
(linéaire: un début, une fin) et l'espace (les trois dimensions du
temple, du rouleau, du volumen). Le cyberespace obéit aux lois de
l'hypertexte. Deux temps simultanés: le temps taxé (par le fournisseur
d'accès ou par les impératifs de productivité, égrené par l'antique
chrono), et le temps aboli, qui fait passer d'un lien à l'autre, d'un
lieu à l'autre à la vitesse de l'électron, dans l'illusion du
déplacement instantané. Quant aux repères, quiconque a lancé une
recherche dans cet espace sait qu'il doit lui-même les définir pour
l'occasion, et se les imposer (sous peine de se disperser, de se
dissoudre), pour échapper au vertige de la vitesse. A cause de cette
"vitesse de la pensée", nous trouvons dans cet espace un "modèle" de
notre cerveau. "Ça tourne dans ma tête", à travers 10, 20, etc...
synapses à la fois, comme un fureteur archivant la toile. Bref les lois
du cyberespace sont celles du rêve et de l'imagination.»

Pour Anne-Bénédicte Joly, écrivain auto-éditant ses livres, le
cyberespace est «le domaine virtuel créé par la mise en relation de
plusieurs ordinateurs communiquant et échangeant entre eux».

Naomi Lipson, écrivain multimédia, traductrice et peintre, ajoute:
«J'aime la métaphore du labyrinthe. Le média se nourrissant lui-même,
le cyberespace contient une infinité de sites sur les labyrinthes.»

Pour Tim McKenna, écrivain et philosophe, «le cyberespace est
l'ensemble des liens existant entre les individus utilisant les
technologies pour communiquer entre eux, soit pour partager des
informations, soit pour discuter. Dire qu'une personne existe dans le
cyberespace revient à dire qu'elle a éliminé la distance en tant que
barrière empêchant de relier personnes et idées.»

Pour Xavier Malbreil, auteur multimédia et modérateur de la liste e-
critures, il s'agit d'«une interconnexion de tous, partout. Avec le
libre accès à des banques de données, pour insuffler également du
contenu dans les échanges interpersonnels.»

Pour Murray Suid, auteur de livres pédagogiques et de logiciels
éducatifs, «le cyberespace est n'importe où, c'est-à-dire partout.
L'exemple le plus simple est ma boîte aux lettres électronique, qui me
suit où que j'aille.»

# Documentalistes

Emmanuel Barthe, documentaliste juridique et modérateur de la liste de
discussion Juriconnexion, relate: «Je ne visualise pas le cyberespace
comme véritable espace physique mais comme un immense média néanmoins
concentré en un lieu unique: l'écran de l'ordinateur. En revanche, je
conçois/pense le cyberespace comme un forum ou une assemblée antique:
beaucoup d'animation, diversité des opinions, des discours, des gens
qui se cachent dans les recoins, des personnes qui ne se parlent pas,
d'autres qui ne parlent qu'entre eux...»

Selon Bakayoko Bourahima, documentaliste à l'ENSEA (Ecole nationale
supérieure de statistique et d'économie appliquée) d'Abidjan, «il y a
encore un peu de fantasme autour de ce mot. Quand j'ai fait
connaissance avec ce mot (utilisé par Jean-Claude Guédon et Nicholas
Négroponte), il m'avait d'abord laissé l'illusion d'un espace extra-
terrestre où les ordinateurs et leurs utilisateurs se transportaient
pour échanger des données et communiquer. Depuis que je navigue moi-
même, je me rends compte qu'il s'agit tout simplement d'un espace
virtuel traduisant le cadre de communication qui rassemble les
internautes à travers le monde.»

Pour Peter Raggett, sous-directeur de la Bibliothèque centrale de
l'OCDE (Organisation de coopération et de développement économiques),
«le cyberespace est cette zone "extérieure" qui se trouve de l'autre
côté du PC lorsqu'on se connecte à l'internet. Pour ses utilisateurs ou
ses clients, tout fournisseur de services internet ou serveur de pages
web se trouve donc dans le cyberespace.»

# Editeurs

Pour Marie-Aude Bourson, créatrice de Gloupsy, site littéraire destiné
aux nouveaux auteurs, le cyberespace est «un espace d'expression, de
liberté et d'échanges où tout peut aller très (trop) vite».

Pour Pierre-Noël Favennec, expert à la direction scientifique de France
Télécom R&D et directeur de collection, «le cyberespace est un monde où
je suis relié par l'image et le son et sans fil avec qui je veux, quand
je veux et où je veux, où j'ai accès à toutes les documentations et
informations souhaitées, et dans lequel ma vie est facilitée par les
agents intelligents et les objets communicants.»

Pour Jacky Minier, créateur de Diamedit, site de promotion d'inédits
artistiques et littéraires, «c'est un espace de liberté pour
l'imaginaire, une dimension inexplorée de la planète, une jungle et un
paradis tout à la fois, où tout est possible même si tout n'est pas
permis par l'éthique, où le contenu du portefeuille des intervenants
n'a aucun rapport direct avec la valeur des contenus des sites. C'est
avant tout une vaste agora, une place publique où l'on s'informe et où
l'on informe. Ça peut être également une place de foires et marchés,
mais l'argent n'y a cours que très accessoirement, même si la
possibilité de vendre en ligne est réelle et ne doit pas être négligée
ni méprisée. Il n'y est pas la seule valeur de référence, contrairement
au monde réel et, même dans les cas très médiatiques de start-up
multimillionnaires, le rapport à l'argent n'est qu'une conséquence, la
matérialisation d'espérances financières, très vite sanctionnée en cas
d'ambitions excessives comme on le voit régulièrement sur le site
"Vakooler: Ki Vakooler aujourd'hui?" (qui peut se transcrire en: Va
couler: qui va couler aujourd'hui?, NDLR), après les envolées lyriques
et délirantes des premiers temps. A terme, je pense que le cyberespace
restera un lieu beaucoup plus convivial que la société réelle.»

Nicolas Pewny, fondateur des éditions du Choucas, écrit pour sa part:
«Je reprendrai volontiers une phrase d'Alain Bron, ami et auteur de
Sanguine sur Toile (publié en 1999 par les éditions du Choucas, NDLR):
"un formidable réservoir de réponses quand on cherche une information
et de questions quand on n'en cherche pas. C'est ainsi que l'imaginaire
peut se développer... (Ma correspondante en Nouvelle-Zélande est-elle
jolie ? L'important, c'est qu'elle ait de l'esprit.)"»

# Gestionnaires

Selon Gérard Jean-François, directeur du centre de ressources
informatiques de l'Université de Caen, «le cyberspace peut être
considéré comme l'ensemble des informations qui sont accessibles sans
aucune restriction sur le réseau internet.»

Pour Pierre Magnenat, responsable de la cellule «gestion et
prospective" du centre informatique de l'Université de Lausanne, le
cyberespace est «l'ensemble des ressources et acteurs connectés et
accessibles à un moment donné.»

Pour Jacques Pataillot, conseiller en management chez Cap Gemini Ernst
& Young, le cyberespace est «l'"économie connectée" (de l'anglais
"connected economy") où tous les agents sont reliés électroniquement
pour les échanges d'information.»

# Linguistes

Pour Guy Antoine, créateur de Windows on Haiti, un site de référence
sur la culture haïtienne, «le cyberespace est au sens propre une
nouvelle frontière pour l'humanité, un endroit où chacun peut avoir sa
place, assez facilement et avec peu de ressources financières, avant
que les règlements inter-gouvernementaux et les impôts ne
l'investissent. Suite à quoi une nouvelle technologie lui succédera.»

Pour Alain Clavet, analyste de politiques au Commissariat aux langues
officielles du Canada, il s'agit d'«un lieu de connaissances partagées
non soumis aux contraintes du temps et de l'espace.»

Eduard Hovy, directeur du Natural Language Group de l'Université de
Californie du Sud, relate pour sa part: «Pour moi, le cyberespace est
représenté par la totalité des informations auxquelles nous pouvons
accéder par l'internet et les systèmes informatiques en général. Il ne
s'agit bien sûr pas d'un espace, et son contenu est sensiblement
différent de celui des bibliothèques. Par exemple, bientôt mon
réfrigérateur, ma voiture et moi-même seront connus du cyberespace, et
toute personne disposant d'une autorisation d'accès (et d'une raison
pour cela) pourra connaître précisément le contenu de mon réfrigérateur
et la vitesse de ma voiture (ainsi que la date à laquelle je devrai
changer les amortisseurs), et ce que je suis en train de regarder
maintenant. En fait, j'espère que la conception de la publicité va
changer, y compris les affiches et les présentations que j'ai sous les
yeux en marchant, afin que cette publicité puisse correspondre à mes
connaissances et à mes goûts, tout simplement en ayant les moyens de
reconnaître que "voici quelqu'un dont la langue maternelle est
l'anglais, qui vit à Los Angeles et dont les revenus sont de tant de
dollars par mois". Ceci sera possible du fait de la nature dynamique
d'un cyberespace constamment mis à jour (contrairement à une
bibliothèque), et grâce à l'existence de puces informatiques de plus en
plus petites et bon marché. Tout comme aujourd'hui j'évolue dans un
espace social qui est un réseau de normes sociales, d'expectations et
de lois, demain, j'évoluerai aussi dans un cyberespace composé
d'informations sur lesquelles je pourrai me baser (parfois), qui
limiteront mon activité (parfois), qui me réjouiront (souvent,
j'espère) et qui me décevront (j'en suis sûr).»

Pour Steven Krauwer, coordinateur d'ELSNET (European Network of
Excellence in Human Language Technologies), «le cyberespace est la
partie de l'univers (incluant personnes, machines et information) que
je peux atteindre "derrière" ma table de travail.»

Selon Zina Tucsnak, ingénieur d'études en informatique au laboratoire
ATILF (Analyse et traitements informatiques du lexique français), «dans
le cyberespace, l'information et la quantité de l'information sont
gouvernées par des lois mathématiques. Mais les modèles mathématiques
n'ont pas trouvé encore leur solution, un peu comme le mouvement
perpétuel ou la quadrature du cercle.»

# Professeurs

Pour Gaëlle Lacaze, ethnologue et professeur d'écrit électronique dans
un institut universitaire professionnel, il s'agit d'«une visuelle en
trois dimensions: superposition de lignes droites mouvantes selon des
directions multiples où les rencontres de lignes créent des points de
contact.»

Pour Patrick Rebollar, professeur de littérature française et
modérateur de la liste de diffusion LITOR (littérature et ordinateur),
le cyberespace est «la réplique virtuelle et très imparfaite du monde
des relations humaines, sociales, commerciales et politiques. En
privant partiellement les utilisateurs de la matérialité du monde
(spatiale, temporelle, corporelle), le cyberespace permet de nombreuses
interactions instantanées et multi-locales. A noter que les êtres
humains se montrent aussi stupides ou intelligents, malveillants ou
dévoués dans le cyberespace que dans l'espace réel...»

Selon Henk Slettenhaar, professeur en technologies de la communication
à la Webster University de Genève, «le cyberespace est notre espace
virtuel, à savoir l'espace de l'information numérique (constitué de
bits, et non d'atomes). Si on considère son spectre, il s'agit d'un
espace limité. Il doit être géré de telle façon que tous les habitants
de la planète puissent l'utiliser et en bénéficier. Il faut donc
éliminer la fracture numérique.»

Pour Christian Vandendorpe, professeur à l'Université d'Ottawa et
spécialiste des théories de la lecture, le cyberespace est «le nouveau
territoire de la culture, un espace qui pourrait jouer le rôle de
l'Agora dans la Grèce ancienne, mais à un niveau planétaire.»

Russon Wooldridge, professeur au département d'études françaises de
l'Université de Toronto, relate: «Je travaille dans la même université
que Marshall McLuhan autrefois (nos carrières se sont un moment
croisées). Le "village global" qu'il entrevoyait à l'époque de la radio
et de la télévision est devenu une réalité dans l'ère d'internet. Mais
un village sans classes sociales (il n'y a pas de châtelain).»

# Visionnaire(s)

Pour Pierre Schweitzer, architecte designer et concepteur du projet
@folio, un support numérique de lecture nomade, «c'est un terme un peu
obscur pour moi. Mais je déteste encore plus "réalité virtuelle".
Bizarre, cette idée de conceptualiser un ailleurs sans pouvoir y mettre
les pieds. Evidemment un peu idéalisé, "sans friction", où les choses
ont des avantages sans les inconvénients, où les autres ne sont plus
des "comme vous", où on prend sans jamais rien donner, "meilleur" -
paraît-il. Facile quand on est sûr de ne jamais aller vérifier. C'est
la porte ouverte à tous les excès, avec un discours technologique à
outrance, déconnecté du réel, mais ça ne prend pas. Dans la réalité,
internet n'est qu'une évolution de nos moyens de communication. Bon
nombre d'applications s'apparentent ni plus ni moins à un télégraphe
évolué (Morse, 1830): modem, email... Les mots du télégraphe
traversaient les océans entre Londres, New-York, Paris et Tokyo, bien
avant l'invention du téléphone. Bien sûr, la commutation téléphonique a
fait quelques progrès: jusqu'à l'hypertexte cliquable sous les doigts,
les URL en langage presqu'humain, bientôt accessibles y compris par les
systèmes d'écriture non alphabétiques... Mais notre vrai temps réel,
c'est celui des messages au fond de nos poches et de ceux qui se
perdent, pas le temps zéro des télécommunications. La segmentation et
la redondance des messages, une trouvaille d'internet? Au 19e siècle,
quand Reuters envoyait ses nouvelles par pigeon voyageur, il en baguait
déjà plusieurs. Nos pages perso? Ce sont des aquariums avec un
répondeur, une radio et trois photos plongés dedans. Tout ce joyeux
"bazar" est dans nos vies réelles, pas dans le "cyberespace".»


= Société de l'information

# Un concept vide de sens

Pour Gérard Jean-François, directeur du centre de ressources
informatiques de l'Université de Caen, «il n'y a pas de société de
l'information particulière. De tout temps, elle a toujours existé. Ce
qu'il faut noter, c'est son évolution continue. Gutenberg l'a fait
évoluer, de même internet.»

Selon Philippe Loubière, traducteur littéraire et dramatique, «il n'y a
pas, je crois, de société de l'information. Internet, la télévision, la
radio ne sont pas des moyens d'information, ce sont des moyens de
communication. L'information participe d'une certaine forme de savoir
sur le monde, et les moyens de communication de masse ne la
transmettent pratiquement pas. Ils l'évoquent dans le meilleur des cas
(ceux des journalistes de terrain par exemple), et la déforment voire
la truquent dans tous les autres. Et (pour autant qu'il le veuille!) le
pouvoir politique n'est hélas plus aujourd'hui assez "le" pouvoir pour
pouvoir faire respecter l'information et la liberté. L'information,
comme toute forme de savoir, est le résultat d'une implication
personnelle et d'un effort de celui qui cherche à s'informer. C'était
vrai au Moyen-Âge, c'est encore vrai aujourd'hui. La seule différence,
c'est qu'aujourd'hui il y a davantage de leurres en travers du chemin
de celui qui cherche.»

Pour Pierre Magnenat, responsable de la cellule «gestion et
prospective» du centre informatique de l'Université de Lausanne, il
s'agit d'«un mot à la mode, qui ne veut rien dire. Une société est par
essence communicative, et donc caractérisée par des échanges
d'informations. Les seules choses qui ont changé, c'est la quantité et
la vitesse de ces échanges.»

Patrick Rebollar, professeur de littérature française et modérateur de
la liste de diffusion LITOR (littérature et ordinateur), définit la
société de l'information comme «une grande mise en scène (mondialisée)
qui fait prendre les vessies pour des lanternes. En l'occurrence, les
gouvernants de toutes sortes, notamment sous le nom de "marché",
diffusent de plus en plus de prescriptions contraignantes (notamment
commerciales, politiques et morales) qu'ils réussissent, un peu grâce
aux merveilles technologiques, à faire passer pour des libertés. Notons
que "cybernétique" et "gouvernement" ont la même racine grecque...»

# Auteurs

Nicolas Ancion, écrivain et responsable éditorial de Luc Pire
électronique, relate: «Pour moi, la société de l'information est
l'arrivée d'un nouveau clivage sur la planète: distinction entre ceux
qui ont accès au savoir, le comprennent et l'utilisent, et ceux qui n'y
ont pas accès pour de nombreuses raisons. Il ne s'agit cependant pas
d'une nouvelle forme de société du tout car le pouvoir de l'information
n'est lié à aucun pouvoir réel (financier, territorial, etc.).
Connaître la vérité ne nourrit personne. Par contre, l'argent permet de
très facilement propager des rumeurs ou des mensonges. La société de
l'information est simplement une version avancée (plus rapide, plus
dure, plus impitoyable) de la société industrielle. Il y a ceux qui
possèdent et jouissent, ceux qui subissent et ceux dont on ne parle
jamais: ceux qui comprennent et ne peuvent pas changer les choses. Au
19e siècle, certains artistes et certains intellectuels se retrouvaient
dans cette position inconfortable. Grâce à la société de l'information,
beaucoup de gens ont rejoint cette catégorie assise entre deux chaises.
Qui possède des biens matériels et a peur de les perdre mais considère
pourtant que les choses ne vont pas dans la bonne direction. Mon
opinion personnelle, par rapport à tout ça, c'est que ce n'est pas
l'information qui sauve. C'est la volonté. Pour changer le monde,
commençons par lever notre cul de notre chaise et retrousser nos
manches.»

Pour Alex Andrachmes, producteur audiovisuel, écrivain et explorateur
d'hypertexte, la société de l'information est «dans l'idéal, un lieu
d'échange, le fameuse agora du village global. Mais l'idéal... Tant que
le débat existe entre les fous du net et les VRP (voyageurs
représentants de commerce, NDLR) de la VPC (vente par correspondance,
NDLR), il y a de l'espoir. Le jour où les grands portails se
refermeront sur la liberté d'échanger des infos en ligne, ça risque
plutôt d'être la société de la désinformation. Ici aussi, des
confusions sont soigneusement entretenues. Quelle information, celles
du 20 heures à relayer telles quelles sur le net? Celles contenues sur
ces fabuleux CD, CD-ROM, DVD chez vous dans les 24 h chrono? Ou toutes
les connaissances contenues dans les milliards de pages non
répertoriées par les principaux moteurs de recherche. Ceux qui ont de
plus en plus tendance à mettre en avant les sites les plus visités, qui
le sont dès lors de plus en plus. Là, on ne parle même plus de
désinformation, de complot de puissances occultes (financières,
politiques ou autres...), mais de surinformation, donc de lassitude, de
non-information, et finalement d'uniformisation de la pensée. Sans
avoir de définition précise, je vois qu'une société de l'information
qui serait figée atteindrait le contraire de sa définition de base. Du
mouvement donc...»

Lucie de Boutiny, romancière multimédia, écrit: «Je préférerais parler
de "communautés de l'information"... Nous sommes plutôt dans une
société de la communication et de la commutation. Il est très
discutable de savoir si nos discussions sont de meilleure qualité et si
nous serions plus savants... Etre informé n'est pas être cultivé.»

Pour Jean-Pierre Cloutier, auteur des Chroniques de Cybérie, chronique
hebdomadaire des actualités de l'internet, la société de l'information
est «une société où l'unité de valeur réelle est l'information
produite, transformée, échangée. Elle correspond au "centre" du
cyberespace. Malheureusement, le concept a tellement été galvaudé,
banalisé, on l'a servi à toutes les sauces politiciennes pour tenter
d'évoquer ce qu'on ne pouvait imaginer dans le détail, ou concevoir
dans l'ensemble, de sorte que l'expression a perdu de son sens.»

Pour Luc Dall'Armellina, co-auteur et webmestre d'oVosite, un espace
d'écriture hypermédia, la société de l'information est «la nôtre, je
pense? L'américano-nord-européenne. A la Bourse, les annonces ont des
effets mesurables en millions de dollars ou d'euros et déclenchent des
impacts économiques et humains parfois très violents: rachats, ventes,
hausses et baisses des valeurs, licenciements. C'est une société où la
valeur absolue est l'information et son contrôle, et la valeur relative
l'humain.»

Jean-Paul, webmestre du site hypermédia cotres.net, définit la société
de l'information en trois mots: «Plus, plus vite. Mais les données ne
sont pas l'information. Il faut les liens, c'est-à-dire le temps. Plus
d'évènements, plus d'écrans pour les couvrir. Plus vite: l'évènement du
jour est liquide. Effacé, recouvert par la vaguelette du lendemain, la
vague du jour d'après, la houle de la semaine, le tsunami du mois.
Cycles aussi "naturels" que les marées estivales du Loch Ness. Pas
"effacé", d'ailleurs, l'évènement d'hier (qui n'est pas "tous les
évènements d'hier"): déja archivé, dans des bases de données qui
donnent l'illusion d'être exhaustives, facilement accessibles et
momentanément gratuites. Mais les données ne donnent rien par elles-
même. S'informer, c'est lier entre elles des données, éliminer celles
qui ne sont pas pertinentes (quitte à revenir sur ces choix plus tard),
se trouver ainsi obligé de chercher d'autres données qui corroborent ou
infirment les précédentes... L'information naît du temps passé à tisser
les liens. Or le temps nous est mesuré, au quartz près. Productique ou
temps libre, nous passons de plus en plus de temps à raccrocher au nez
de spammeurs qui nous interrompent pour nous revendre nos désirs (dont
nous informons les bases de données qui les leur vendent). Ce qui est
intéressant dans ce bonneteau est que les infos que nous fournissons
sur nous-mêmes, nous les truquons suffisamment pour que les commerciaux
n'arrivent pas à en tirer les lois du succès: Survivor II est un bide,
après le succès de la version I. De cette incertitude viennent les
trous dans le filet qui laissent parvenir jusqu'à nous certaines infos.
Bref la "société de l'information", c'est le jeu des regards dans le
tableau de de La Tour "La diseuse de bonne aventure". Le jeune homme
qui se fait dépouiller en est conscient, et complice. Il a visiblement
les moyens de s'offrir les flatteries des trois jolies filles tout en
exigeant de la vieille Diseuse qu'elle lui rende l'une de ces piécettes
dont il a pris la précaution de gonfler ostensiblement la bourse qu'on
lui coupe.»

Pour Anne-Bénédicte Joly, écrivain auto-éditant ses livres, la société
de l'information permet «l'accès au plus grand nombre de la plus grande
quantité d'information possible tout en garantissant la partialité de
l'information et en fournissant les clefs de compréhension nécessaires
à sa bonne utilisation.»

Tim McKenna, écrivain et philosophe, écrit: «Je considère la société de
l'information comme la forme tangible de la conscience collective de
Jung. L'information réside essentiellement dans notre subconscient
mais, grâce à l'existence de navigateurs, l'information est désormais
plus facile à récupérer. Cette information favorise une meilleure
connaissance de nous-mêmes en tant qu'individus et en tant qu'êtres
humains.»

Selon Xavier Malbreil, auteur multimédia et modérateur de la liste e-
critures, la société de l'information est «la circulation de
l'information en temps réel. La connaissance immédiate. L'oubli
immédiat. L'espace saturé d'ondes nous entourant, et nous, corps
humains, devenant peu à peu un simple creux laissé par les ondes, une
simple interconnexion. Corps humains devenant instants de
l'information.»

Pour Murray Suid, auteur de livres pédagogiques et de logiciels
éducatifs, il s'agit d'«une société dans laquelle les idées et le
savoir sont plus importants que les objets.»

# Documentalistes

Selon Emmanuel Barthe, documentaliste juridique et modérateur de la
liste de discussion Juriconnexion, «il s'agit nettement moins d'une
"société" de l'information que d'une économie de l'information.
J'espère que la société, elle, ne sera jamais dominée par
l'information, mais restera cimentée par des liens entre les hommes de
toute nature, qu'ils communiquent bien ou mal, peu ou beaucoup.»

Pour Bakayoko Bourahima, documentaliste à l'ENSEA (Ecole nationale
supérieure de statistique et d'économie appliquée) d'Abidjan, la
société de l'information est «la société de l'informatique et de
l'internet.»

Pour Peter Raggett, sous-directeur de la Bibliothèque centrale de
l'OCDE (Organisation de coopération et de développement économiques),
«la société de l'information est cette société dont le produit le plus
précieux est l'information. Jusqu'au 20e siècle, ce sont les produits
manufacturiers qui ont été les plus considérés. Ils ont ensuite été
remplacés par l'information. En fait, on parle maintenant davantage
d'une société du savoir, dans laquelle, du point de vue économique, le
produit le plus prisé est le savoir acquis par chacun.»

# Editeurs

Pour Marie-Aude Bourson, créatrice de Gloupsy, site littéraire destiné
aux nouveaux auteurs, il s'agit d'«une société où l'information circule
très vite (trop peut-être), et où chaque acteur se doit de rester
toujours informé s'il ne veut pas s'exclure. L'information elle-même
devient une véritable valeur monnayable.»

Pour Pierre-Noël Favennec, expert à la direction scientifique de France
Télécom R&D et directeur de collection, il s'agit d'«une société dans
laquelle tout membre de cette société a accès immédiatement à toutes
les informations souhaitées.»

Olivier Gainon, créateur de CyLibris et pionnier de l'édition
littéraire en ligne, relate: «Ce que nous vivons aujourd'hui, c'est la
mise en réseau de notre société, au sens où, à terme, beaucoup des
objets quotidiens seront connectés au Réseau (avec un grand R, qui sera
lui-même composé de dizaines de réseaux différents). Bref, c'est une
nouvelle manière de vivre et, à terme, certainement une nouvelle
société. S'agit-il d'une société de "l'information"? Je n'en suis pas
certain. Faut-il que nous définissions collectivement ce que nous
voulons dans cette société? Cela me semble urgent, et c'est un débat
qui concerne tout le monde, pas uniquement les "connectés". Bref, sur
quelles valeurs de société fonder notre action future? Voilà un vrai
débat. (...) "La Toile" de Jean-Pierre Balpe me semble aujourd'hui la
meilleure illustration de ce débat. La société qu'il décrit au travers
de ce roman est à mon sens la plus probable à court terme (l'action se
passe en 2015). Est-ce cela que nous voulons? Est-ce ce type
d'organisation? Peut-être, mais mon souci, c'est que ce choix soit
conscient et non subi.»

Selon Jacky Minier, créateur de Diamedit, site de promotion d'inédits
artistiques et littéraires, «la société de l'information amène un
recadrage des hiérarchies dans les rapports qui s'établissent entre les
gens, de manière beaucoup plus naturelle, à partir des discussions en
forums notamment. Dans la vie réelle, on est souvent influencé, voire
impressionné, par les titres ou la largeur du bureau d'un interlocuteur
"installé" dans le système. Sur le net, seuls comptent le sens contenu
dans le propos et la manière de l'exprimer. On distingue très vite les
véritables intelligences raffinées des clowns ou autres mythomanes. Une
forme de pédagogie conviviale, non intentionnelle et surtout non
magistrale, s'en dégage généralement qui profite au visiteur lambda,
lequel parfois apporte aussi sa propre expérience. Tout ça laisse
augurer d'une créativité multiforme, dans un bouillonnement commun à
des milliers de cerveaux reliés fonctionnant à la manière d'une
fourmilière. C'est non seulement un véritable moyen d'échange du
savoir, mais de surcroît un moyen de l'augmenter en quantité, de
l'approfondir, de l'intégrer entre différentes disciplines. Le net va
rendre les gens plus intelligents en favorisant leur plus grande
convivialité, en cassant les départements et domaines réservés de
certains mandarins. Mais il est clair qu'il faudra aussi faire
attention aux dérives que cette liberté implique.»

Pour Nicolas Pewny, fondateur des éditions du Choucas, il s'agit d'«une
société qui pourrait apporter beaucoup, si l'on empêche qu'elle ne rime
trop avec "consommation" et tout ce qui accompagne ce mot. Mais il est
déjà trop tard peut-être...»

François Vadrot, PDG de la société de cyberpresse FTPress, la définit
comme «une société dont l'information est le moteur, dans tous les sens
du terme.»

# Linguistes

Pour Alain Clavet, analyste de politiques au Commissariat aux langues
officielles du Canada, la société de l'information est «le constat que
la valeur ajoutée centrale (en référence à une notion économique, celle
de la valeur ajoutée) devient de plus en plus l'intelligence de
l'information. Ainsi, dans une société de l'information, la
connaissance devient la plus-value recherchée.»

Selon Eduard Hovy, directeur du Natural Language Group de l'Université
de Californie du Sud,  «une société de l'information est une société
dans laquelle la majorité des gens a conscience de l'importance de
cette information en tant que produit de base, et y attache donc tout
naturellement du prix. Au cours de l'histoire, il s'est toujours trouvé
des gens qui ont compris combien cette information était importante,
afin de servir leurs propres intérêts. Mais quand la société, dans sa
majorité, commence à travailler avec et sur l'information en tant que
telle, cette société peut être dénommée société de l'information. Ceci
peut sembler une définition tournant un peu en rond ou vide de sens,
mais je vous parie que, pour chaque société, les anthropologues sont
capables de déterminer quel est le pourcentage de la société se
consacrant au traitement de l'information comme produit de base. Dans
les sociétés anciennes, ils trouveront uniquement des professeurs, des
conseillers de dirigeants et des sages. Dans les sociétés suivantes,
ils trouveront des bibliothécaires, des experts à la retraite exerçant
une activité de consultant, etc. Les différentes étapes de la
communication de l'information - d'abord verbale, puis écrite, puis
imprimée, puis électronique - ont chaque fois élargi (dans le temps et
dans l'espace) le champ de propagation de cette information, en rendant
de ce fait de moins en moins nécessaire le réapprentissage et la
répétition de certaines tâches difficiles. Dans une société de
l'information très évoluée, je suppose, il devrait être possible de
formuler votre objectif, et les services d'information (à la fois les
agents du cyberespace et les experts humains) oeuvreraient ensemble
pour vous donner les moyens de réaliser cet objectif, ou bien se
chargeraient de le réaliser pour vous, et réduiraient le plus possible
votre charge de travail en la limitant à un travail vraiment nouveau ou
à un travail nécessitant vraiment d'être refait à partir de documents
rassemblés pour vous dans cette intention.»

Pour Steven Krauwer, coordinateur d'ELSNET (European Network of
Excellence in Human Language Technologies), «la société de
l'information est une société dans laquelle: (a) l'essentiel du savoir
et de l'information n'est plus stocké dans des cerveaux ou des livres
mais sur des médias électroniques; (b) les dépôts d'information sont
distribués et interconnectés au moyen d'une infrastructure spécifique,
et accessibles de partout; (c) les processus sociaux sont devenus
tellement dépendants de cette information et de son infrastructure que
les citoyens non connectés au système d'information ne peuvent
pleinement participer au fonctionnement de la société.»

Selon Zina Tucsnak, ingénieur d'études en informatique au laboratoire
ATILF (Analyse et traitements informatiques du lexique français), «la
société de l'information peut être définie comme un milieu dans lequel
se développent la culture et la civilisation par l'intermédiaire de
l'informatique, qui restera la base et la théorie de cette société.»

# Professeurs

Pour Emilie Devriendt, élève professeur à l'Ecole normale supérieure de
Paris, «le syntagme "société de l'information" est plus une formule
(journalistique, politique) à la mode depuis plusieurs années, qu'une
véritable notion. Cette formule tend communément je crois, à désigner
une nouvelle "ère" socio-économique, post-industrielle, qui
transformerait les relations sociales du fait de la diffusion
généralisée des nouvelles technologies de l'information et de la
communication (NTIC). Personnellement, je n'adhère pas à cette vision
des choses. Si la diffusion croissante des NTIC est indéniable et
constitue un phénomène socio-économique propre à l'époque
contemporaine, je ne crois pas qu'il faille y voir la marque de
l'avènement d'une nouvelle société "de l'information". La formule
"société de l'information" est construite sur le modèle terminologique
(socio-économique) de la "société industrielle". Mais le parallèle est
trompeur: "société de l'information" met l'accent sur un contenu, alors
que "société industrielle" désigne l'infrastructure économique de cette
société. L'information en tant que produit (industriel ou service)
apparaît peut-être plus complexe que, par exemple, les produits
alimentaires, mais cette complexité ne suffit pas à définir l'avènement
dont il est question. D'autant plus que l'emploi inconditionnel de la
formule a contribué à faire de l'information un terme passe-partout,
très éloigné même de sa théorisation mathématique (Shannon), de sa
signification informatique initiale. Elle traduit uniquement une
idéologie du progrès électronique mise en place dans les années 1950 et
véhiculée ensuite par nos gouvernements et la plupart de nos
journalistes, qui définissent fallacieusement le développement des NTIC
comme un "nécessaire" vecteur de progrès social. Quelques analystes
(sociologues et historiens des techniques comme Mattelart, Lacroix,
Guichard, Wolton) ont très bien montré cela.»

Pour Henk Slettenhaar, professeur en technologies de la communication à
la Webster University de Genève, «la société de l'information est
l'ensemble des personnes utilisant quotidiennement le cyberespace de
manière intensive et qui n'envisageraient pas de vivre sans cela, à
savoir les nantis, ceux qui sont du bon côté de la fracture numérique.»

Pour Gaëlle Lacaze, ethnologue et professeur d'écrit électronique dans
un institut universitaire professionnel, il s'agit d'«une société où
l'information est reçue et digérée, sans être étouffée par la
profusion.»

Russon Wooldridge, professeur au département d'études françaises de
l'Université de Toronto, écrit: «Si on veut parler de "société" il ne
peut pas être question d'une opposition "haves" vs. "have-nots" (munis
vs. démunis), sauf dans la mesure où l'accès à l'information est plus
ou moins libre ou limité d'un point de vue technologique ou économique,
voire politique. Par exemple, l'accès à l'information en ligne est plus
libre au Canada qu'en France, plus libre en France qu'en Algérie, etc.
Internet est potentiellement un moyen pour que chacun puisse
s'approprier son propre contrôle de l'information, qui n'est plus
diffusée par les seuls canaux dirigistes, comme l'Edition ou
l'Université, entre autres.»

# Visionnaires

Olivier Pujol, PDG de Cytale et promoteur de sa tablette de lecture, la
définit comme «une société où l'accès à l'information, l'information
elle-même et la capacité à bien utiliser l'information sont des biens
plus précieux que les biens matériels. Il faut noter que l'information
a toujours été un avantage professionnel considérable. Il fut un temps
où un avantage concurrentiel pouvait exister sur un territoire limité,
et être protégé pour un temps long, par le secret, ou l'ignorance des
autres. Les voyages, la mondialisation des échanges, la performance de
la logistique ont énormément affaibli la notion de protection
"géographique" d'un avantage concurrentiel. La société de l'information
est une société où la protection de l'information est presque
impossible, et où son usage devient donc la valeur essentielle.»

Laissons le mot de la fin à Pierre Schweitzer, architecte designer et
concepteur du projet @folio, un support numérique de lecture nomade:
«J'aime bien l'idée que l'information, ce n'est que la forme des
messages. La circulation des messages est facilitée, techniquement, et
elle s'intensifie. Et désormais, le monde évolue avec ça.»



CHRONOLOGIE

[Chaque ligne débute par l'année ou bien l'année/mois. Par exemple,
1971/07 signifie juillet 1971.]

  1968: Le code ASCII est le premier système d'encodage informatique.
  1971/07: Le Projet Gutenberg est la première bibliothèque numérique.
  1974: L'internet fait ses débuts.
  1977: L'UNIMARC est créé en tant que format bibliographique commun.
  1983: L'internet prend son envol.
  1984: Le copyleft est institué pour les logiciels.
  1990: Le web fait ses débuts.
  1991/01: L'Unicode est un système d'encodage pour toutes les langues.
  1993/01: L'Online Books Page est le premier répertoire d'ebooks
gratuits.
  1993/06: Adobe lance le format PDF et l'Acrobat Reader.
  1993/11: Mosaic est le premier logiciel de navigation sur le web.
  1994: Le premier site de bibliothèque est mis en ligne.
  1994: Les éditeurs utilisent le web comme outil de marketing.
  1995/07: Amazon.com est la première grande librairie en ligne.
  1995: La grande presse se met en ligne.
  1996/03: Le Palm Pilot est le premier assistant personnel (PDA).
  1996/04: L'Internet Archive est créée pour archiver le web.
  1996/07: CyLibris est le pionnier francophone de l’édition
électronique.
  1996/10: Le projet @folio travaille sur un baladeur de textes
«ouvert».
  1996: Des professeurs se penchent sur de nouvelles méthodes
d'enseignement.
  1997/01: La convergence multimédia est le sujet d'un colloque.
  1997/04: E Ink développe une technologie d’encre électronique.
  1997/10: Gallica est la section numérique de la Bibliothèque
nationale de France.
  1997: L'édition électronique commence à se généraliser.
  1997: Le Logos Dictionary est mis en ligne gratuitement.
  1998/05: Les éditions 00h00 vendent «uniquement» des livres
numériques.
  1999/09: Le format Open eBook (OeB) est un standard de livre
numérique.
  1999/12: WebEncyclo est la première encyclopédie francophone en accès
libre.
  1999/12: Britannica.com est première encyclopédie anglophone en accès
libre.
  1999: Les bibliothécaires numériques font carrière.
  1999: Certains auteurs se mettent au numérique.
  2000/01: Le Million Book Project veut proposer un million de livres
sur le web.
  2000/02: yourDictionary.com est un portail pour les langues.
  2000/03: Mobipocket se consacre aux livres numériques pour assistant
personnel.
  2000/07: La moitié des usagers de l'internet est non anglophone.
  2000/07: Stephen King auto-publie un roman en ligne.
  2000/08: Microsoft lance le format LIT et le Microsoft Reader.
  2000/09: Le Grand dictionnaire terminologique (GDT) est bilingue
français-anglais.
  2000/09: La librairie Numilog se consacre aux livres numériques.
  2000/09: Le portail Handicapzéro démontre que l'internet est pour
tous.
  2000/10: Distributed Proofreaders numérise les livres du domaine
public.
  2000/10: La Public Library of Science lancera des revues en ligne
gratuites.
  2000/11: La version numérisée de la Bible de Gutenberg est
disponible.
  2001/01: Wikipédia est la première grande encyclopédie collaborative
gratuite.
  2001: Creative Commons rénove le droit d'auteur en l'adaptant au web.
  2003/09: Les cours du MIT OpenCourseWare sont à la disposition de
tous.
  2004/01: Le Projet Gutenberg Europe sera multilingue.
  2004/10: Google lance Google Print pour le rebaptiser ensuite Google
Books.
  2005/04: Amazon.com rachète la société Mobipocket.
  2005/10: L'Open Content Alliance lance une bibliothèque numérique
universelle.
  2006/08: Le catalogue collectif WorldCat devient gratuit sur le web.
  2006/10: Microsoft lance Live Search Books mais l'abandonne ensuite.
  2006/10: Sony lance sa tablette de lecture Sony Reader.
  2007/03: Citizendium lance une encyclopédie en ligne collaborative
«fiable».
  2007/03: IATE (Inter-Active Terminology for Europe) est une base
terminologique européenne.
  2007/05: L'Encyclopedia of Life répertoriera toutes les espèces
végétales et animales.
  2007/11: Amazon.com lance sa tablette de lecture Kindle.
  2008/05: Numilog devient une filiale d'Hachette Livre.
  2008/10: Google Books propose un accord aux associations d'auteurs et
d'éditeurs.
  2008/11: Europeana est la bibliothèque numérique européenne.
  2009/02: Amazon.com lance le Kindle 2.



REMERCIEMENTS

Ce livre doit beaucoup à toutes les personnes ayant accepté de répondre
à mes questions, dont certaines pendant plusieurs années. Certains
entretiens ont été publiés tels quels par le NEF (Net des études
françaises), Université de Toronto. Ils sont disponibles en ligne
<www.etudes-francaises.net/entretiens/index.html>. D'autres entretiens
ont été directement inclus dans ce livre, avec des textes de Nicolas
Ancion, Alex Andrachmes, Guy Antoine, Silvaine Arabo, Arlette Attali,
Marc Autret, Isabelle Aveline, Jean-Pierre Balpe, Emmanuel Barthe,
Robert Beard, Michael Behrens, Michel Benoît, Guy Bertrand, Olivier
Bogros, Christian Boitet, Bernard Boudic, Bakayoko Bourahima, Marie-
Aude Bourson, Lucie de Boutiny, Anne-Cécile Brandenbourger, Alain Bron,
Patrice Cailleaud, Tyler Chambers, Pascal Chartier, Richard Chotin,
Alain Clavet, Jean-Pierre Cloutier, Jacques Coubard, Luc
Dall’Armellina, Kushal Dave, Cynthia Delisle, Emilie Devriendt, Bruno
Didier, Catherine Domain, Helen Dry, Bill Dunlap, Pierre-Noël Favennec,
Gérard Fourestier, Pierre François Gagnon, Olivier Gainon, Jacques
Gauchey, Raymond Godefroy, Muriel Goiran, Marcel Grangier, Barbara
Grimes, Michael Hart, Roberto Hernández Montoya, Randy Hobler, Eduard
Hovy, Christiane Jadelot, Gérard Jean-François, Jean-Paul, Anne-
Bénédicte Joly, Brian King, Geoffrey Kingscott, Steven Krauwer, Gaëlle
Lacaze, Michel Landaret, Hélène Larroche, Pierre Le Loarer, Claire Le
Parco, Annie Le Saux, Fabrice Lhomme, Philippe Loubière, Pierre
Magnenat, Xavier Malbreil, Alain Marchiset, Maria Victoria Marinetti,
Michael Martin, Tim McKenna, Emmanuel Ménard, Yoshi Mikami, Jacky
Minier, Jean-Philippe Mouton, John Mark Ockerbloom, Caoimhín Ó
Donnaíle, Jacques Pataillot, Alain Patez, Nicolas Pewny, Marie-Joseph
Pierre, Hervé Ponsot, Olivier Pujol, Anissa Rachef, Peter Raggett,
Patrick Rebollar, Philippe Renaut, Jean-Baptiste Rey, Philippe Rivière,
Blaise Rosnay, Bruno de Sa Moreira, Pierre Schweitzer, Henk
Slettenhaar, Murray Suid, June Thompson, Zina Tucsnak, François Vadrot,
Christian Vandendorpe, Robert Ware, Russon Wooldridge et Denis Zwirn.


Copyright © 2009 Marie Lebert. Tous droits réservés.








End of Project Gutenberg's Une courte histoire de l'eBook, by Marie Lebert

